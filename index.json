[{"categories":null,"content":"１、什么是Socket 在计算机通信领域，socket 被翻译为“套接字”，它是计算机之间进行通信的一种约定或一种方式。通过 socket 这种约定，一台计算机可以接收其他计算机的数据，也可以向其他计算机发送数据 socket起源于Unix，而Unix/Linux基本哲学之一就是“一切皆文件”，都可以用“打开open –\u003e 读写write/read –\u003e 关闭close”模式来操作。 我的理解就是Socket就是该模式的一个实现：即socket是一种特殊的文件，一些socket函数就是对其进行的操作（读/写IO、打开、关闭）。 Socket()函数返回一个整型的Socket描述符，随后的连接建立、数据传输等操作都是通过该Socket实现的。 ","date":"2022-04-05","objectID":"/posts/socket/:1:0","tags":["计网"],"title":"Socket","uri":"/posts/socket/"},{"categories":null,"content":"２、网络中进程如何通信 既然Socket主要是用来解决网络通信的，那么我们就来理解网络中进程是如何通信的。 2.1、本地进程间通信 a、消息传递（管道、消息队列、FIFO） b、同步（互斥量、条件变量、读写锁、文件和写记录锁、信号量） c、共享内存（匿名的和具名的，eg:channel） d、远程过程调用(RPC) 2.2、网络中进程如何通信 我们要理解网络中进程如何通信，得解决两个问题： ａ、我们要如何标识一台主机，即怎样确定我们将要通信的进程是在那一台主机上运行。 ｂ、我们要如何标识唯一进程，本地通过pid标识，网络中应该怎样标识？ 解决办法： ａ、TCP/IP协议族已经帮我们解决了这个问题，网络层的“ip地址”可以唯一标识网络中的主机 ｂ、传输层的“协议+端口”可以唯一标识主机中的应用程序（进程），因此，我们利用三元组（ip地址，协议，端口）就可以标识网络的进程了，网络中的进程通信就可以利用这个标志与其它进程进行交互 ","date":"2022-04-05","objectID":"/posts/socket/:2:0","tags":["计网"],"title":"Socket","uri":"/posts/socket/"},{"categories":null,"content":"３、Socket怎么通信 现在，我们知道了网络中进程间如何通信，即利用三元组【ip地址，协议，端口】可以进行网络间通信了，那我们应该怎么实现了，因此，我们socket应运而生，它就是利用三元组解决网络通信的一个中间件工具，就目前而言，几乎所有的应用程序都是采用socket，如UNIX BSD的套接字（socket）和UNIX System V的TLI（已经被淘汰）。 Socket通信的数据传输方式，常用的有两种： ａ、SOCK_STREAM：表示面向连接的数据传输方式。数据可以准确无误地到达另一台计算机，如果损坏或丢失，可以重新发送，但效率相对较慢。常见的 http 协议就使用 SOCK_STREAM 传输数据，因为要确保数据的正确性，否则网页不能正常解析。 ｂ、SOCK_DGRAM：表示无连接的数据传输方式。计算机只管传输数据，不作数据校验，如果数据在传输中损坏，或者没有到达另一台计算机，是没有办法补救的。也就是说，数据错了就错了，无法重传。因为 SOCK_DGRAM 所做的校验工作少，所以效率比 SOCK_STREAM 高。 例如：QQ 视频聊天和语音聊天就使用 SOCK_DGRAM 传输数据，因为首先要保证通信的效率，尽量减小延迟，而数据的正确性是次要的，即使丢失很小的一部分数据，视频和音频也可以正常解析，最多出现噪点或杂音，不会对通信质量有实质的影响 ","date":"2022-04-05","objectID":"/posts/socket/:3:0","tags":["计网"],"title":"Socket","uri":"/posts/socket/"},{"categories":null,"content":"４、TCP/IP协议 4.1、概念 TCP/IP【TCP（传输控制协议）和IP（网际协议）】提供点对点的链接机制，将数据应该如何封装、定址、传输、路由以及在目的地如何接收，都加以标准化。它将软件通信过程抽象化为四个抽象层，采取协议堆栈的方式，分别实现出不同通信协议。协议族下的各种协议，依其功能不同，被分别归属到这四个层次结构之中，常被视为是简化的七层OSI模型。 它们之间好比送信的线路和驿站的作用，比如要建议送信驿站，必须得了解送信的各个细节。 TCP（Transmission Control Protocol，传输控制协议）是一种面向连接的、可靠的、基于字节流的通信协议，数据在传输前要建立连接，传输完毕后还要断开连接，客户端在收发数据前要使用 connect() 函数和服务器建立连接。建立连接的目的是保证IP地址、端口、物理链路等正确无误，为数据的传输开辟通道。 TCP建立连接时要传输三个数据包，俗称三次握手（Three-way Handshaking）。可以形象的比喻为下面的对话： [Shake 1] 套接字A：“你好，套接字B，我这里有数据要传送给你，建立连接吧。” [Shake 2] 套接字B：“好的，我这边已准备就绪。” [Shake 3] 套接字A：“谢谢你受理我的请求。 4.2、TCP的粘包问题以及数据的无边界性：　https://blog.csdn.net/m0_37947204/article/details/80490512 4.4、TCP数据报结构： 带阴影的几个字段需要重点说明一下： (1) 序号：Seq（Sequence Number）序号占32位，用来标识从计算机A发送到计算机B的数据包的序号，计算机发送数据时对此进行标记。 (2) 确认号：Ack（Acknowledge Number）确认号占32位，客户端和服务器端都可以发送，Ack = Seq + 1。 (3) 标志位：每个标志位占用1Bit，共有6个，分别为 URG、ACK、PSH、RST、SYN、FIN，具体含义如下： （1）URG：紧急指针（urgent pointer）有效。 （2）ACK：确认序号有效。 （3）PSH：接收方应该尽快将这个报文交给应用层。 （4）RST：重置连接。 （5）SYN：建立一个新连接。 （6）FIN：断开一个连接。 4.5、连接的建立（三次握手）： 使用 connect() 建立连接时，客户端和服务器端会相互发送三个数据包，请看下图： 　20180529001324885.jpeg 客户端调用 socket() 函数创建套接字后，因为没有建立连接，所以套接字处于CLOSED状态；服务器端调用 listen() 函数后，套接字进入LISTEN状态，开始监听客户端请求 这时客户端发起请求： 1) 当客户端调用 connect() 函数后，TCP协议会组建一个数据包，并设置 SYN 标志位，表示该数据包是用来建立同步连接的。同时生成一个随机数字 1000，填充“序号（Seq）”字段，表示该数据包的序号。完成这些工作，开始向服务器端发送数据包，客户端就进入了SYN-SEND状态。 2) 服务器端收到数据包，检测到已经设置了 SYN 标志位，就知道这是客户端发来的建立连接的“请求包”。服务器端也会组建一个数据包，并设置 SYN 和 ACK 标志位，SYN 表示该数据包用来建立连接，ACK 用来确认收到了刚才客户端发送的数据包 服务器生成一个随机数 2000，填充“序号（Seq）”字段。2000 和客户端数据包没有关系。 服务器将客户端数据包序号（1000）加1，得到1001，并用这个数字填充“确认号（Ack）”字段。 服务器将数据包发出，进入SYN-RECV状态 3) 客户端收到数据包，检测到已经设置了 SYN 和 ACK 标志位，就知道这是服务器发来的“确认包”。客户端会检测“确认号（Ack）”字段，看它的值是否为 1000+1，如果是就说明连接建立成功。 接下来，客户端会继续组建数据包，并设置 ACK 标志位，表示客户端正确接收了服务器发来的“确认包”。同时，将刚才服务器发来的数据包序号（2000）加1，得到 2001，并用这个数字来填充“确认号（Ack）”字段。 客户端将数据包发出，进入ESTABLISED状态，表示连接已经成功建立。 4) 服务器端收到数据包，检测到已经设置了 ACK 标志位，就知道这是客户端发来的“确认包”。服务器会检测“确认号（Ack）”字段，看它的值是否为 2000+1，如果是就说明连接建立成功，服务器进入ESTABLISED状态。 至此，客户端和服务器都进入了ESTABLISED状态，连接建立成功，接下来就可以收发数据了。 4.6、TCP四次握手断开连接 建立连接非常重要，它是数据正确传输的前提；断开连接同样重要，它让计算机释放不再使用的资源。如果连接不能正常断开，不仅会造成数据传输错误，还会导致套接字不能关闭，持续占用资源，如果并发量高，服务器压力堪忧。 断开连接需要四次握手，可以形象的比喻为下面的对话： [Shake 1] 套接字A：“任务处理完毕，我希望断开连接。” [Shake 2] 套接字B：“哦，是吗？请稍等，我准备一下。” 等待片刻后…… [Shake 3] 套接字B：“我准备好了，可以断开连接了。” [Shake 4] 套接字A：“好的，谢谢合作。” 下图演示了客户端主动断开连接的场景： 建立连接后，客户端和服务器都处于ESTABLISED状态。这时，客户端发起断开连接的请求： 客户端调用 close() 函数后，向服务器发送 FIN 数据包，进入FIN_WAIT_1状态。FIN 是 Finish 的缩写，表示完成任务需要断开连接。 服务器收到数据包后，检测到设置了 FIN 标志位，知道要断开连接，于是向客户端发送“确认包”，进入CLOSE_WAIT状态。 注意：服务器收到请求后并不是立即断开连接，而是先向客户端发送“确认包”，告诉它我知道了，我需要准备一下才能断开连接。 客户端收到“确认包”后进入FIN_WAIT_2状态，等待服务器准备完毕后再次发送数据包。 等待片刻后，服务器准备完毕，可以断开连接，于是再主动向客户端发送 FIN 包，告诉它我准备好了，断开连接吧。然后进入LAST_ACK状态。 客户端收到服务器的 FIN 包后，再向服务器发送 ACK 包，告诉它你断开连接吧。然后进入TIME_WAIT状态。 服务器收到客户端的 ACK 包后，就断开连接，关闭套接字，进入CLOSED状态。 4.7、关于 TIME_WAIT 状态的说明 客户端最后一次发送 ACK包后进入 TIME_WAIT 状态，而不是直接进入 CLOSED 状态关闭连接，这是为什么呢？ TCP 是面向连接的传输方式，必须保证数据能够正确到达目标机器，不能丢失或出错，而网络是不稳定的，随时可能会毁坏数据，所以机器A每次向机器B发送数据包后，都要求机器B”确认“，回传ACK包，告诉机器A我收到了，这样机器A才能知道数据传送成功了。如果机器B没有回传ACK包，机器A会重新发送，直到机器B回传ACK包。 客户端最后一次向服务器回传ACK包时，有可能会因为网络问题导致服务器收不到，服务器会再次发送 FIN 包，如果这时客户端完全关闭了连接，那么服务器无论如何也收不到ACK包了，所以客户端需要等待片刻、确认对方收到ACK包后才能进入CLOSED状态。那么，要等待多久呢？ 数据包在网络中是有生存时间的，超过这个时间还未到达目标主机就会被丢弃，并通知源主机。这称为报文最大生存时间（MSL，Maximum Segment Lifetime）。TIME_WAIT 要等待 2MSL 才会进入 CLOSED 状态。ACK 包到达服务器需要 MSL 时间，服务器重传 FIN 包也需要 MSL 时间，2MSL 是数据包往返的最大时间，如果 2MSL 后还未收到服务器重传的 FIN 包，就说明服务器已经收到了 ACK 包 4.８.优雅的断开连接–shutdown() close()/closesocket()和shutdown()的区别 确切地说，close() / closesocket() 用来关闭套接字，将套接字描述符（或句柄）从内存清除，之后再也不能使用该套接字，与C语言中的 fclose() 类似。应用程序关闭套接字后，与该套接字相关的连接和缓存也失去了意义，TCP协议会自动触发关闭连接的操作。 shutdown() 用来关闭连接，而不是套接字，不管调用多少次 shutdown()，套接字依然存在，直到调用 close() / closesocket() 将套接字从内存清除。 调用 close()/closesocket() 关闭套接字时，或调用 shutdown() 关闭输出流时，都会向对方发送 FIN 包。FIN 包表示数据传输完毕，计算机收到 FIN 包就知道不会再有数据传送过来了。 默认情况下，close()/closesocket() 会立即向网络中发送FIN包，不管输出缓冲区中是否还有数据，而shutdown() 会等输出缓冲区中的数据传输完毕再发送FIN包。也就意味着，调用 close()/closesocket() 将丢失输出缓冲区中的数据，而调用 shutdown() 不会 ","date":"2022-04-05","objectID":"/posts/socket/:4:0","tags":["计网"],"title":"Socket","uri":"/posts/socket/"},{"categories":null,"content":"５、OSI模型 TCP/IP对OSI的网络模型层进行了划分如下： TCP/IP协议参考模型把所有的TCP/IP系列协议归类到四个抽象层中 应用层：TFTP，HTTP，SNMP，FTP，SMTP，DNS，Telnet 等等 传输层：TCP，UDP 网络层：IP，ICMP，OSPF，EIGRP，IGMP 数据链路层：SLIP，CSLIP，PPP，MTU 每一抽象层建立在低一层提供的服务上，并且为高一层提供服务，看起来大概是这样子的 　","date":"2022-04-05","objectID":"/posts/socket/:5:0","tags":["计网"],"title":"Socket","uri":"/posts/socket/"},{"categories":null,"content":"６、Socket常用函数接口及其原理 图解socket函数： 6.1、使用socket()函数创建套接字 int socket(int af, int type, int protocol); af 为地址族（Address Family），也就是 IP 地址类型，常用的有 AF_INET 和 AF_INET6。AF 是“Address Family”的简写，INET是“Inetnet”的简写。AF_INET 表示 IPv4 地址，例如 127.0.0.1；AF_INET6 表示 IPv6 地址，例如 1030::C9B4:FF12:48AA:1A2B。 大家需要记住127.0.0.1，它是一个特殊IP地址，表示本机地址，后面的教程会经常用到。 type 为数据传输方式，常用的有 SOCK_STREAM 和 SOCK_DGRAM protocol 表示传输协议，常用的有 IPPROTO_TCP 和 IPPTOTO_UDP，分别表示 TCP 传输协议和 UDP 传输协议 6.2、使用bind()和connect()函数 socket() 函数用来创建套接字，确定套接字的各种属性，然后服务器端要用 bind() 函数将套接字与特定的IP地址和端口绑定起来，只有这样，流经该IP地址和端口的数据才能交给套接字处理；而客户端要用 connect() 函数建立连接 int bind(int sock, struct sockaddr *addr, socklen_t addrlen); sock 为 socket 文件描述符，addr 为 sockaddr 结构体变量的指针，addrlen 为 addr 变量的大小，可由 sizeof() 计算得出 下面的代码，将创建的套接字与IP地址 127.0.0.1、端口 1234 绑定： //创建套接字 int serv_sock = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP); //创建sockaddr_in结构体变量 struct sockaddr_in serv_addr; memset(\u0026serv_addr, 0, sizeof(serv_addr)); //每个字节都用0填充 serv_addr.sin_family = AF_INET; //使用IPv4地址 serv_addr.sin_addr.s_addr = inet_addr(\"127.0.0.1\"); //具体的IP地址 serv_addr.sin_port = htons(1234); //端口 //将套接字和IP、端口绑定 bind(serv_sock, (struct sockaddr*)\u0026serv_addr, sizeof(serv_addr)); connect() 函数用来建立连接，它的原型为： int connect(int sock, struct sockaddr *serv_addr, socklen_t addrlen); 6.3、使用listen()和accept()函数 于服务器端程序，使用 bind() 绑定套接字后，还需要使用 listen() 函数让套接字进入被动监听状态，再调用 accept() 函数，就可以随时响应客户端的请求了。 通过** listen() 函数**可以让套接字进入被动监听状态，它的原型为： int listen(int sock, int backlog); sock 为需要进入监听状态的套接字，backlog 为请求队列的最大长度。 所谓被动监听，是指当没有客户端请求时，套接字处于“睡眠”状态，只有当接收到客户端请求时，套接字才会被“唤醒”来响应请求。 请求队列 当套接字正在处理客户端请求时，如果有新的请求进来，套接字是没法处理的，只能把它放进缓冲区，待当前请求处理完毕后，再从缓冲区中读取出来处理。如果不断有新的请求进来，它们就按照先后顺序在缓冲区中排队，直到缓冲区满。这个缓冲区，就称为请求队列（Request Queue）。 缓冲区的长度（能存放多少个客户端请求）可以通过 listen() 函数的 backlog 参数指定，但究竟为多少并没有什么标准，可以根据你的需求来定，并发量小的话可以是10或者20。 如果将 backlog 的值设置为 SOMAXCONN，就由系统来决定请求队列长度，这个值一般比较大，可能是几百，或者更多。 当请求队列满时，就不再接收新的请求，对于 Linux，客户端会收到 ECONNREFUSED 错误 注意：listen() 只是让套接字处于监听状态，并没有接收请求。接收请求需要使用 accept() 函数。 当套接字处于监听状态时，可以通过 accept() 函数来接收客户端请求。它的原型为： int accept(int sock, struct sockaddr *addr, socklen_t *addrlen); 它的参数与 listen() 和 connect() 是相同的：sock 为服务器端套接字，addr 为 sockaddr_in 结构体变量，addrlen 为参数 addr 的长度，可由 sizeof() 求得。 accept() 返回一个新的套接字来和客户端通信，addr 保存了客户端的IP地址和端口号，而 sock 是服务器端的套接字，大家注意区分。后面和客户端通信时，要使用这个新生成的套接字，而不是原来服务器端的套接字。 最后需要说明的是：listen() 只是让套接字进入监听状态，并没有真正接收客户端请求，listen() 后面的代码会继续执行，直到遇到 accept()。accept() 会阻塞程序执行（后面代码不能被执行），直到有新的请求到来。 6.4、socket数据的接收和发送 Linux下数据的接收和发送 Linux 不区分套接字文件和普通文件，使用 write() 可以向套接字中写入数据，使用 read() 可以从套接字中读取数据。 前面我们说过，两台计算机之间的通信相当于两个套接字之间的通信，在服务器端用 write() 向套接字写入数据，客户端就能收到，然后再使用 read() 从套接字中读取出来，就完成了一次通信。 write() 的原型为： ssize_t write(int fd, const void *buf, size_t nbytes); fd 为要写入的文件的描述符，buf 为要写入的数据的缓冲区地址，nbytes 为要写入的数据的字节数。 write() 函数会将缓冲区 buf 中的 nbytes 个字节写入文件 fd，成功则返回写入的字节数，失败则返回 -1。 read() 的原型为： ssize_t read(int fd, void *buf, size_t nbytes); fd 为要读取的文件的描述符，buf 为要接收数据的缓冲区地址，nbytes 为要读取的数据的字节数。 read() 函数会从 fd 文件中读取 nbytes 个字节并保存到缓冲区 buf，成功则返回读取到的字节数（但遇到文件结尾则返回0），失败则返回 -1。 6.5、socket缓冲区以及阻塞模式 socket缓冲区 每个 socket 被创建后，都会分配两个缓冲区，输入缓冲区和输出缓冲区。 write()/send() 并不立即向网络中传输数据，而是先将数据写入缓冲区中，再由TCP协议将数据从缓冲区发送到目标机器。一旦将数据写入到缓冲区，函数就可以成功返回，不管它们有没有到达目标机器，也不管它们何时被发送到网络，这些都是TCP协议负责的事情。 TCP协议独立于 write()/send() 函数，数据有可能刚被写入缓冲区就发送到网络，也可能在缓冲区中不断积压，多次写入的数据被一次性发送到网络，这取决于当时的网络情况、当前线程是否空闲等诸多因素，不由程序员控制。 read()/recv() 函数也是如此，也从输入缓冲区中读取数据，而不是直接从网络中读取 这些I/O缓冲区特性可整理如下： （1）I/O缓冲区在每个TCP套接字中单独存在； （2）I/O缓冲区在创建套接字时自动生成； （3）即使关闭套接字也会继续传送输出缓冲区中遗留的数据； （4）关闭套接字将丢失输入缓冲区中的数据。 输入输出缓冲区的默认大小一般都是 8K，可以通过 getsockopt() 函数获取： unsigned optVal; int optLen = sizeof(int); getsockopt(servSock, SOL_SOCKET, SO_SNDBUF, (char*)\u0026optVal, \u0026optLen); printf(\"Buffer length: %d\\n\", optVal); 阻塞模式 对于TCP套接字（默认情况下），当使用 write()/send() 发送数据时： 1) 首先会检查缓冲区，如果缓冲区的可用空间长度小于要发送的数据，那么 write()/send() 会被阻塞（暂停执行），直到缓冲区中的数据被发送到目标机器，腾出足够的空间，才唤醒 write()/send() 函数继续写入数据。 2) 如果TCP协议正在向网络发送数据，那么输出缓冲区会被锁定，不允许写入，write()/send() 也会被阻塞，直到数据发送完毕缓冲区解锁，write()/send() 才会被唤醒。 3) 如果要写入的数据大于缓冲区的最大长度，那么将分批写入。 4) 直到所有数据被写入缓冲区 write()/send() 才能返回。 当使用 read()/recv() 读取数据时： 1) 首先会检查缓冲区，如果","date":"2022-04-05","objectID":"/posts/socket/:6:0","tags":["计网"],"title":"Socket","uri":"/posts/socket/"},{"categories":null,"content":"SQL语句：先编译后执行 存储过程(Stored Procedure)： 　一组可编程的函数，是为了完成特定功能的SQL语句集，经编译创建并保存在数据库中，用户可通过指定存储过程的名字并给定参数(需要时)来调用执行。 优点(为什么要用存储过程？)： 　①将重复性很高的一些操作，封装到一个存储过程中，简化了对这些SQL的调用 　②批量处理：SQL+循环，减少流量，也就是“跑批” 　③统一接口，确保数据的安全 相对于oracle数据库来说，MySQL的存储过程相对功能较弱，使用较少。 一、存储过程的创建和调用 　\u003e存储过程就是具有名字的一段代码，用来完成一个特定的功能。 　\u003e创建的存储过程保存在数据库的数据字典中。 ","date":"2022-04-05","objectID":"/posts/middleware/mysql/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/:0:0","tags":["数据库"],"title":"存储过程","uri":"/posts/middleware/mysql/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/"},{"categories":null,"content":"1、创建存储过程 CREATE[DEFINER={user|CURRENT_USER}]PROCEDUREsp_name([proc_parameter[,...]])[characteristic...]routine_bodyproc_parameter:[IN|OUT|INOUT]param_nametypecharacteristic:COMMENT'string'|LANGUAGESQL|[NOT]DETERMINISTIC|{CONTAINSSQL|NOSQL|READSSQLDATA|MODIFIESSQLDATA}|SQLSECURITY{DEFINER|INVOKER}routine_body:ValidSQLroutinestatement[begin_label:]BEGIN[statement_list]……END[end_label] #创建数据库，备份数据表用于示例操作 mysql\u003ecreatedatabasedb1;mysql\u003eusedb1;mysql\u003ecreatetablePLAYERSasselect*fromTENNIS.PLAYERS;mysql\u003ecreatetableMATCHESasselect*fromTENNIS.MATCHES;示例：创建一个存储过程，删除给定球员参加的所有比赛 mysql\u003edelimiter$$#将语句的结束符号从分号;临时改为两个$$(可以是自定义)mysql\u003eCREATEPROCEDUREdelete_matches(INp_playernoINTEGER)-\u003eBEGIN-\u003eDELETEFROMMATCHES-\u003eWHEREplayerno=p_playerno;-\u003eEND$$QueryOK,0rowsaffected(0.01sec)mysql\u003edelimiter;#将语句的结束符号恢复为分号解析： 　默认情况下，存储过程和默认数据库相关联，如果想指定存储过程创建在某个特定的数据库下，那么在过程名前面加数据库名做前缀； 　在定义过程时，使用DELIMITER $$ 命令将语句的结束符号从分号 ; 临时改为两个$$，使得过程体中使用的分号被直接传递到服务器，而不会被客户端（如mysql）解释。 ","date":"2022-04-05","objectID":"/posts/middleware/mysql/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/:1:0","tags":["数据库"],"title":"存储过程","uri":"/posts/middleware/mysql/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/"},{"categories":null,"content":"2、调用存储过程：call sp_name[(传参)]; mysql\u003eselect*fromMATCHES;+---------+--------+----------+-----+------+ |MATCHNO|TEAMNO|PLAYERNO|WON|LOST|+---------+--------+----------+-----+------+ |1|1|6|3|1||7|1|57|3|0||8|1|8|0|3||9|2|27|3|2||11|2|112|2|3|+---------+--------+----------+-----+------+ 5rowsinset(0.00sec)mysql\u003ecalldelete_matches(57);QueryOK,1rowaffected(0.03sec)mysql\u003eselect*fromMATCHES;+---------+--------+----------+-----+------+ |MATCHNO|TEAMNO|PLAYERNO|WON|LOST|+---------+--------+----------+-----+------+ |1|1|6|3|1||8|1|8|0|3||9|2|27|3|2||11|2|112|2|3|+---------+--------+----------+-----+------+ 4rowsinset(0.00sec)解析： 　在存储过程中设置了需要传参的变量p_playerno，调用存储过程的时候，通过传参将57赋值给p_playerno，然后进行存储过程里的SQL操作。 ","date":"2022-04-05","objectID":"/posts/middleware/mysql/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/:2:0","tags":["数据库"],"title":"存储过程","uri":"/posts/middleware/mysql/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/"},{"categories":null,"content":"3、存储过程体 　\u003e存储过程体包含了在过程调用时必须执行的语句，例如：dml、ddl语句，if-then-else和while-do语句、声明变量的declare语句等 　\u003e过程体格式：以begin开始，以end结束(可嵌套) BEGINBEGINBEGINstatements;ENDENDEND注意：每个嵌套块及其中的每条语句，必须以分号结束，表示过程体结束的begin-end块(又叫做复合语句compound statement)，则不需要分号。 ","date":"2022-04-05","objectID":"/posts/middleware/mysql/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/:3:0","tags":["数据库"],"title":"存储过程","uri":"/posts/middleware/mysql/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/"},{"categories":null,"content":"4、为语句块贴标签 [begin_label:]BEGIN[statement_list]END[end_label]例如：label1:BEGINlabel2:BEGINlabel3:BEGINstatements;ENDlabel3;ENDlabel2;ENDlabel1标签有两个作用： 　①增强代码的可读性 　②在某些语句(例如:leave和iterate语句)，需要用到标签 二、存储过程的参数 　存储过程可以有0个或多个参数，用于存储过程的定义。 3种参数类型： 　IN输入参数：表示调用者向过程传入值（传入值可以是字面量或变量） 　OUT输出参数：表示过程向调用者传出值(可以返回多个值)（传出值只能是变量） 　INOUT输入输出参数：既表示调用者向过程传入值，又表示过程向调用者传出值（值只能是变量） ","date":"2022-04-05","objectID":"/posts/middleware/mysql/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/:4:0","tags":["数据库"],"title":"存储过程","uri":"/posts/middleware/mysql/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/"},{"categories":null,"content":"1、in输入参数 mysql\u003edelimiter$$mysql\u003ecreateprocedurein_param(inp_inint)-\u003ebegin-\u003eselectp_in;-\u003esetp_in=2;-\u003eselectP_in;-\u003eend$$mysql\u003edelimiter;mysql\u003eset@p_in=1;mysql\u003ecallin_param(@p_in);+------+ |p_in|+------+ |1|+------+ +------+ |P_in|+------+ |2|+------+ mysql\u003e select @p_in; +-------+ |@p_in|+-------+ |1|+-------+ 以上可以看出，p_in在存储过程中被修改，但并不影响@p_id的值，因为前者为局部变量、后者为全局变量。 ","date":"2022-04-05","objectID":"/posts/middleware/mysql/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/:5:0","tags":["数据库"],"title":"存储过程","uri":"/posts/middleware/mysql/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/"},{"categories":null,"content":"2、out输出参数 mysql\u003edelimiter//mysql\u003ecreateprocedureout_param(outp_outint)-\u003ebegin-\u003eselectp_out;-\u003esetp_out=2;-\u003eselectp_out;-\u003eend-\u003e//mysql\u003edelimiter;mysql\u003eset@p_out=1;mysql\u003ecallout_param(@p_out);+-------+ |p_out|+-------+ |NULL|+-------+ #因为out是向调用者输出参数，不接收输入的参数，所以存储过程里的p_out为null +-------+ |p_out|+-------+ |2|+-------+ mysql\u003e select @p_out; +--------+ |@p_out|+--------+ |2|+--------+ #调用了out_param存储过程，输出参数，改变了p_out变量的值 ","date":"2022-04-05","objectID":"/posts/middleware/mysql/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/:6:0","tags":["数据库"],"title":"存储过程","uri":"/posts/middleware/mysql/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/"},{"categories":null,"content":"3、inout输入参数 mysql\u003edelimiter$$mysql\u003ecreateprocedureinout_param(inoutp_inoutint)-\u003ebegin-\u003eselectp_inout;-\u003esetp_inout=2;-\u003eselectp_inout;-\u003eend-\u003e$$mysql\u003edelimiter;mysql\u003eset@p_inout=1;mysql\u003ecallinout_param(@p_inout);+---------+ |p_inout|+---------+ |1|+---------+ +---------+ |p_inout|+---------+ |2|+---------+ mysql\u003e select @p_inout; +----------+ |@p_inout|+----------+ |2|+----------+ 调用了inout_param存储过程，接受了输入的参数，也输出参数，改变了变量 注意： 　①如果过程没有参数，也必须在过程名后面写上小括号 　例：CREATE PROCEDURE sp_name ([proc_parameter[,…]]) …… 　②确保参数的名字不等于列的名字，否则在过程体中，参数名被当做列名来处理 墙裂建议： 　\u003e输入值使用in参数； 　\u003e返回值使用out参数； 　\u003einout参数就尽量的少用。 ref https://www.cnblogs.com/geaozhang/p/6797357.html ","date":"2022-04-05","objectID":"/posts/middleware/mysql/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/:7:0","tags":["数据库"],"title":"存储过程","uri":"/posts/middleware/mysql/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B/"},{"categories":null,"content":"软件测试中，最常听到“黑盒测试”与“白盒测试”，它们是软件测试中最基本的测试方法。 　那么究竟何为“黑盒”，何为“白盒”呢？下面就对其概念与常用方法进行一下介绍。 黑盒测试，白盒测试与灰盒测试的比较和区别如下所示： 黑盒测试 白盒测试 灰盒测试 不需要了解内部工作结构(代码)。测试用例只需要GUI(图形用户界面)。 测试需要了解内部工作结构(软件编码)。 部分了解内部工作结构。 黑盒测试也称为功能测试，数据驱动测试和封闭盒测试。 白盒测试也称为结构测试，透明盒测试，基于代码的测试和透明测试。 灰盒测试也称为半透明测试，因为测试人员对编码知识有限。 测试方法包括试验技术和错误猜测方法，因为测试人员不需要知道软件的内部编码。 通过验证软件中固有的系统边界和数据域来进行白盒测试，因为不缺乏内部编码知识。 如果测试人员具有编码知识，则通过验证软件的数据域和内部系统边界来进行。 输入表的测试空间(用于创建测试用例的输入)非常庞大，在所有测试空间中最大。 与黑盒测试相比，输入表的测试空间(用于创建测试用例的输入)较少。 输入表的测试空间(用于创建测试用例的输入)小于黑盒和白盒测试。 发现软件的隐藏错误非常困难，因为错误可能是由于黑盒测试未知的内部工作造成的。 发现隐藏错误很简单，因为它可能是由于内部工作，这在白盒测试中得到了深入探索。 很难发现隐藏的错误，可在用户级测试中找到。 它不适用于算法测试。 它非常适合并推荐用于算法测试。 它不被考虑用于算法测试。 黑盒测试中的时间消耗取决于功能规范的可用性。 由于冗长的代码，白盒测试需要很长时间来设计测试用例。 测试用例设计可以在短时间内完成。 测试人员，开发人员和最终用户可以参与测试。 只有测试人员和开发人员才能参与测试; 最终用户不能涉及。 测试人员，开发人员和最终用户可以参与测试。 这是所有测试过程中耗时最少的过程。 在所有测试过程中，整个测试过程是最耗时的。 比白盒测试耗时更少。 黑盒测试涵盖了抵御病毒攻击的弹性和安全性。 白盒测试不包括针对病毒攻击的弹性和安全性。 灰盒测试不包括针对病毒攻击的弹性和安全性。 黑盒测试的基础是外部期望内部行为未知。 灰盒测试的基础是编码，负责内部工作。 基于高级数据库图表和数据流图进行测试。 它不像白盒和灰盒测试方法那么详尽。 黑盒和灰盒测试方法之间最为详尽。 部分详尽; 取决于基于编码或基于GUI的测试用例的类型。 ","date":"2022-03-30","objectID":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/:0:0","tags":null,"title":"黑盒测试白盒测试","uri":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":"黑盒测试： 　也称功能测试、数据驱动测试，它将被测软件看作一个打不开的黑盒，主要根据功能需求设计测试用例，进行测试。 　概念：黑盒测试是从一种从软件外部对软件实施的测试，也称功能测试或基于规格说明的测试。其基本观点是：任何程序都可以看作是从输入定义域到输出值域的映射，这种观点将被测程序看作一个打不开的黑盒，黑盒里面的内容(实现)是完全不知道的，只知道软件要做什么。因无法看到盒子中的内容，所以不知道软件是如何实现的，也不关心黑盒里面的结构，只关心软件的输入数据和输出结果。 　检测软件功能能否按照需求规格说明书的规定正常工作，是否有功能遗漏； 　检测是否有人机交互错误，是否有数据结构和外部数据库访问错误，是否能恰当地接收数据并保持外部信息（如数据库或文件）等的完整性； 　检测行为、性能等特性是否满足要求等； 检测程序初始化和终止方面的错误等。 　优点： 　① 与软件具体实现无关，如果软件实现发生了变化，测试用例仍可用； 　② 设计黑盒测试用例可以和软件实现同时进行，因此可压缩项目总开发时间。 ","date":"2022-03-30","objectID":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/:1:0","tags":null,"title":"黑盒测试白盒测试","uri":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":"黑盒测试常用方法 　等价类划分 　边界值分析 　因果图 　决策表分析 ","date":"2022-03-30","objectID":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/:1:1","tags":null,"title":"黑盒测试白盒测试","uri":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":"等价类划分 　完全不考虑程序的内部结构，只根据程序规格说明书对输入范围进行划分，把所有可能的输入数据，即程序输入域划分为若干个互不相交的子集，称为等价类，然后从每个等价类中选取少数具有代表性的数据作为测试用例，进行测试。 　划分原则：区间、数值、数值集合、限制条件或规则、细分等价类 ","date":"2022-03-30","objectID":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/:1:2","tags":null,"title":"黑盒测试白盒测试","uri":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":"边界值分析 　边界值和等价类密切相关，输入等价类和输出等价类的边界是要着重测试的边界情况。在等价类的划分过程中产生了许多等价类边界。边界是最容易出错的地方，所以，从等价类中选取测试数据时应该关注边界值。 　在等价类划分基础上进行边界值分析测试的基本思想是，选取正好等于、刚刚大于或刚刚小于等价类边界的值作为测试数据，而不是选取等价类中的典型值或任意值做为测试数据。 　对于一个n变量的程序，边界值分析测试会产生4n+1个测试用例。 ","date":"2022-03-30","objectID":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/:1:3","tags":null,"title":"黑盒测试白盒测试","uri":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":"因果图 　（1）确定软件规格中的原因和结果。分析规格说明中哪些是原因（即输入条件或输入条件的等价类），哪些是结果（即输出条件），并给每个原因和结果赋予一个标识符。 　（2）确定原因和结果之间的逻辑关系。分析软件规格说明中的语义，找出原因与结果之间、原因与原因之间对应的关系，根据这些关系画出因果图。 　（3）确定因果图中的各个约束。由于语法或环境的限制，有些原因与原因之间、原因与结果之间的组合情况不可能出现。为表明这些特殊情况，在因果图上用一些记号表明约束或限制条件。 　（4）把因果图转换为决策表。 　（5）根据决策表设计测试用例。 ","date":"2022-03-30","objectID":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/:1:4","tags":null,"title":"黑盒测试白盒测试","uri":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":"决策表分析 　在所有的黑盒测试方法中，基于决策表的测试是最严格，最具有逻辑性的测试方法。 　决策表是把作为条件的所有输入的各种组合值以及对应输出值都罗列出来而形成的表格。 　它能够将复杂的问题按照各种可能的情况全部列举出来，简明并避免遗漏。因此，利用决策表能够设计出完整的测试用例集合。 ","date":"2022-03-30","objectID":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/:1:5","tags":null,"title":"黑盒测试白盒测试","uri":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":"步骤： 　（1）列出所有的条件桩和动作桩。 　（2）确定规则的个数。 　（3）填入条件项。 　（4）填入动作项，得到初始决策表。 　（5）简化决策表，合并相似规则。 　对于n个条件的决策表，相应有2n个规则 　决策表合并原则：即若表中有两条以上规则具有相同的动作，并且在条件项之间存在极为相似的关系，便可以合并。 ","date":"2022-03-30","objectID":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/:1:6","tags":null,"title":"黑盒测试白盒测试","uri":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":"白盒测试： 　也称结构测试或逻辑驱动测试，它是知道产品内部工作过程，可通过测试来检测产品内部动作是否按照规格说明书的规定正常进行，按照程序内部的结构测试程序，检验程序中的每条通路是否都有能按预定要求正确工作，而不顾它的功能。 ","date":"2022-03-30","objectID":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/:2:0","tags":null,"title":"黑盒测试白盒测试","uri":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":"白盒测试常用方法 　基本覆盖标准：逻辑驱动、循环、基路测试等，主要用于软件验证。 　“白盒”法全面了解程序内部逻辑结构、对所有逻辑路径进行测试。 　“白盒”法是穷举路径测试。在使用这一方案时，测试者必须检查程序的内部结构，从检查程序的逻辑着手，得出测试数据。贯穿程序的独立路径数是天文数字。但即使每条路径都测试了仍然可能有错误。 　原因： 　第一，穷举路径测试决不能查出程序违反了设计规范，即程序本身是个错误的程序。 　第二，穷举路径测试不可能查出程序中因遗漏路径而出错。 　第三，穷举路径测试可能发现不了一些与数据相关的错误。 ","date":"2022-03-30","objectID":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/:2:1","tags":null,"title":"黑盒测试白盒测试","uri":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":"白盒测试逻辑驱动方法 　语句覆盖 　判定覆盖 　条件覆盖 　判定/条件覆盖 　条件组合覆盖 　路径覆盖 ","date":"2022-03-30","objectID":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/:2:2","tags":null,"title":"黑盒测试白盒测试","uri":"/posts/%E9%BB%91%E7%9B%92%E6%B5%8B%E8%AF%95%E7%99%BD%E7%9B%92%E6%B5%8B%E8%AF%95/"},{"categories":null,"content":"0、算法概述 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:1:0","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"0.1 算法分类 十种常见排序算法可以分为两大类： 比较类排序：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破O(nlogn)，因此也称为非线性时间比较类排序。 非比较类排序：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，以线性时间运行，因此也称为线性时间非比较类排序。  ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:1:1","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"0.2 算法复杂度 0.3 相关概念 稳定：如果a原本在b前面，而a=b，排序之后a仍然在b的前面。 不稳定：如果a原本在b的前面，而a=b，排序之后 a 可能会出现在 b 的后面。 时间复杂度：对排序数据的总的操作次数。反映当n变化时，操作次数呈现什么规律。 **空间复杂度：**是指算法在计算机 内执行时所需存储空间的度量，它也是数据规模n的函数。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:1:2","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"1、冒泡排序（Bubble Sort） 冒泡排序是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:2:0","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"1.1 算法描述 比较相邻的元素。如果第一个比第二个大，就交换它们两个； 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数； 针对所有的元素重复以上的步骤，除了最后一个； 重复步骤1~3，直到排序完成。 1.2 动图演示 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:2:1","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"1.3 代码实现 function bubbleSort(arr) { var len = arr.length; for (var i = 0; i \u003c len - 1; i++) { for (var j = 0; j \u003c len - 1 - i; j++) { if (arr[j] \u003e arr[j+1]) { // 相邻元素两两对比 var temp = arr[j+1]; // 元素交换 arr[j+1] = arr[j]; arr[j] = temp; } } } return arr; } ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:2:2","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"2、选择排序（Selection Sort） 选择排序(Selection-sort)是一种简单直观的排序算法。它的工作原理：首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:3:0","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"2.1 算法描述 n个记录的直接选择排序可经过n-1趟直接选择排序得到有序结果。具体算法描述如下： 初始状态：无序区为R[1..n]，有序区为空； 第i趟排序(i=1,2,3…n-1)开始时，当前有序区和无序区分别为R[1..i-1]和R(i..n）。该趟排序从当前无序区中-选出关键字最小的记录 R[k]，将它与无序区的第1个记录R交换，使R[1..i]和R[i+1..n)分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区； n-1趟结束，数组有序化了。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:3:1","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"2.2 动图演示 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:3:2","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"2.3 代码实现 function selectionSort(arr) { var len = arr.length; var minIndex, temp; for (var i = 0; i \u003c len - 1; i++) { minIndex = i; for (var j = i + 1; j \u003c len; j++) { if (arr[j] \u003c arr[minIndex]) { // 寻找最小的数 minIndex = j; // 将最小数的索引保存 } } temp = arr[i]; arr[i] = arr[minIndex]; arr[minIndex] = temp; } return arr; } ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:3:3","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"2.4 算法分析 表现最稳定的排序算法之一，因为无论什么数据进去都是O(n2)的时间复杂度，所以用到它的时候，数据规模越小越好。唯一的好处可能就是不占用额外的内存空间了吧。理论上讲，选择排序可能也是平时排序一般人想到的最多的排序方法了吧。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:3:4","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"3、插入排序（Insertion Sort） 插入排序（Insertion-Sort）的算法描述是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:4:0","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"3.1 算法描述 一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下： 从第一个元素开始，该元素可以认为已经被排序； 取出下一个元素，在已经排序的元素序列中从后向前扫描； 如果该元素（已排序）大于新元素，将该元素移到下一位置； 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置； 将新元素插入到该位置后； 重复步骤2~5。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:4:1","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"3.2 动图演示 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:4:2","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"3.2 代码实现 function insertionSort(arr) { var len = arr.length; var preIndex, current; for (var i = 1; i \u003c len; i++) { preIndex = i - 1; current = arr[i]; while (preIndex \u003e= 0 \u0026\u0026 arr[preIndex] \u003e current) { arr[preIndex + 1] = arr[preIndex]; preIndex--; } arr[preIndex + 1] = current; } return arr; } ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:4:3","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"3.4 算法分析 插入排序在实现上，通常采用in-place排序（即只需用到O(1)的额外空间的排序），因而在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:4:4","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"4、希尔排序（Shell Sort） 1959年Shell发明，第一个突破O(n2)的排序算法，是简单插入排序的改进版。它与插入排序的不同之处在于，它会优先比较距离较远的元素。希尔排序又叫缩小增量排序。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:5:0","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"4.1 算法描述 先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述： 选择一个增量序列t1，t2，…，tk，其中ti\u003etj，tk=1； 按增量序列个数k，对序列进行k 趟排序； 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:5:1","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"4.2 动图演示 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:5:2","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"4.3 代码实现 // 修改于 2019-03-06 function shellSort(arr) { var len = arr.length; for (var gap = Math.floor(len / 2); gap \u003e 0; gap = Math.floor(gap / 2)) { // 注意：这里和动图演示的不一样，动图是分组执行，实际操作是多个分组交替执行 for (var i = gap; i \u003c len; i++) { var j = i; var current = arr[i]; while (j - gap \u003e= 0 \u0026\u0026 current \u003c arr[j - gap]) { arr[j] = arr[j - gap]; j = j - gap; } arr[j] = current; } } return arr; } ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:5:3","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"4.4 算法分析 希尔排序的核心在于间隔序列的设定。既可以提前设定好间隔序列，也可以动态的定义间隔序列。动态定义间隔序列的算法是《算法（第4版）》的合著者Robert Sedgewick提出的。　","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:5:4","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"5、归并排序（Merge Sort） 归并排序是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:6:0","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"5.1 算法描述 把长度为n的输入序列分成两个长度为n/2的子序列； 对这两个子序列分别采用归并排序； 将两个排序好的子序列合并成一个最终的排序序列。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:6:1","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"5.2 动图演示 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:6:2","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"5.3 代码实现 function mergeSort(arr) { var len = arr.length; if (len \u003c 2) { return arr; } var middle = Math.floor(len / 2), left = arr.slice(0, middle), right = arr.slice(middle); return merge(mergeSort(left), mergeSort(right)); } function merge(left, right) { var result = []; while (left.length\u003e0 \u0026\u0026 right.length\u003e0) { if (left[0] \u003c= right[0]) { result.push(left.shift()); } else { result.push(right.shift()); } } while (left.length) result.push(left.shift()); while (right.length) result.push(right.shift()); return result; } ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:6:3","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"5.4 算法分析 归并排序是一种稳定的排序方法。和选择排序一样，归并排序的性能不受输入数据的影响，但表现比选择排序好的多，因为始终都是O(nlogn）的时间复杂度。代价是需要额外的内存空间。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:6:4","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"6、快速排序（Quick Sort） 快速排序的基本思想：通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:7:0","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"6.1 算法描述 快速排序使用分治法来把一个串（list）分为两个子串（sub-lists）。具体算法描述如下： 从数列中挑出一个元素，称为 “基准”（pivot）； 重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区退出之后，该基准就处于数列的中间位置。这个称为分区（partition）操作； 递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:7:1","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"6.2 动图演示 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:7:2","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"6.3 代码实现 function quickSort(arr, left, right) { var len = arr.length, partitionIndex, left = typeof left != 'number' ? 0 : left, right = typeof right != 'number' ? len - 1 : right; if (left \u003c right) { partitionIndex = partition(arr, left, right); quickSort(arr, left, partitionIndex-1); quickSort(arr, partitionIndex+1, right); } return arr; } function partition(arr, left ,right) { // 分区操作 var pivot = left, // 设定基准值（pivot） index = pivot + 1; for (var i = index; i \u003c= right; i++) { if (arr[i] \u003c arr[pivot]) { swap(arr, i, index); index++; } } swap(arr, pivot, index - 1); return index-1; } function swap(arr, i, j) { var temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:7:3","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"7、堆排序（Heap Sort） 堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:8:0","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"7.1 算法描述 将初始待排序关键字序列(R1,R2….Rn)构建成大顶堆，此堆为初始的无序区； 将堆顶元素R[1]与最后一个元素R[n]交换，此时得到新的无序区(R1,R2,……Rn-1)和新的有序区(Rn),且满足R[1,2…n-1]\u003c=R[n]； 由于交换后新的堆顶R[1]可能违反堆的性质，因此需要对当前无序区(R1,R2,……Rn-1)调整为新堆，然后再次将R[1]与无序区最后一个元素交换，得到新的无序区(R1,R2….Rn-2)和新的有序区(Rn-1,Rn)。不断重复此过程直到有序区的元素个数为n-1，则整个排序过程完成。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:8:1","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"7.2 动图演示 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:8:2","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"7.3 代码实现 var len; // 因为声明的多个函数都需要数据长度，所以把len设置成为全局变量 function buildMaxHeap(arr) { // 建立大顶堆 len = arr.length; for (var i = Math.floor(len/2); i \u003e= 0; i--) { heapify(arr, i); } } function heapify(arr, i) { // 堆调整 var left = 2 * i + 1, right = 2 * i + 2, largest = i; if (left \u003c len \u0026\u0026 arr[left] \u003e arr[largest]) { largest = left; } if (right \u003c len \u0026\u0026 arr[right] \u003e arr[largest]) { largest = right; } if (largest != i) { swap(arr, i, largest); heapify(arr, largest); } } function swap(arr, i, j) { var temp = arr[i]; arr[i] = arr[j]; arr[j] = temp; } function heapSort(arr) { buildMaxHeap(arr); for (var i = arr.length - 1; i \u003e 0; i--) { swap(arr, 0, i); len--; heapify(arr, 0); } return arr; } ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:8:3","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"8、计数排序（Counting Sort） 计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:9:0","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"8.1 算法描述 找出待排序的数组中最大和最小的元素； 统计数组中每个值为i的元素出现的次数，存入数组C的第i项； 对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）； 反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:9:1","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"8.2 动图演示 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:9:2","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"8.3 代码实现 function countingSort(arr, maxValue) { var bucket = new Array(maxValue + 1), sortedIndex = 0; arrLen = arr.length, bucketLen = maxValue + 1; for (var i = 0; i \u003c arrLen; i++) { if (!bucket[arr[i]]) { bucket[arr[i]] = 0; } bucket[arr[i]]++; } for (var j = 0; j \u003c bucketLen; j++) { while(bucket[j] \u003e 0) { arr[sortedIndex++] = j; bucket[j]--; } } return arr; } ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:9:3","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"8.4 算法分析 计数排序是一个稳定的排序算法。当输入的元素是 n 个 0到 k 之间的整数时，时间复杂度是O(n+k)，空间复杂度也是O(n+k)，其排序速度快于任何比较排序算法。当k不是很大并且序列比较集中时，计数排序是一个很有效的排序算法。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:9:4","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"9、桶排序（Bucket Sort） 桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。桶排序 (Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排）。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:10:0","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"9.1 算法描述 设置一个定量的数组当作空桶； 遍历输入数据，并且把数据一个一个放到对应的桶里去； 对每个不是空的桶进行排序； 从不是空的桶里把排好序的数据拼接起来。  ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:10:1","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"9.2 图片演示 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:10:2","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"9.3 代码实现 function bucketSort(arr, bucketSize) { if (arr.length === 0) { return arr; } var i; var minValue = arr[0]; var maxValue = arr[0]; for (i = 1; i \u003c arr.length; i++) { if (arr[i] \u003c minValue) { minValue = arr[i]; // 输入数据的最小值 } else if (arr[i] \u003e maxValue) { maxValue = arr[i]; // 输入数据的最大值 } } // 桶的初始化 var DEFAULT_BUCKET_SIZE = 5; // 设置桶的默认数量为5 bucketSize = bucketSize || DEFAULT_BUCKET_SIZE; var bucketCount = Math.floor((maxValue - minValue) / bucketSize) + 1; var buckets = new Array(bucketCount); for (i = 0; i \u003c buckets.length; i++) { buckets[i] = []; } // 利用映射函数将数据分配到各个桶中 for (i = 0; i \u003c arr.length; i++) { buckets[Math.floor((arr[i] - minValue) / bucketSize)].push(arr[i]); } arr.length = 0; for (i = 0; i \u003c buckets.length; i++) { insertionSort(buckets[i]); // 对每个桶进行排序，这里使用了插入排序 for (var j = 0; j \u003c buckets[i].length; j++) { arr.push(buckets[i][j]); } } return arr; } ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:10:3","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"9.4 算法分析 桶排序最好情况下使用线性时间O(n)，桶排序的时间复杂度，取决与对各个桶之间数据进行排序的时间复杂度，因为其它部分的时间复杂度都为O(n)。很显然，桶划分的越小，各个桶之间的数据越少，排序所用的时间也会越少。但相应的空间消耗就会增大。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:10:4","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"10、基数排序（Radix Sort） 基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:11:0","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"10.1 算法描述 取得数组中的最大数，并取得位数； arr为原始数组，从最低位开始取每个位组成radix数组； 对radix进行计数排序（利用计数排序适用于小范围数的特点）； ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:11:1","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"10.2 动图演示 ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:11:2","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"10.3 代码实现 var counter = []; function radixSort(arr, maxDigit) { var mod = 10; var dev = 1; for (var i = 0; i \u003c maxDigit; i++, dev *= 10, mod *= 10) { for(var j = 0; j \u003c arr.length; j++) { var bucket = parseInt((arr[j] % mod) / dev); if(counter[bucket]==null) { counter[bucket] = []; } counter[bucket].push(arr[j]); } var pos = 0; for(var j = 0; j \u003c counter.length; j++) { var value = null; if(counter[j]!=null) { while ((value = counter[j].shift()) != null) { arr[pos++] = value; } } } } return arr; } ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:11:3","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"10.4 算法分析 基数排序基于分别排序，分别收集，所以是稳定的。但基数排序的性能比桶排序要略差，每一次关键字的桶分配都需要O(n)的时间复杂度，而且分配之后得到新的关键字序列又需要O(n)的时间复杂度。假如待排数据可以分为d个关键字，则基数排序的时间复杂度将是O(d*2n) ，当然d要远远小于n，因此基本上还是线性级别的。 基数排序的空间复杂度为O(n+k)，其中k为桶的数量。一般来说n»k，因此额外空间需要大概n个左右。 ref https://www.cnblogs.com/onepixel/articles/7674659.html ","date":"2022-03-30","objectID":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:11:4","tags":null,"title":"排序算法","uri":"/posts/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":null,"content":"1 重点概念 ","date":"2022-03-30","objectID":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/:1:0","tags":null,"title":"二叉树","uri":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":null,"content":"1.1 结点概念 结点是数据结构中的基础，是构成复杂数据结构的基本组成单位。 ","date":"2022-03-30","objectID":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/:1:1","tags":null,"title":"二叉树","uri":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":null,"content":"1.2 树结点声明 本系列文章中提及的结点专指树的结点。例如：结点A在图中表示为： ","date":"2022-03-30","objectID":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/:1:2","tags":null,"title":"二叉树","uri":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":null,"content":"2 树 ","date":"2022-03-30","objectID":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/:2:0","tags":null,"title":"二叉树","uri":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":null,"content":"2.1 定义 **树（Tree）**是n（n\u003e=0)个结点的有限集。n=0时称为空树。在任意一颗非空树中： 1）有且仅有一个特定的称为根（Root）的结点； 2）当n\u003e1时，其余结点可分为m(m\u003e0)个互不相交的有限集T1、T2、……、Tn，其中每一个集合本身又是一棵树，并且称为根的子树。 此外，树的定义还需要强调以下两点： 1）n\u003e0时根结点是唯一的，不可能存在多个根结点，数据结构中的树只能有一个根结点。 2）m\u003e0时，子树的个数没有限制，但它们一定是互不相交的。 示例树： 图2.1为一棵普通的树： 图2.1 普通树 由树的定义可以看出，树的定义使用了递归的方式。递归在树的学习过程中起着重要作用，如果对于递归不是十分了解，建议先看看递归算法 ","date":"2022-03-30","objectID":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/:2:1","tags":null,"title":"二叉树","uri":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":null,"content":"2.2 结点的度 结点拥有的子树数目称为结点的度。 图2.2中标注了图2.1所示树的各个结点的度。 图2.2 度示意图 2.3 结点关系 结点子树的根结点为该结点的孩子结点。相应该结点称为孩子结点的双亲结点。 图2.2中，A为B的双亲结点，B为A的孩子结点。 同一个双亲结点的孩子结点之间互称兄弟结点。 图2.2中，结点B与结点C互为兄弟结点。 2.4 结点层次 从根开始定义起，根为第一层，根的孩子为第二层，以此类推。 图2.3表示了图2.1所示树的层次关系 图2.3 层示意图 2.5 树的深度 树中结点的最大层次数称为树的深度或高度。图2.1所示树的深度为4。 ","date":"2022-03-30","objectID":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/:2:2","tags":null,"title":"二叉树","uri":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":null,"content":"3 二叉树 ","date":"2022-03-30","objectID":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/:3:0","tags":null,"title":"二叉树","uri":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":null,"content":"3.1 定义 二叉树是n(n\u003e=0)个结点的有限集合，该集合或者为空集（称为空二叉树），或者由一个根结点和两棵互不相交的、分别称为根结点的左子树和右子树组成。 ","date":"2022-03-30","objectID":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/:3:1","tags":null,"title":"二叉树","uri":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":null,"content":"3.2 二叉树特点 由二叉树定义以及图示分析得出二叉树有以下特点： 1）每个结点最多有两颗子树，所以二叉树中不存在度大于2的结点。 2）左子树和右子树是有顺序的，次序不能任意颠倒。 3）即使树中某结点只有一棵子树，也要区分它是左子树还是右子树。 ","date":"2022-03-30","objectID":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/:3:2","tags":null,"title":"二叉树","uri":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":null,"content":"3.3 二叉树性质 1）在二叉树的第i层上最多有2i-1 个节点 。（i\u003e=1） 2）二叉树中如果深度为k,那么最多有2k-1个节点。(k\u003e=1） 3）n0=n2+1 n0表示度数为0的节点数，n2表示度数为2的节点数。 4）在完全二叉树中，具有n个节点的完全二叉树的深度为[log2n]+1，其中[log2n]是向下取整。 5）若对含 n 个结点的完全二叉树从上到下且从左至右进行 1 至 n 的编号，则对完全二叉树中任意一个编号为 i 的结点有如下特性： (1) 若 i=1，则该结点是二叉树的根，无双亲, 否则，编号为 [i/2] 的结点为其双亲结点; (2) 若 2i\u003en，则该结点无左孩子， 否则，编号为 2i 的结点为其左孩子结点； (3) 若 2i+1\u003en，则该结点无右孩子结点， 否则，编号为2i+1 的结点为其右孩子结点。 ","date":"2022-03-30","objectID":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/:3:3","tags":null,"title":"二叉树","uri":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":null,"content":"3.5 满二叉树 满二叉树：在一棵二叉树中。如果所有分支结点都存在左子树和右子树，并且所有叶子都在同一层上，这样的二叉树称为满二叉树。 满二叉树的特点有： 1）叶子只能出现在最下一层。出现在其它层就不可能达成平衡。 2）非叶子结点的度一定是2。 3）在同样深度的二叉树中，满二叉树的结点个数最多，叶子数最多。 图3.4 满二叉树 ","date":"2022-03-30","objectID":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/:3:4","tags":null,"title":"二叉树","uri":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":null,"content":"3.6 完全二叉树 完全二叉树：对一颗具有n个结点的二叉树按层编号，如果编号为i(1\u003c=i\u003c=n)的结点与同样深度的满二叉树中编号为i的结点在二叉树中位置完全相同，则这棵二叉树称为完全二叉树。 图3.5展示一棵完全二叉树 图3.5 完全二叉树 特点： 1）叶子结点只能出现在最下层和次下层。 2）最下层的叶子结点集中在树的左部。 3）倒数第二层若存在叶子结点，一定在右部连续位置。 4）如果结点度为1，则该结点只有左孩子，即没有右子树。 5）同样结点数目的二叉树，完全二叉树深度最小。 注：满二叉树一定是完全二叉树，但反过来不一定成立。 ","date":"2022-03-30","objectID":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/:3:5","tags":null,"title":"二叉树","uri":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":null,"content":"平衡二叉树（AVL）树 在AVL树中，任一节点对应的两棵子树的最大高度差为1，因此它也被称为高度平衡树 为什么要有平衡二叉树 二叉搜索树一定程度上可以提高搜索效率，但是当原序列有序时，例如序列 A = {1，2，3，4，5，6}，构造二叉搜索树如图 1.1。依据此序列构造的二叉搜索树为右斜树，同时二叉树退化成单链表，搜索效率降低为 O(n)。 在此二叉搜索树中查找元素 6 需要查找 6 次。 二叉搜索树的查找效率取决于树的高度，因此保持树的高度最小，即可保证树的查找效率。同样的序列 A，将其改为图 1.2 的方式存储，查找元素 6 时只需比较 3 次，查找效率提升一倍。 可以看出当节点数目一定，保持树的左右两端保持平衡，树的查找效率最高。 这种左右子树的高度相差不超过 1 的树为平衡二叉树。 ","date":"2022-03-30","objectID":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/:3:6","tags":null,"title":"二叉树","uri":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":null,"content":"哈夫曼树 路径：在一棵树中，一个结点到另一个结点之间的通路，称为路径。图 1 中，从根结点到结点 a 之间的通路就是一条路径。 路径长度：在一条路径中，每经过一个结点，路径长度都要加 1 。例如在一棵树中，规定根结点所在层数为1层，那么从根结点到第 i 层结点的路径长度为 i - 1 。图 1 中从根结点到结点 c 的路径长度为 3。 结点的权：给每一个结点赋予一个新的数值，被称为这个结点的权。例如，图 1 中结点 a 的权为 7，结点 b 的权为 5。 结点的带权路径长度：指的是从根结点到该结点之间的路径长度与该结点的权的乘积。例如，图 1 中结点 b 的带权路径长度为 2 * 5 = 10 。 树的带权路径长度为树中所有叶子结点的带权路径长度之和。通常记作 “WPL” 。例如图 1 中所示的这颗树的带权路径长度为： WPL = 7 * 1 + 5 * 2 + 2 * 3 + 4 * 3 若以{4,5,6,7,8}作为叶子结点的权值构造哈夫曼树，则其带权路径长度是（）。 ","date":"2022-03-30","objectID":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/:3:7","tags":null,"title":"二叉树","uri":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":null,"content":"3.7 二叉树的存储结构 3.7.2 二叉链表 既然顺序存储不能满足二叉树的存储需求，那么考虑采用链式存储。由二叉树定义可知，二叉树的每个结点最多有两个孩子。因此，可以将结点数据结构定义为一个数据和两个指针域。表示方式如图3.11所示： ","date":"2022-03-30","objectID":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/:3:8","tags":null,"title":"二叉树","uri":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":null,"content":"3.8 二叉树遍历 二叉树的遍历一个重点考查的知识点。 3.8.1 定义 二叉树的遍历是指从二叉树的根结点出发，按照某种次序依次访问二叉树中的所有结点，使得每个结点被访问一次，且仅被访问一次。 二叉树的访问次序可以分为四种： 前序遍历 中序遍历 后序遍历 层序遍历 3.8.2 前序遍历 前序遍历通俗的说就是从二叉树的根结点出发，当第一次到达结点时就输出结点数据，按照先向左在向右的方向访问。 3.13 图3.13所示二叉树访问如下： 从根结点出发，则第一次到达结点A，故输出A; 继续向左访问，第一次访问结点B，故输出B； 按照同样规则，输出D，输出H； 当到达叶子结点H，返回到D，此时已经是第二次到达D，故不在输出D，进而向D右子树访问，D右子树不为空，则访问至I，第一次到达I，则输出I； I为叶子结点，则返回到D，D左右子树已经访问完毕，则返回到B，进而到B右子树，第一次到达E，故输出E； 向E左子树，故输出J； 按照同样的访问规则，继续输出C、F、G； 则3.13所示二叉树的前序遍历输出为： ABDHIEJCFG 3.8.3 中序遍历 中序遍历就是从二叉树的根结点出发，当第二次到达结点时就输出结点数据，按照先向左在向右的方向访问。 图3.13所示二叉树中序访问如下： 从根结点出发，则第一次到达结点A，不输出A，继续向左访问，第一次访问结点B，不输出B；继续到达D，H； 到达H，H左子树为空，则返回到H，此时第二次访问H，故输出H； H右子树为空，则返回至D，此时第二次到达D，故输出D； 由D返回至B，第二次到达B，故输出B； 按照同样规则继续访问，输出J、E、A、F、C、G； 则3.13所示二叉树的中序遍历输出为： HDIBJEAFCG 3.8.4 后序遍历 后序遍历就是从二叉树的根结点出发，当第三次到达结点时就输出结点数据，按照先向左在向右的方向访问。 图3.13所示二叉树后序访问如下： 从根结点出发，则第一次到达结点A，不输出A，继续向左访问，第一次访问结点B，不输出B；继续到达D，H； 到达H，H左子树为空，则返回到H，此时第二次访问H，不输出H； H右子树为空，则返回至H，此时第三次到达H，故输出H； 由H返回至D，第二次到达D，不输出D； 继续访问至I，I左右子树均为空，故第三次访问I时，输出I； 返回至D，此时第三次到达D，故输出D； 按照同样规则继续访问，输出J、E、B、F、G、C，A； 则图3.13所示二叉树的后序遍历输出为： HIDJEBFGCA 虽然二叉树的遍历过程看似繁琐，但是由于二叉树是一种递归定义的结构，故采用递归方式遍历二叉树的代码十分简单。 递归实现代码如下： /*二叉树的前序遍历递归算法*/ void PreOrderTraverse(BiTree T) { if(T==NULL) return; printf(\"%c\", T-\u003edata); /*显示结点数据，可以更改为其他对结点操作*/ PreOrderTraverse(T-\u003elchild); /*再先序遍历左子树*/ PreOrderTraverse(T-\u003erchild); /*最后先序遍历右子树*/ } /*二叉树的中序遍历递归算法*/ void InOrderTraverse(BiTree T) { if(T==NULL) return; InOrderTraverse(T-\u003elchild); /*中序遍历左子树*/ printf(\"%c\", T-\u003edata); /*显示结点数据，可以更改为其他对结点操作*/ InOrderTraverse(T-\u003erchild); /*最后中序遍历右子树*/ } /*二叉树的后序遍历递归算法*/ void PostOrderTraverse(BiTree T) { if(T==NULL) return; PostOrderTraverse(T-\u003elchild); /*先后序遍历左子树*/ PostOrderTraverse(T-\u003erchild); /*再后续遍历右子树*/ printf(\"%c\", T-\u003edata); /*显示结点数据，可以更改为其他对结点操作*/ } 3.8.5 层次遍历 层次遍历就是按照树的层次自上而下的遍历二叉树。针对图3.13所示二叉树的层次遍历结果为： ABCDEFGHIJ 层次遍历的详细方法可以参考二叉树的按层遍历法。 3.8.6 遍历常考考点 对于二叉树的遍历有一类典型题型。 1）已知前序遍历序列和中序遍历序列，确定一棵二叉树。 例题：若一棵二叉树的前序遍历为ABCDEF，中序遍历为CBAEDF，请画出这棵二叉树。 分析：前序遍历第一个输出结点为根结点，故A为根结点。早中序遍历中根结点处于左右子树结点中间，故结点A的左子树中结点有CB，右子树中结点有EDF。 ref http://data.biancheng.net/view/192.html2 ","date":"2022-03-30","objectID":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/:3:9","tags":null,"title":"二叉树","uri":"/posts/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"categories":null,"content":"使用fastgit git config --global url.\"https://github.com.cnpmjs.org/\".insteadOf \"https://github.com/\" git config protocol.https.allow always git config --global --unset url.\"https://hub.fastgit.org/\".insteadOf \"https://github.com/\" ","date":"2022-03-27","objectID":"/posts/github%E5%8A%A0%E9%80%9F/:1:0","tags":null,"title":"Github加速","uri":"/posts/github%E5%8A%A0%E9%80%9F/"},{"categories":null,"content":"使用fakev git config --global url.\"https://gh.dlpu.workers.dev/\".insteadOf \"https://github.com/\" git config protocol.https.allow always git config --global url.\"https://gh.fakev.cn/\".insteadOf \"https://github.com/\" git config protocol.https.allow always 删除 git config –global –unset url.“https://gh.dlpu.workers.dev/\".insteadOf “https://github.com/\" ","date":"2022-03-27","objectID":"/posts/github%E5%8A%A0%E9%80%9F/:2:0","tags":null,"title":"Github加速","uri":"/posts/github%E5%8A%A0%E9%80%9F/"},{"categories":null,"content":"修改hosts 140.82.113.4 github.com 13.229.188.59 github.com 13.114.40.48 github.com 185.199.110.153 github.com 185.199.111.153 github.com 185.199.109.153 github.com 185.199.108.153 github.com ","date":"2022-03-27","objectID":"/posts/github%E5%8A%A0%E9%80%9F/:3:0","tags":null,"title":"Github加速","uri":"/posts/github%E5%8A%A0%E9%80%9F/"},{"categories":null,"content":" 你知道的越多，你不知道的越多 点赞再看，养成习惯 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:0:0","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"前言 Redis在互联网技术存储方面使用如此广泛，几乎所有的后端技术面试官都要在Redis的使用和原理方面对小伙伴们进行360°的刁难。作为一个在互联网公司面一次拿一次offer的面霸（请允许我使用一下夸张的修辞手法），打败了无数竞争对手，每次都只能看到无数落寞的身影失望的离开，略感愧疚，在一个寂寞难耐的夜晚，我痛定思痛，决定开始写《吊打面试官》系列，希望能帮助各位读者以后面试势如破竹，对面试官进行360°的反击，吊打问你的面试官，让一同面试的同僚铩羽而归，疯狂收割大厂offer！ ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:0","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"面试开始 一个大腹便便，穿着格子衬衣的中年男子，拿着一个满是划痕的mac向你走来，看着快秃顶的头发，心想着肯定是尼玛顶级架构师吧！但是我们腹有诗书气自华，虚都不虚。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:1","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"小伙子您好，看你简历上写了你项目里面用到了Redis，你们为啥用Redis？ 心里忍不住暗骂，这叫啥问题，大家不都是用的这个嘛，但是你不能说出来。 认真回答道：帅气迷人的面试官您好，因为传统的关系型数据库如Mysql已经不能适用所有的场景了，比如秒杀的库存扣减，APP首页的访问流量高峰等等，都很容易把数据库打崩，所以引入了缓存中间件，目前市面上比较常用的缓存中间件有Redis 和 Memcached 不过中和考虑了他们的优缺点，最后选择了Redis。 至于更细节的对比朋友们记得查阅Redis 和 Memcached 的区别，比如两者的优缺点对比和各自的场景，后续我有时间也会写出来。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:2","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"那小伙子，我再问你，Redis有哪些数据结构呀？ 字符串String、字典Hash、列表List、集合Set、有序集合SortedSet。 这里我相信99%的读者都能回答上来Redis的5个基本数据类型。如果回答不出来的小伙伴我们就要加油补课哟，大家知道五种类型最适合的场景更好。 但是，如果你是Redis中高级用户，而且你要在这次面试中突出你和其他候选人的不同，还需要加上下面几种数据结构HyperLogLog、Geo、Pub/Sub。 如果你还想加分，那你说还玩过Redis Module，像BloomFilter，RedisSearch，Redis-ML，这个时候面试官得眼睛就开始发亮了，心想这个小伙子有点东西啊。 注：本人在面试回答到Redis相关的问题的时候，经常提到BloomFilter（布隆过滤器）这玩意的使用场景是真的多，而且用起来是真的香，原理也好理解，看一下文章就可以在面试官面前侃侃而谈了，不香么？下方传送门 ↓ 避免缓存穿透的利器之BloomFilter ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:3","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"如果有大量的key需要设置同一时间过期，一般需要注意什么？ 如果大量的key过期时间设置的过于集中，到过期的那个时间点，redis可能会出现短暂的卡顿现象。严重的话会出现缓存雪崩，我们一般需要在时间上加一个随机值，使得过期时间分散一些。 电商首页经常会使用定时任务刷新缓存，可能大量的数据失效时间都十分集中，如果失效时间一样，又刚好在失效的时间点大量用户涌入，就有可能造成缓存雪崩 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:4","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"那你使用过Redis分布式锁么，它是什么回事？ 先拿setnx来争抢锁，抢到之后，再用expire给锁加一个过期时间防止锁忘记了释放。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:5","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"这时候对方会告诉你说你回答得不错，然后接着问如果在setnx之后执行expire之前进程意外crash或者要重启维护了，那会怎么样？ 这时候你要给予惊讶的反馈：唉，是喔，这个锁就永远得不到释放了。紧接着你需要抓一抓自己得脑袋，故作思考片刻，好像接下来的结果是你主动思考出来的，然后回答：我记得set指令有非常复杂的参数，这个应该是可以同时把setnx和expire合成一条指令来用的！ ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:6","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"对方这时会显露笑容，心里开始默念：嗯，这小子还不错，开始有点意思了。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:7","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如何将它们全部找出来？ 使用keys指令可以扫出指定模式的key列表。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:8","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"对方接着追问：如果这个redis正在给线上的业务提供服务，那使用keys指令会有什么问题？ 这个时候你要回答redis关键的一个特性：redis的单线程的。keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。 不过，增量式迭代命令也不是没有缺点的： 举个例子， 使用 SMEMBERS 命令可以返回集合键当前包含的所有元素， 但是对于 SCAN 这类增量式迭代命令来说， 因为在对键进行增量式迭代的过程中， 键可能会被修改， 所以增量式迭代命令只能对被返回的元素提供有限的保证 。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:9","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"使用过Redis做异步队列么，你是怎么用的？ 一般使用list结构作为队列，rpush生产消息，lpop消费消息。当lpop没有消息的时候，要适当sleep一会再重试。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:10","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"如果对方追问可不可以不用sleep呢？ list还有个指令叫blpop，在没有消息的时候，它会阻塞住直到消息到来。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:11","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"如果对方接着追问能不能生产一次消费多次呢？ 使用pub/sub主题订阅者模式，可以实现 1:N 的消息队列。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:12","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"如果对方继续追问 pub/su b有什么缺点？ 在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如RocketMQ等。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:13","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"如果对方究极TM追问Redis如何实现延时队列？ 这一套连招下来，我估计现在你很想把面试官一棒打死（面试官自己都想打死自己了怎么问了这么多自己都不知道的），如果你手上有一根棒球棍的话，但是你很克制。平复一下激动的内心，然后神态自若的回答道：使用sortedset，拿时间戳作为score，消息内容作为key调用zadd来生产消息，消费者用zrangebyscore指令获取N秒之前的数据轮询进行处理。 到这里，面试官暗地里已经对你竖起了大拇指。并且已经默默给了你A+，但是他不知道的是此刻你却竖起了中指，在椅子背后。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:14","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"Redis是怎么持久化的？服务主从数据怎么交互的？ RDB做镜像全量持久化，AOF做增量持久化。因为RDB会耗费较长时间，不够实时，在停机的时候会导致大量丢失数据，所以需要AOF来配合使用。在redis实例重启时，会使用RDB持久化文件重新构建内存，再使用AOF重放近期的操作指令来实现完整恢复重启之前的状态。 这里很好理解，把RDB理解为一整个表全量的数据，AOF理解为每次操作的日志就好了，服务器重启的时候先把表的数据全部搞进去，但是他可能不完整，你再回放一下日志，数据不就完整了嘛。不过Redis本身的机制是 AOF持久化开启且存在AOF文件时，优先加载AOF文件；AOF关闭或者AOF文件不存在时，加载RDB文件；加载AOF/RDB文件城后，Redis启动成功； AOF/RDB文件存在错误时，Redis启动失败并打印错误信息 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:15","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"对方追问那如果突然机器掉电会怎样？ 取决于AOF日志sync属性的配置，如果不要求性能，在每条写指令时都sync一下磁盘，就不会丢失数据。但是在高性能的要求下每次都sync是不现实的，一般都使用定时sync，比如1s1次，这个时候最多就会丢失1s的数据。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:16","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"对方追问RDB的原理是什么？ 你给出两个词汇就可以了，fork和cow。fork是指redis通过创建子进程来进行RDB操作，cow指的是copy on write，子进程创建后，父子进程共享数据段，父进程继续提供读写服务，写脏的页面数据会逐渐和子进程分离开来。 ******注：回答这个问题的时候，如果你还能说出AOF和RDB的优缺点，我觉得我是面试官在这个问题上我会给你点赞，两者其实区别还是很大的，而且涉及到Redis集群的数据同步问题等等。想了解的伙伴也可以留言，我会专门写一篇来介绍的。 ****** ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:17","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"Pipeline有什么好处，为什么要用pipeline？ 可以将多次IO往返的时间缩减为一次，前提是pipeline执行的指令之间没有因果相关性。使用redis-benchmark进行压测的时候可以发现影响redis的QPS峰值的一个重要因素是pipeline批次指令的数目。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:18","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"Redis的同步机制了解么？ Redis可以使用主从同步，从从同步。第一次同步时，主节点做一次bgsave，并同时将后续修改操作记录到内存buffer，待完成后将RDB文件全量同步到复制节点，复制节点接受完成后将RDB镜像加载到内存。加载完成后，再通知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。后续的增量数据通过AOF日志同步即可，有点类似数据库的binlog。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:19","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"是否使用过Redis集群，集群的高可用怎么保证，集群的原理是什么？ Redis Sentinal着眼于高可用，在master宕机时会自动将slave提升为master，继续提供服务。 Redis Cluster着眼于扩展性，在单个redis内存不足时，使用Cluster进行分片存储。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:1:20","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"面试结束 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:2:0","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"小伙子你可以的，什么时候有时间来上班啊，要不明天就来吧？ 你强装镇定，这么急啊我还需要租房，要不下礼拜一吧。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:2:1","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"好的 心想这小子这么NB是不是很多Offer在手上，不行我得叫hr给他加钱。 能撑到最后，你自己都忍不住自己给自己点个赞了! （暗示点赞，每次都看了不点赞，你们想白嫖我么？你们好坏喲，不过我喜欢）。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:2:2","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":"总结 在技术面试的时候，不管是Redis还是什么问题，如果你能举出实际的例子，或者是直接说自己开发过程的问题和收获会给面试官的印象分会加很多，回答逻辑性也要强一点，不要东一点西一点，容易把自己都绕晕的。 还有一点就是我问你为啥用Redis你不要一上来就直接回答问题了，你可以这样回答： 帅气的面试官您好，首先我们的项目DB遇到了瓶颈，特别是秒杀和热点数据这样的场景DB基本上就扛不住了，那就需要缓存中间件的加入了，目前市面上有的缓存中间件有 Redis 和 Memcached ，他们的优缺点……，综合这些然后再结合我们项目特点，最后我们在技术选型的时候选了谁。 如果你这样有条不紊，有理有据的回答了我的问题而且还说出这么多我问题外的知识点，我会觉得你不只是一个会写代码的人，你逻辑清晰，你对技术选型，对中间件对项目都有自己的理解和思考，说白了就是你的offer有戏了。 好了 以上就是这篇文章的全部内容了，我后面会不断更新《吊打面试官》系列和Java技术栈相关的文章。如果你有什么想知道的，也可以留言给我，我一有时间就会写出来，我们共同进步。 非常感谢您能看到这里，如果这个文章写得还不错的话 求点赞 求关注 求分享 求留言 各位的支持和认可，就是我创作的最大动力，OK各位我们下期见！ 敖丙 | 文 后面会持续更新《吊打面试官》系列可以关注我的公众号第一时间阅读，也会有朋友一线大厂的内推机会不定期推出（字节跳动，阿里，网易，PDD，滴滴，蘑菇街等），就业上有什么问题也可以直接微信滴滴我，我也是个新人，不过不影响我们一起进步，作为渣男，我给不了你工作，还给不了你温暖嘛？ ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/:3:0","tags":["redis"],"title":"redis基础","uri":"/posts/middleware/redis/redis%E5%9F%BA%E7%A1%80/"},{"categories":null,"content":" 你知道的越多，你不知道的越多 点赞再看，养成习惯 GitHub上已经开源https://github.com/Java…，有面试点思维导图，欢迎Star和完善 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:0:0","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"前言 Redis在互联网技术存储方面使用如此广泛，几乎所有的后端技术面试官都要在Redis的使用和原理方面对小伙伴们进行360°的刁难。 作为一个在互联网公司面一次拿一次Offer的面霸，打败了无数竞争对手，每次都只能看到无数落寞的身影失望的离开，略感愧疚（请允许我使用一下夸张的修辞手法）。 于是在一个寂寞难耐的夜晚，我痛定思痛，决定开始写**《吊打面试官》**系列，希望能帮助各位读者以后面试势如破竹，对面试官进行360°的反击，吊打问你的面试官，让一同面试的同僚瞠目结舌，疯狂收割大厂Offer！ ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:1:0","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"絮叨 上一期因为是在双十一一直在熬夜的大环境下完成的，所以我自己觉得质量明显没之前的好，我这不一睡好就加班加点准备补偿大家，来点干货。（熬夜太容易感冒了，这次点个赞别白嫖了！） 顺带提一嘴，我把我准备写啥画了一个思维导图，以后总不能每篇都放个贼大的图吧，就开源到了我的**GitHub**，大家有兴趣可以去完善和**Star**。 这篇我就先放出来大家看看，感觉还是差点意思，等大家完善了。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:2:0","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"回望过去 上一期吊打系列我们提到了Redis相关的一些知识，还没看的小伙伴可以回顾一下 《吊打面试官》系列-Redis基础 《吊打面试官》系列-缓存雪崩、击穿、穿透 《吊打面试官》系列-Redis哨兵、持久化、主从、手撕LRU 《吊打面试官》系列-Redis终章-凛冬将至、FPX-新王登基 这期我就从缓存到一些常见的问题讲一下，有一些我是之前提到过的，不过可能大部分仔是第一次看，我就重复发一下。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:0","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"缓存知识点 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:1","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"缓存有哪些类型？ 缓存是高并发场景下提高热点数据访问性能的一个有效手段，在开发项目时会经常使用到。 缓存的类型分为：本地缓存、分布式缓存和多级缓存。 本地缓存： 本地缓存就是在进程的内存中进行缓存，比如我们的 JVM 堆中，可以用 LRUMap 来实现，也可以使用 Ehcache 这样的工具来实现。 本地缓存是内存访问，没有远程交互开销，性能最好，但是受限于单机容量，一般缓存较小且无法扩展。 分布式缓存： 分布式缓存可以很好得解决这个问题。 分布式缓存一般都具有良好的水平扩展能力，对较大数据量的场景也能应付自如。缺点就是需要进行远程请求，性能不如本地缓存。 多级缓存： 为了平衡这种情况，实际业务中一般采用多级缓存，本地缓存只保存访问频率最高的部分热点数据，其他的热点数据放在分布式缓存中。 在目前的一线大厂中，这也是最常用的缓存方案，单考单一的缓存方案往往难以撑住很多高并发的场景。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:2","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"淘汰策略 不管是本地缓存还是分布式缓存，为了保证较高性能，都是使用内存来保存数据，由于成本和内存限制，当存储的数据超过缓存容量时，需要对缓存的数据进行剔除。 一般的剔除策略有 FIFO 淘汰最早数据、LRU 剔除最近最少使用、和 LFU 剔除最近使用频率最低的数据几种策略。 noeviction:返回错误当内存限制达到并且客户端尝试执行会让更多内存被使用的命令（大部分的写入指令，但DEL和几个例外） allkeys-lru: 尝试回收最少使用的键（LRU），使得新添加的数据有空间存放。 volatile-lru: 尝试回收最少使用的键（LRU），但仅限于在过期集合的键,使得新添加的数据有空间存放。 allkeys-random: 回收随机的键使得新添加的数据有空间存放。 volatile-random: 回收随机的键使得新添加的数据有空间存放，但仅限于在过期集合的键。 volatile-ttl: 回收在过期集合的键，并且优先回收存活时间（TTL）较短的键,使得新添加的数据有空间存放。 如果没有键满足回收的前提条件的话，策略volatile-lru, volatile-random以及volatile-ttl就和noeviction 差不多了。 其实在大家熟悉的LinkedHashMap中也实现了Lru算法的，实现如下： 当容量超过100时，开始执行LRU策略：将最近最少未使用的 TimeoutInfoHolder 对象 evict 掉。 真实面试中会让你写LUR算法，你可别搞原始的那个，那真TM多，写不完的，你要么怼上面这个，要么怼下面这个，找一个数据结构实现下Java版本的LRU还是比较容易的，知道啥原理就好了。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:3","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"Memcache 注意后面会把 Memcache 简称为 MC。 先来看看 MC 的特点： MC 处理请求时使用多线程异步 IO 的方式，可以合理利用 CPU 多核的优势，性能非常优秀； MC 功能简单，使用内存存储数据； MC 的内存结构以及钙化问题我就不细说了，大家可以查看官网了解下； MC 对缓存的数据可以设置失效期，过期后的数据会被清除； 失效的策略采用延迟失效，就是当再次使用数据时检查是否失效； 当容量存满时，会对缓存中的数据进行剔除，剔除时除了会对过期 key 进行清理，还会按 LRU 策略对数据进行剔除。 另外，使用 MC 有一些限制，这些限制在现在的互联网场景下很致命，成为大家选择Redis、MongoDB的重要原因： key 不能超过 250 个字节； value 不能超过 1M 字节； key 的最大失效时间是 30 天； 只支持 K-V 结构，不提供持久化和主从同步功能。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:4","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"Redis 先简单说一下 Redis 的特点，方便和 MC 比较。 与 MC 不同的是，Redis 采用单线程模式处理请求。这样做的原因有 2 个：一个是因为采用了非阻塞的异步事件处理机制；另一个是缓存数据都是内存操作 IO 时间不会太长，单线程可以避免线程上下文切换产生的代价。 Redis 支持持久化，所以 Redis 不仅仅可以用作缓存，也可以用作 NoSQL 数据库。 相比 MC，Redis 还有一个非常大的优势，就是除了 K-V 之外，还支持多种数据格式，例如 list、set、sorted set、hash 等。 Redis 提供主从同步机制，以及 Cluster 集群部署能力，能够提供高可用服务。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:5","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"详解 Redis Redis 的知识点结构如下图所示。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:6","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"功能 来看 Redis 提供的功能有哪些吧！ 我们先看基础类型： String： String 类型是 Redis 中最常使用的类型，内部的实现是通过 SDS（Simple Dynamic String ）来存储的。SDS 类似于 Java 中的 ArrayList，可以通过预分配冗余空间的方式来减少内存的频繁分配。 这是最简单的类型，就是普通的 set 和 get，做简单的 KV 缓存。 但是真实的开发环境中，很多仔可能会把很多比较复杂的结构也统一转成String去存储使用，比如有的仔他就喜欢把对象或者List转换为JSONString进行存储，拿出来再反序列话啥的。 我在这里就不讨论这样做的对错了，但是我还是希望大家能在最合适的场景使用最合适的数据结构，对象找不到最合适的但是类型可以选最合适的嘛，之后别人接手你的代码一看这么规范，诶这小伙子有点东西呀，看到你啥都是用的String，垃圾！ 好了这些都是题外话了，道理还是希望大家记在心里，习惯成自然嘛，小习惯成就你。 String的实际应用场景比较广泛的有： 缓存功能：String字符串是最常用的数据类型，不仅仅是Redis，各个语言都是最基本类型，因此，利用Redis作为缓存，配合其它数据库作为存储层，利用Redis支持高并发的特点，可以大大加快系统的读写速度、以及降低后端数据库的压力。 计数器：许多系统都会使用Redis作为系统的实时计数器，可以快速实现计数和查询的功能。而且最终的数据结果可以按照特定的时间落地到数据库或者其它存储介质当中进行永久保存。 共享用户Session：用户重新刷新一次界面，可能需要访问一下数据进行重新登录，或者访问页面缓存Cookie，但是可以利用Redis将用户的Session集中管理，在这种模式只需要保证Redis的高可用，每次用户Session的更新和获取都可以快速完成。大大提高效率。 Hash： 这个是类似 Map 的一种结构，这个一般就是可以将结构化的数据，比如一个对象（前提是这个对象没嵌套其他的对象）给缓存在 Redis 里，然后每次读写缓存的时候，可以就操作 Hash 里的某个字段。 但是这个的场景其实还是多少单一了一些，因为现在很多对象都是比较复杂的，比如你的商品对象可能里面就包含了很多属性，其中也有对象。我自己使用的场景用得不是那么多。 List： List 是有序列表，这个还是可以玩儿出很多花样的。 比如可以通过 List 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的东西。 比如可以通过 lrange 命令，读取某个闭区间内的元素，可以基于 List 实现分页查询，这个是很棒的一个功能，基于 Redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西，性能高，就一页一页走。 比如可以搞个简单的消息队列，从 List 头怼进去，从 List 屁股那里弄出来。 List本身就是我们在开发过程中比较常用的数据结构了，热点数据更不用说了。 消息队列：Redis的链表结构，可以轻松实现阻塞队列，可以使用左进右出的命令组成来完成队列的设计。比如：数据的生产者可以通过Lpush命令从左边插入数据，多个数据消费者，可以使用BRpop命令阻塞的“抢”列表尾部的数据。 文章列表或者数据分页展示的应用。 比如，我们常用的博客网站的文章列表，当用户量越来越多时，而且每一个用户都有自己的文章列表，而且当文章多时，都需要分页展示，这时可以考虑使用Redis的列表，列表不但有序同时还支持按照范围内获取元素，可以完美解决分页查询功能。大大提高查询效率。 Set： Set 是无序集合，会自动去重的那种。 直接基于 Set 将系统里需要去重的数据扔进去，自动就给去重了，如果你需要对一些数据进行快速的全局去重，你当然也可以基于 JVM 内存里的 HashSet 进行去重，但是如果你的某个系统部署在多台机器上呢？得基于Redis进行全局的 Set 去重。 可以基于 Set 玩儿交集、并集、差集的操作，比如交集吧，我们可以把两个人的好友列表整一个交集，看看俩人的共同好友是谁？对吧。 反正这些场景比较多，因为对比很快，操作也简单，两个查询一个Set搞定。 Sorted Set： Sorted set 是排序的 Set，去重但可以排序，写进去的时候给一个分数，自动根据分数排序。 有序集合的使用场景与集合类似，但是set集合不是自动有序的，而Sorted set可以利用分数进行成员间的排序，而且是插入时就排序好。所以当你需要一个有序且不重复的集合列表时，就可以选择Sorted set数据结构作为选择方案。 排行榜：有序集合经典使用场景。例如视频网站需要对用户上传的视频做排行榜，榜单维护可能是多方面：按照时间、按照播放量、按照获得的赞数等。 用Sorted Sets来做带权重的队列，比如普通消息的score为1，重要消息的score为2，然后工作线程可以选择按score的倒序来获取工作任务。让重要的任务优先执行。 微博热搜榜，就是有个后面的热度值，前面就是名称 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:7","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"高级用法： Bitmap : 位图是支持按 bit 位来存储信息，可以用来实现 布隆过滤器（BloomFilter）； HyperLogLog: 供不精确的去重计数功能，比较适合用来做大规模数据的去重统计，例如统计 UV； Geospatial: 可以用来保存地理位置，并作位置距离计算或者根据半径计算位置等。有没有想过用Redis来实现附近的人？或者计算最优地图路径？ 这三个其实也可以算作一种数据结构，不知道还有多少朋友记得，我在梦开始的地方，Redis基础中提到过，你如果只知道五种基础类型那只能拿60分，如果你能讲出高级用法，那就觉得你有点东西。 pub/sub： 功能是订阅发布功能，可以用作简单的消息队列。 Pipeline： 可以批量执行一组指令，一次性返回全部结果，可以减少频繁的请求应答。 Lua： Redis 支持提交 Lua 脚本来执行一系列的功能。 我在前电商老东家的时候，秒杀场景经常使用这个东西，讲道理有点香，利用他的原子性。 话说你们想看秒杀的设计么？我记得我面试好像每次都问啊，想看的直接点赞后评论秒杀吧。 事务： 最后一个功能是事务，但 Redis 提供的不是严格的事务，Redis 只保证串行执行命令，并且能保证全部执行，但是执行命令失败时并不会回滚，而是会继续执行下去。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:8","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"持久化 Redis 提供了 RDB 和 AOF 两种持久化方式，RDB 是把内存中的数据集以快照形式写入磁盘，实际操作是通过 fork 子进程执行，采用二进制压缩存储；AOF 是以文本日志的形式记录 Redis 处理的每一个写入或删除操作。 RDB 把整个 Redis 的数据保存在单一文件中，比较适合用来做灾备，但缺点是快照保存完成之前如果宕机，这段时间的数据将会丢失，另外保存快照时可能导致服务短时间不可用。 AOF 对日志文件的写入操作使用的追加模式，有灵活的同步策略，支持每秒同步、每次修改同步和不同步，缺点就是相同规模的数据集，AOF 要大于 RDB，AOF 在运行效率上往往会慢于 RDB。 细节的点大家去高可用这章看，特别是两者的优缺点，以及怎么抉择。 《吊打面试官》系列-Redis哨兵、持久化、主从、手撕LRU ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:9","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"高可用 来看 Redis 的高可用。Redis 支持主从同步，提供 Cluster 集群部署模式，通过 Sentine l哨兵来监控 Redis 主服务器的状态。当主挂掉时，在从节点中根据一定策略选出新主，并调整其他从 slaveof 到新主。 选主的策略简单来说有三个： slave 的 priority 设置的越低，优先级越高； 同等情况下，slave 复制的数据越多优先级越高； 相同的条件下 runid 越小越容易被选中。 在 Redis 集群中，sentinel 也会进行多实例部署，sentinel 之间通过 Raft 协议来保证自身的高可用。 Redis Cluster 使用分片机制，在内部分为 16384 个 slot 插槽，分布在所有 master 节点上，每个 master 节点负责一部分 slot。数据操作时按 key 做 CRC16 来计算在哪个 slot，由哪个 master 进行处理。数据的冗余是通过 slave 节点来保障。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:10","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"哨兵 哨兵必须用三个实例去保证自己的健壮性的，哨兵+主从并不能保证数据不丢失，但是可以保证集群的高可用。 为啥必须要三个实例呢？我们先看看两个哨兵会咋样。 master宕机了 s1和s2两个哨兵只要有一个认为你宕机了就切换了，并且会选举出一个哨兵去执行故障，但是这个时候也需要大多数哨兵都是运行的。 那这样有啥问题呢？M1宕机了，S1没挂那其实是OK的，但是整个机器都挂了呢？哨兵就只剩下S2个裸屌了，没有哨兵去允许故障转移了，虽然另外一个机器上还有R1，但是故障转移就是不执行。 经典的哨兵集群是这样的： M1所在的机器挂了，哨兵还有两个，两个人一看他不是挂了嘛，那我们就选举一个出来执行故障转移不就好了。 暖男我，小的总结下哨兵组件的主要功能： 集群监控：负责监控 Redis master 和 slave 进程是否正常工作。 消息通知：如果某个 Redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。 故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。 配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:11","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"主从 提到这个，就跟我前面提到的数据持久化的RDB和AOF有着比密切的关系了。 我先说下为啥要用主从这样的架构模式，前面提到了单机QPS是有上限的，而且Redis的特性就是必须支撑读高并发的，那你一台机器又读又写，这谁顶得住啊，不当人啊！但是你让这个master机器去写，数据同步给别的slave机器，他们都拿去读，分发掉大量的请求那是不是好很多，而且扩容的时候还可以轻松实现水平扩容。 你启动一台slave 的时候，他会发送一个psync命令给master ，如果是这个slave第一次连接到master，他会触发一个全量复制。master就会启动一个线程，生成RDB快照，还会把新的写请求都缓存在内存中，RDB文件生成后，master会将这个RDB发送给slave的，slave拿到之后做的第一件事情就是写进本地的磁盘，然后加载进内存，然后master会把内存里面缓存的那些新命名都发给slave。 我发出来之后来自CSDN的网友：Jian_Shen_Zer 问了个问题： 主从同步的时候，新的slaver进来的时候用RDB，那之后的数据呢？有新的数据进入master怎么同步到slaver啊 敖丙答：笨，AOF嘛，增量的就像MySQL的Binlog一样，把日志增量同步给从服务就好了 key 失效机制 Redis 的 key 可以设置过期时间，过期后 Redis 采用主动和被动结合的失效机制，一个是和 MC 一样在访问时触发被动删除，另一种是定期的主动删除。 定期+惰性+内存淘汰 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:12","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"缓存常见问题 缓存更新方式 这是决定在使用缓存时就该考虑的问题。 缓存的数据在数据源发生变更时需要对缓存进行更新，数据源可能是 DB，也可能是远程服务。更新的方式可以是主动更新。数据源是 DB 时，可以在更新完 DB 后就直接更新缓存。 当数据源不是 DB 而是其他远程服务，可能无法及时主动感知数据变更，这种情况下一般会选择对缓存数据设置失效期，也就是数据不一致的最大容忍时间。 这种场景下，可以选择失效更新，key 不存在或失效时先请求数据源获取最新数据，然后再次缓存，并更新失效期。 但这样做有个问题，如果依赖的远程服务在更新时出现异常，则会导致数据不可用。改进的办法是异步更新，就是当失效时先不清除数据，继续使用旧的数据，然后由异步线程去执行更新任务。这样就避免了失效瞬间的空窗期。另外还有一种纯异步更新方式，定时对数据进行分批更新。实际使用时可以根据业务场景选择更新方式。 数据不一致 第二个问题是数据不一致的问题，可以说只要使用缓存，就要考虑如何面对这个问题。缓存不一致产生的原因一般是主动更新失败，例如更新 DB 后，更新 Redis 因为网络原因请求超时；或者是异步更新失败导致。 解决的办法是，如果服务对耗时不是特别敏感可以增加重试；如果服务对耗时敏感可以通过异步补偿任务来处理失败的更新，或者短期的数据不一致不会影响业务，那么只要下次更新时可以成功，能保证最终一致性就可以。 缓存穿透 缓存穿透。产生这个问题的原因可能是外部的恶意攻击，例如，对用户信息进行了缓存，但恶意攻击者使用不存在的用户id频繁请求接口，导致查询缓存不命中，然后穿透 DB 查询依然不命中。这时会有大量请求穿透缓存访问到 DB。 解决的办法如下。 对不存在的用户，在缓存中保存一个空对象进行标记，防止相同 ID 再次访问 DB。不过有时这个方法并不能很好解决问题，可能导致缓存中存储大量无用数据。 使用 BloomFilter 过滤器，BloomFilter 的特点是存在性检测，如果 BloomFilter 中不存在，那么数据一定不存在；如果 BloomFilter 中存在，实际数据也有可能会不存在。非常适合解决这类的问题。 缓存击穿 缓存击穿，就是某个热点数据失效时，大量针对这个数据的请求会穿透到数据源。 解决这个问题有如下办法。 可以使用互斥锁更新，保证同一个进程中针对同一个数据不会并发请求到 DB，减小 DB 压力。 使用随机退避方式，失效时随机 sleep 一个很短的时间，再次查询，如果失败再执行更新。 针对多个热点 key 同时失效的问题，可以在缓存时使用固定时间加上一个小的随机数，避免大量热点 key 同一时刻失效。 缓存雪崩 缓存雪崩，产生的原因是缓存挂掉，这时所有的请求都会穿透到 DB。 解决方法： 使用快速失败的熔断策略，减少 DB 瞬间压力； 使用主从模式和集群模式来尽量保证缓存服务的高可用。 实际场景中，这两种方法会结合使用。 老朋友都知道为啥我没有大篇幅介绍这个几个点了吧，我在之前的文章实在是写得太详细了，忍不住点赞那种，我这里就不做重复拷贝了。 《吊打面试官》系列-Redis基础 《吊打面试官》系列-缓存雪崩、击穿、穿透 《吊打面试官》系列-Redis哨兵、持久化、主从、手撕LRU 《吊打面试官》系列-Redis终章_凛冬将至、FPX_新王登基 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:13","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"考点与加分项 拿笔记一下！ 考点 面试的时候问你缓存，主要是考察缓存特性的理解，对 MC、Redis 的特点和使用方式的掌握。 要知道缓存的使用场景，不同类型缓存的使用方式，例如： 对 DB 热点数据进行缓存减少 DB 压力；对依赖的服务进行缓存，提高并发性能； 单纯 K-V 缓存的场景可以使用 MC，而需要缓存 list、set 等特殊数据格式，可以使用 Redis； 需要缓存一个用户最近播放视频的列表可以使用 Redis 的 list 来保存、需要计算排行榜数据时，可以使用 Redis 的 zset 结构来保存。 要了解 MC 和 Redis 的常用命令，例如原子增减、对不同数据结构进行操作的命令等。 了解 MC 和 Redis 在内存中的存储结构，这对评估使用容量会很有帮助。 了解 MC 和 Redis 的数据失效方式和剔除策略，比如主动触发的定期剔除和被动触发延期剔除 要理解 Redis 的持久化、主从同步与 Cluster 部署的原理，比如 RDB 和 AOF 的实现方式与区别。 要知道缓存穿透、击穿、雪崩分别的异同点以及解决方案。 不管你有没有电商经验我觉得你都应该知道秒杀的具体实现，以及细节点。 …….. 欢迎去GitHub补充 加分项 如果想要在面试中获得更好的表现，还应了解下面这些加分项。 是要结合实际应用场景来介绍缓存的使用。例如调用后端服务接口获取信息时，可以使用本地+远程的多级缓存；对于动态排行榜类的场景可以考虑通过 Redis 的 Sorted set 来实现等等。 最好你有过分布式缓存设计和使用经验，例如项目中在什么场景使用过 Redis，使用了什么数据结构，解决哪类的问题；使用 MC 时根据预估值大小调整 McSlab 分配参数等等。 最好可以了解缓存使用中可能产生的问题。比如 Redis 是单线程处理请求，应尽量避免耗时较高的单个请求任务，防止相互影响；Redis 服务应避免和其他 CPU 密集型的进程部署在同一机器；或者禁用 Swap 内存交换，防止 Redis 的缓存数据交换到硬盘上，影响性能。再比如前面提到的 MC 钙化问题等等。 要了解 Redis 的典型应用场景，例如，使用 Redis 来实现分布式锁；使用 Bitmap 来实现 BloomFilter，使用 HyperLogLog 来进行 UV 统计等等。 知道 Redis4.0、5.0 中的新特性，例如支持多播的可持久化消息队列 Stream；通过 Module 系统来进行定制功能扩展等等。 …….. 还是那句话欢迎去GitHub补充。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:3:14","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"总结 这次是对我Redis系列的总结，这应该是Redis相关的最后一篇文章了，其实四篇看下来的小伙伴很多都从一知半解到了一脸懵逼，哈哈开个玩笑。 我觉得我的方式应该还好，大部分小伙伴还是比较能理解的，这篇之后我就不会写Redis相关的文章了(秒杀看大家想看的热度吧)，有啥问题可以微信找我，下个系列写啥？ 大家不用急，下个系列前我会发个有意思的文章，是我在公司代码创意大赛拿奖的文章，我觉得还是有点东西，我忍不住分享一下，顺便就在那期发起投票吧哈哈。 我看到很多小伙伴都有评论说想看别的，大概搜集了一下，还没留言的这期赶紧哟： 掘金 愚辛 ：想看计算机基础，网络和操作系统那些（FPX牛脾） cherish君：讲讲dubbo经常遇到的面试题目，太多人喜欢问dubbo😃 Java架构养成记：真的很香啊，下一期讲Dubbbo（重点SPI）然后讲MQ好吗 CSDN 小殿下：看完了所有的redis篇 希望可以出ssm 博客园 程然：Dubbo Dubbo 开源中国 linshi2019：这期明显是赶工之作啊 敖丙：这条我回一下，鞭策我，我很喜欢，不过说实话还是希望大家理解下，我双十一熬夜三天了，现在给你们写的时候也是值班回家2点左右了，我一天吃饭工作时间肯定是固定的，想写点东西就只有挤出睡觉时间了，这种产出肯定没周末全情投入写的来的质量高。 其实第一期看过来的小伙伴应该也知道，我在排版，还有很多文案，配图其实我一直都有在改进的，光是名词高亮我都要弄很久，因为怕大家看单一的黑白色调枯燥。 我是真的用心在搞，还是希望大家支持下理解下。 知乎、简书、思否、慕课手记没人看不知道为啥，懂行的老铁可以跟我说一下。 我只想说你们想看的肯定都在我开头和GITHub那个图里吧，问题不大，后面都会写的。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:4:0","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"鸣谢 最后感谢下，新浪微博的技术专家张雷。 他于2013年加入新浪微博，作为核心技术人员参与了微博服务化、混合云等多个重点项目，是微博开源的RPC框架Motan的技术负责人，同时也负责微博的Service Mesh方案的研发与推广，专注于高可用架构及服务中间件开发方向。 他负责的Motan框架每天承载着万亿级别的请求调用，是微博平台服务化的基石，每次的突发热点事件、每次的春晚流量高峰，都离不开Motan框架的支撑与保障。此外，他也多次应邀在ArchSummit、WOT、GIAC技术峰会做技术分享。 感谢他对文章部分文案提供的支持和思路。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:5:0","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"END 好了各位，以上就是这篇文章的全部内容了，能看到这里的人呀，都是人才。 我后面会每周都更新几篇《吊打面试官》系列和互联网常用技术栈相关的文章。如果你有什么想知道的，也可以留言给我，我一有时间就会写出来，我们共同进步。 非常感谢人才们能看到这里，如果这个文章写得还不错，觉得「敖丙」我有点东西的话 求点赞👍 求关注❤️ 求分享👥 求留言💬 对暖男我来说 非常有用！！！ 各位的支持和认可，就是我创作的最大动力，我们下篇文章见！ 敖丙 | 文 【原创】【转载请联系本人】 《吊打面试官》系列每周持续更新，可以关注我的公众号JavaFamily第一时间阅读和催更（公众号比博客早一到两天哟），GitHub上已经开源https://github.com/Java…，有面试点思维导图，欢迎Star和完善里面也有我个人微信有什么问题也可以直接滴滴我，我们一起进步。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/:6:0","tags":["redis","八股文"],"title":"redis常见面试题","uri":"/posts/middleware/redis/redis%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":" 你知道的越多，你不知道的越多 点赞再看，养成习惯 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/:0:0","tags":["redis"],"title":"分布式锁、并发竞争、双写一致性","uri":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":null,"content":"前言 Redis在互联网技术存储方面使用如此广泛，几乎所有的后端技术面试官都要在Redis的使用和原理方面对小伙伴们进行360°的刁难。作为一个在互联网公司面一次拿一次offer的面霸（请允许我使用一下夸张的修辞手法），打败了无数竞争对手，每次都只能看到无数落寞的身影失望的离开，略感愧疚，在一个寂寞难耐的夜晚，我痛定思痛，决定开始写**《吊打面试官》**系列，希望能帮助各位读者以后面试势如破竹，对面试官进行360°的反击，吊打问你的面试官，让一同面试的同僚瞠目结舌，疯狂收割大厂offer！ ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/:1:0","tags":["redis"],"title":"分布式锁、并发竞争、双写一致性","uri":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":null,"content":"絮叨 男儿何不带吴钩，收取关山五十州 FPX 🐂B，LPL两年连冠🏆 🐂B！ 看着金色的雨落下，我到窗边，发现天有点蓝，风有点绵，我的眼角又湿了! 最近双十一讲道理有点忙的说，直接肝爆，就是这样作为暖男的我，还是给你们挤出时间搞出终章，忍不住给自己点赞👍 放个双十一照片证明真的忙，希望别取关！！！ 现在你们在看的时候，我应该还在睡觉哈哈。困🛌 之前跟你们说的，限流，降级，是不是在双十一又应验了，下单接口其实没挂，牺牲部分用户体验，保住服务器，你多点几下是可以成功的，等流量高峰过去了，所有的用户全部都恢复正常访问，服务器也没啥事。 去年退款接口被打崩了，今年阿里明显也聪明了很多。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/:2:0","tags":["redis"],"title":"分布式锁、并发竞争、双写一致性","uri":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":null,"content":"正文 上几期吊打系列我们提到了Redis的很多知识，还没看的小伙伴可以回顾一下 《吊打面试官》系列-Redis基础 《吊打面试官》系列-缓存雪崩、击穿、穿透 《吊打面试官》系列-Redis哨兵、持久化、主从、手撕LRU 那提到Redis我相信各位在面试，或者实际开发过程中对基本类型的使用场景，并发竞争带来的问题，以及缓存数据库双写入一致性的问题等，我们有请下一位受害者。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/:3:0","tags":["redis"],"title":"分布式锁、并发竞争、双写一致性","uri":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":null,"content":"面试开始 一个大腹便便，穿着格子衬衣的中年男子，拿着一个满是划痕的mac向你走来，看着快秃顶的头发，心想着肯定是尼玛顶级架构师吧！但是我们腹有诗书气自华，虚都不虚。（这不是第一篇文章的面试官么？） ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/:4:0","tags":["redis"],"title":"分布式锁、并发竞争、双写一致性","uri":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":null,"content":"小伙子，你还记得我在第一章里面问过你，Redis有几种基础数据类型么？ 嗯嗯，帅气的面试官，我肯定记得，没齿难忘！！！ 我特么谢谢你，都四面了还不给Offer！ ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/:4:1","tags":["redis"],"title":"分布式锁、并发竞争、双写一致性","uri":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":null,"content":"那你能说一下他们的特性，还有分别的使用场景么？ 行吧，那我先从String说起。 String： 这是最简单的类型，就是普通的 set 和 get，做简单的 KV 缓存。 但是真实的开发环境中，很多仔可能会把很多比较复杂的结构也统一转成String去存储使用，比如有的仔他就喜欢把对象或者List转换为JSONString进行存储，拿出来再反序列话啥的。 我在这里就不讨论这样做的对错了，但是我还是希望大家能在最合适的场景使用最合适的数据结构，对象找不到最合适的但是类型可以选最合适的嘛，之后别人接手你的代码一看这么规范，诶这小伙子有点东西呀，看到你啥都是用的String，垃圾！ 好了这些都是题外话了，道理还是希望大家记在心里，习惯成自然嘛，小习惯成就你。 String的实际应用场景比较广泛的有： 缓存功能：String字符串是最常用的数据类型，不仅仅是Redis，各个语言都是最基本类型，因此，利用Redis作为缓存，配合其它数据库作为存储层，利用Redis支持高并发的特点，可以大大加快系统的读写速度、以及降低后端数据库的压力。 计数器：许多系统都会使用Redis作为系统的实时计数器，可以快速实现计数和查询的功能。而且最终的数据结果可以按照特定的时间落地到数据库或者其它存储介质当中进行永久保存。 共享用户Session：用户重新刷新一次界面，可能需要访问一下数据进行重新登录，或者访问页面缓存Cookie，但是可以利用Redis将用户的Session集中管理，在这种模式只需要保证Redis的高可用，每次用户Session的更新和获取都可以快速完成。大大提高效率。 Hash： 这个是类似 Map 的一种结构，这个一般就是可以将结构化的数据，比如一个对象（前提是这个对象没嵌套其他的对象）给缓存在 Redis 里，然后每次读写缓存的时候，可以就操作 Hash 里的某个字段。 但是这个的场景其实还是多少单一了一些，因为现在很多对象都是比较复杂的，比如你的商品对象可能里面就包含了很多属性，其中也有对象。我自己使用的场景用得不是那么多。 List： List 是有序列表，这个还是可以玩儿出很多花样的。 比如可以通过 List 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的东西。 比如可以通过 lrange 命令，读取某个闭区间内的元素，可以基于 List 实现分页查询，这个是很棒的一个功能，基于 Redis 实现简单的高性能分页，可以做类似微博那种下拉不断分页的东西，性能高，就一页一页走。 比如可以搞个简单的消息队列，从 List 头怼进去，从 List 屁股那里弄出来。 List本身就是我们在开发过程中比较常用的数据结构了，热点数据更不用说了。 消息队列：Redis的链表结构，可以轻松实现阻塞队列，可以使用左进右出的命令组成来完成队列的设计。比如：数据的生产者可以通过Lpush命令从左边插入数据，多个数据消费者，可以使用BRpop命令阻塞的“抢”列表尾部的数据。 文章列表或者数据分页展示的应用。 比如，我们常用的博客网站的文章列表，当用户量越来越多时，而且每一个用户都有自己的文章列表，而且当文章多时，都需要分页展示，这时可以考虑使用Redis的列表，列表不但有序同时还支持按照范围内获取元素，可以完美解决分页查询功能。大大提高查询效率。 Set： Set 是无序集合，会自动去重的那种。 直接基于 Set 将系统里需要去重的数据扔进去，自动就给去重了，如果你需要对一些数据进行快速的全局去重，你当然也可以基于 JVM 内存里的 HashSet 进行去重，但是如果你的某个系统部署在多台机器上呢？得基于Redis进行全局的 Set 去重。 可以基于 Set 玩儿交集、并集、差集的操作，比如交集吧，我们可以把两个人的好友列表整一个交集，看看俩人的共同好友是谁？对吧。 反正这些场景比较多，因为对比很快，操作也简单，两个查询一个Set搞定。 Sorted Set： Sorted set 是排序的 Set，去重但可以排序，写进去的时候给一个分数，自动根据分数排序。 有序集合的使用场景与集合类似，但是set集合不是自动有序的，而Sorted set可以利用分数进行成员间的排序，而且是插入时就排序好。所以当你需要一个有序且不重复的集合列表时，就可以选择Sorted set数据结构作为选择方案。 排行榜：有序集合经典使用场景。例如视频网站需要对用户上传的视频做排行榜，榜单维护可能是多方面：按照时间、按照播放量、按照获得的赞数等。 用Sorted Sets来做带权重的队列，比如普通消息的score为1，重要消息的score为2，然后工作线程可以选择按score的倒序来获取工作任务。让重要的任务优先执行。 微博热搜榜，就是有个后面的热度值，前面就是名称 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/:4:2","tags":["redis"],"title":"分布式锁、并发竞争、双写一致性","uri":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":null,"content":"小结 Redis基础类型有五种，这个我在基础里面也有提到了，这个问题其实一般都是对P6以下，也就是1-3年左右的小伙伴可能是会问得比较多的问题。 能回答出来五种我想大家都可以，但是不知道大家是否知道，五种类型具体的使用场景，以及什么时候用什么类型最合适呢？ 要是你回答的不好，没说出几种数据类型，也没说什么场景，你完了，面试官对你印象肯定不好，觉得你平时就是做个简单的 set 和 get。所以看似很简单的面试题实则最容易看出你的深浅了，大家都要注意打好基础。 ###你有没有考虑过，如果你多个系统同时操作（并发）Redis带来的数据问题？ 嗯嗯这个问题我以前开发的时候遇到过，其实并发过程中确实会有这样的问题，比如下面这样的情况 系统A、B、C三个系统，分别去操作Redis的同一个Key，本来顺序是1，2，3是正常的，但是因为系统A网络突然抖动了一下，B，C在他前面操作了Redis，这样数据不就错了么。 就好比下单，支付，退款三个顺序你变了，你先退款，再下单，再支付，那流程就会失败，那数据不就乱了？你订单还没生成你却支付，退款了？明显走不通了，这在线上是很恐怖的事情。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/:5:0","tags":["redis"],"title":"分布式锁、并发竞争、双写一致性","uri":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":null,"content":"那这种情况怎么解决呢？ 我们可以找个管家帮我们管理好数据的嘛！ 某个时刻，多个系统实例都去更新某个 key。可以基于 Zookeeper 实现分布式锁。每个系统通过 Zookeeper 获取分布式锁，确保同一时间，只能有一个系统实例在操作某个 Key，别人都不允许读和写。 你要写入缓存的数据，都是从 MySQL 里查出来的，都得写入 MySQL 中，写入 MySQL 中的时候必须保存一个时间戳，从 MySQL 查出来的时候，时间戳也查出来。 每次要写之前，先判断一下当前这个 Value 的时间戳是否比缓存里的 Value 的时间戳要新。如果是的话，那么可以写，否则，就不能用旧的数据覆盖新的数据。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/:5:1","tags":["redis"],"title":"分布式锁、并发竞争、双写一致性","uri":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":null,"content":"你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？ 一般来说，如果允许缓存可以稍微的跟数据库偶尔有不一致的情况，也就是说如果你的系统不是严格要求 “缓存+数据库” 必须保持一致性的话，最好不要做这个方案，即：读请求和写请求串行化，串到一个内存队列里去。 串行化可以保证一定不会出现不一致的情况，但是它也会导致系统的吞吐量大幅度降低，用比正常情况下多几倍的机器去支撑线上的一个请求。 把一些列的操作都放到队列里面，顺序肯定不会乱，但是并发高了，这队列很容易阻塞，反而会成为整个系统的弱点，瓶颈 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/:5:2","tags":["redis"],"title":"分布式锁、并发竞争、双写一致性","uri":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":null,"content":"你了解最经典的KV、DB读写模式么？ 最经典的缓存+数据库读写的模式，就是 Cache Aside Pattern 读的时候，先读缓存，缓存没有的话，就读数据库，然后取出数据后放入缓存，同时返回响应。 更新的时候，先更新数据库，然后再删除缓存。 ###为什么是删除缓存，而不是更新缓存？ 原因很简单，很多时候，在复杂点的缓存场景，缓存不单单是数据库中直接取出来的值。 比如可能更新了某个表的一个字段，然后其对应的缓存，是需要查询另外两个表的数据并进行运算，才能计算出缓存最新的值的。 另外更新缓存的代价有时候是很高的。是不是说，每次修改数据库的时候，都一定要将其对应的缓存更新一份？也许有的场景是这样，但是对于比较复杂的缓存数据计算的场景，就不是这样了。如果你频繁修改一个缓存涉及的多个表，缓存也频繁更新。但是问题在于，这个缓存到底会不会被频繁访问到？ 举个栗子：一个缓存涉及的表的字段，在 1 分钟内就修改了 20 次，或者是 100 次，那么缓存更新 20 次、100 次；但是这个缓存在 1 分钟内只被读取了 1 次，有大量的冷数据。 实际上，如果你只是删除缓存的话，那么在 1 分钟内，这个缓存不过就重新计算一次而已，开销大幅度降低。用到缓存才去算缓存。 其实删除缓存，而不是更新缓存，就是一个 Lazy 计算的思想，不要每次都重新做复杂的计算，不管它会不会用到，而是让它到需要被使用的时候再重新计算。 像 Mybatis，Hibernate，都有懒加载思想。查询一个部门，部门带了一个员工的 List，没有必要说每次查询部门，都里面的 1000 个员工的数据也同时查出来啊。80% 的情况，查这个部门，就只是要访问这个部门的信息就可以了。先查部门，同时要访问里面的员工，那么这个时候只有在你要访问里面的员工的时候，才会去数据库里面查询 1000 个员工。 ###Redis 和 Memcached 有啥区别，为啥选择用Redis作为你们的缓存中间件？ Redis 支持复杂的数据结构： Redis 相比 Memcached 来说，拥有更多的数据结构，能支持更丰富的数据操作。如果需要缓存能够支持更复杂的结构和操作， Redis 会是不错的选择。 Redis 原生支持集群模式： 在 redis3.x 版本中，便能支持 Cluster 模式，而 Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据。 性能对比： 由于 Redis 只使用单核，而 Memcached 可以使用多核，所以平均每一个核上 Redis 在存储小数据时比 Memcached 性能更高。而在 100k 以上的数据中，Memcached 性能要高于 Redis，虽然 Redis 最近也在存储大数据的性能上进行优化，但是比起 Remcached，还是稍有逊色。 Tip：其实面试官这么问，是想看你知道为啥用这个技术栈么？你为啥选这个技术栈，你是否做过技术选型的对比，优缺点你是否了解，你啥都不知道，只是为了用而用，那你可能就差点意思了。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/:5:3","tags":["redis"],"title":"分布式锁、并发竞争、双写一致性","uri":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":null,"content":"Redis 的线程模型了解么？ Redis 内部使用文件事件处理器 file event handler，这个文件事件处理器是单线程的，所以 Redis 才叫做单线程的模型。它采用 IO 多路复用机制同时监听多个 Socket，根据 Socket 上的事件来选择对应的事件处理器进行处理。 文件事件处理器的结构包含 4 个部分： 多个 Socket IO 多路复用程序 文件事件分派器 事件处理器（连接应答处理器、命令请求处理器、命令回复处理器） 多个 Socket 可能会并发产生不同的操作，每个操作对应不同的文件事件，但是 IO 多路复用程序会监听多个 Socket，会将 Socket 产生的事件放入队列中排队，事件分派器每次从队列中取出一个事件，把该事件交给对应的事件处理器进行处理。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/:5:4","tags":["redis"],"title":"分布式锁、并发竞争、双写一致性","uri":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":null,"content":"面试结束 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/:6:0","tags":["redis"],"title":"分布式锁、并发竞争、双写一致性","uri":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":null,"content":"小伙子对你面试了四轮，你说话有理有据，逻辑清晰，来公司后肯定是一把好手，我想要不你来当我的Leader吧，哈哈？ 面试官别跟我开玩笑了，我跟您这样日积月累的技术专家还是有很多差距的，您的经验和技术上的深度，没有很长时间的磨练是无法达到的，我还得多跟您学习。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/:6:1","tags":["redis"],"title":"分布式锁、并发竞争、双写一致性","uri":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":null,"content":"好的，小伙子有点东西，你年少有为不自卑，知道什么是珍贵，就是你了来上班吧。 好的面试官，不过我想我在Java基础，MQ，Dubbo等等领域还有好多知识点您没问我，要不下次继续面我？ 强行，为吊打下一期埋伏笔哈哈，下期写啥你们定！！！ 能撑到最后，你自己都忍不住自己给自己点个赞了! （暗示点赞，每次都看了不点赞，你们想白嫖我么？你们好坏喲，不过我喜欢）。 《吊打面试官》Redis系列 —- 全剧终 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/:6:2","tags":["redis"],"title":"分布式锁、并发竞争、双写一致性","uri":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":null,"content":"总结 既然都说了是Redis的终章我最后也做个Redis方面常见面试题，题目的总结，答案大家要去思考我前面的文章基本上都提到了，结果可以去我公众号回复【答案】获取，不过我还是希望大家能看到题目就能想到答案，并且记在心中，教大家怎么回答只是帮大家组织下语言，真正的场景解决方案还是要大家理解的。 （周三以后出答案，我先睡会） 0、在集群模式下，Redis 的 Key 是如何寻址的？分布式寻址都有哪些算法？了解一致性 Hash 算法吗？ 1、使用Redis有哪些好处？ 2、Redis相比Memcached有哪些优势？ 3、Redis常见性能问题和解决方案 4、MySQL里有2000w数据，Redis中只存20w的数据，如何保证Redis中的数据都是热点数据？ 5、Memcache与Redis的区别都有哪些？ 6、Redis 常见的性能问题都有哪些？如何解决？ 7、在什么样的场景下可以充分的利用Redis的特性，大大提高Redis的效率？ 8、Redis的缓存雪崩、穿透、击穿了解么？有什么异同点？分别怎么解决？ 9、Redis的基本类型有哪些？他们的使用场景了解么？比较高级的用法你使用过么？ 10、Redis主从怎么同步数据的？集群的高可用怎么保证？持久化机制了解么？ 11、为什么 redis 单线程却能支撑高并发？ 12、如何保证缓存和数据库数据的一致性？ 13、项目中是怎么用缓存的，用了缓存之后会带来什么问题？ ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/:7:0","tags":["redis"],"title":"分布式锁、并发竞争、双写一致性","uri":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":null,"content":"絮叨+ 最后我想说的就是，我这四章只是介绍到了一些Redis面试比较常见的问题，其实还有很多点我都没回答到，大家如果为了对付面试可能是够用了，但是我们技术人员还是要保持对技术的敬畏心，你不能浅尝即止，还是要深究的。 你永远只会用，不去考虑用了会带来的问题，以及出现问题之后的解决方案，我觉得你大概率会停滞不前，既然入都入了这行了，为啥不武装一下自己。 其实学习技术是个反哺的过程，学习的时候可能你只是感觉知识广度、深度上去了，一个知识点你这样，两个、三个知识点你都这样，最后你发现你的技术已经跟身边一样P6的仔不一样了，这样你可能在团队重大项目的贡献都上去了，那P7的晋升几率是不是大了，钱是不是上去了，女朋友是不是好看了，房子是不是大了。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/:8:0","tags":["redis"],"title":"分布式锁、并发竞争、双写一致性","uri":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":null,"content":"End 好了各位，以上就是这篇文章的全部内容了，能看到这里的人呀，都是人才，我后面会每周都更新几篇《吊打面试官》系列和Java技术栈相关的文章。如果你有什么想知道的，也可以留言给我，或者去公众号加我微信，我一有时间就会写出来，我们共同进步。 非常感谢人才们能看到这里，如果这个文章写得还不错，觉得**「敖丙」**我**有点东西**的话 **求点赞👍** **求关注❤️** **求分享👥** **求留言💬** 对暖男我来说**非常有用**。 各位的支持和认可，就是我创作的最大动力，我们下篇文章见！ 敖丙 | 文 【原创】【转载请联系本人】 **《吊打面试官》**系列每周持续更新，可以关注我的公众号「 JavaFamily 」第时间阅读和催更（公众号比博客早一到两天哟），里面也有我个人微信有什么问题也可以直接滴滴我，我也是个新人，不过不影响我们一起进步，作为渣男，我给不了你幸福，还给不了你温暖嘛？ ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/:9:0","tags":["redis"],"title":"分布式锁、并发竞争、双写一致性","uri":"/posts/middleware/redis/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%B9%B6%E5%8F%91%E7%AB%9E%E4%BA%89%E5%8F%8C%E5%86%99%E4%B8%80%E8%87%B4%E6%80%A7/"},{"categories":null,"content":" 你知道的越多，你不知道的越多 点赞再看，养成习惯 GitHub上已经开源 https://github.com/JavaFamily 有一线大厂面试点脑图和个人联系方式，欢迎Star和指教 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/:0:0","tags":["redis"],"title":"秒杀系统设计","uri":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"前言 Redis在互联网技术存储方面使用如此广泛，几乎所有的后端技术面试官都要在Redis的使用和原理方面对小伙伴们进行360°的刁难。 作为一个在互联网公司面一次拿一次Offer的面霸，打败了无数竞争对手，每次都只能看到无数落寞的身影失望的离开，略感愧疚（请允许我使用一下夸张的修辞手法）。 于是在一个寂寞难耐的夜晚，我痛定思痛，决定开始写**《吊打面试官》**系列，希望能帮助各位读者以后面试势如破竹，对面试官进行360°的反击，吊打问你的面试官，让一同面试的同僚瞠目结舌，疯狂收割大厂Offer！ ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/:1:0","tags":["redis"],"title":"秒杀系统设计","uri":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"絮叨 之前写了很多Redis相关的知识点，我又大概回头看了下，除了比较底层的东西没写很深之外，我基本上的点都提到过了，我相信如果只是为了应付面试应该是够了的，但是如果你想把它们真正的吸收纳为己用，还是需要大量的知识积累，和很多实际操作的。 就我自己而言Redis在开发过程中实在用得太普遍了，热点数据的存储啊，整体性能的提升啊都会用到，但是就像我说的技术就是一把双刃剑，使用它们随之而来的问题也会很多的，我在老东家双十二就遇到缓存雪崩问题让整体服务宕机3分钟，相必大家都知道阿里今年的双十一数据了，那三分钟在这种时候到底值多少钱？真的不敢想象。 Redis的普遍我就拿掘金我自己的认知举例，不知道对不对，但是目测是对的。 **大家看到问题所在了么？**是的热门的赞的数据不是最新的，我盲猜一波上面的热门文章是缓存。失效时间应该是几十分钟的，为啥这么做呢？ 热门文章是大家共同都会看到的，也就是热点数据，在那做缓存，他是不需要那么高的实时性的，那下面的文章列表是最新发布的文章，有高实时性的特点，大家访问多的放在缓存还可以给DB减少压力，我也不知道掘金是不是这么做的哈，反正道理是这么个道理了。 那什么场景是使用Redis比较复杂的场景，而且需要大量中间件和业务逻辑去配合的呢？ 秒杀！是的就是今天的主题秒杀，我就用我自己的思路带大家一起看一下，设计一个秒杀从前到后，从内到外到底要技术人员做多少准备。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/:2:0","tags":["redis"],"title":"秒杀系统设计","uri":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"捞一下 上一期吊打系列我们提到了Redis相关的一些知识，还没看的小伙伴可以回顾一下 ，这对于这期的阅读很有帮助，涉及到主从同步、读写分离、持久化这样的知识点。 《吊打面试官》系列-Redis基础 《吊打面试官》系列-缓存雪崩、击穿、穿透 《吊打面试官》系列-Redis哨兵、持久化、主从、手撕LRU 《吊打面试官》系列-Redis终章凛冬将至、FPX新王登基 《吊打面试官》系列-Redis常见面试题（带答案） 打好基础才可以写出更好的代码哟！不然就等着产品测试怼你吧。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/:3:0","tags":["redis"],"title":"秒杀系统设计","uri":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"正文 首先设计一个系统之前，我们需要先确认我们的业务场景是怎么样子的，我就带着大家一起假设一个场景好吧。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/:4:0","tags":["redis"],"title":"秒杀系统设计","uri":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"场景 我们现场要卖100件下面这个婴儿纸尿裤，然后我们根据以往这样秒杀活动的数据经验来看，目测来抢这100件纸尿裤的人足足有10万人。（南极人打钱！） 你一听，完了呀，这我们的服务器哪里顶得住啊！说真的直接打DB肯定挂。但是别急嘛，有暖男敖丙在，我们在开始之前应该先思考下会出现哪些问题？ ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/:4:1","tags":["redis"],"title":"秒杀系统设计","uri":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"问题 高并发： 是的高并发这个是我们想都不用想的一个点，一瞬间这么多人进来这不是高并发什么时候是呢？ 是吧，秒杀的特点就是这样时间极短、 瞬间用户量大。 正常的店铺营销都是用极低的价格配合上短信、APP的精准推送，吸引特别多的用户来参与这场秒杀，爽了商家苦了开发呀。 秒杀大家都知道如果真的营销到位，价格诱人，几十万的流量我觉得完全不是问题，那单机的Redis我感觉3-4W的QPS还是能顶得住的，但是再高了就没办法了，那这个数据随便搞个热销商品的秒杀可能都不止了。 大量的请求进来，我们需要考虑的点就很多了，缓存雪崩，缓存击穿，缓存穿透这些我之前提到的点都是有可能发生的，出现问题打挂DB那就很难受了，活动失败用户体验差，活动人气没了，最后背锅的还是开发。 超卖： 但凡是个秒杀，都怕超卖，我这里举例的只是尿不湿，要是换成100个华为MatePro30，商家的预算经费卖100个可以赚点还可以造势，结果你写错程序多卖出去200个，你不发货用户投诉你，平台封你店，你发货就血亏，你怎么办？ （没事看了敖丙的文章直接不怕） 那最后只能杀个开发祭天解气了，秒杀的价格本来就低了，基本上都是不怎么赚钱的，超卖了就恐怖了呀，所以超卖也是很关键的一个点。 恶意请求： 你这么低的价格，假如我抢到了，我转手卖掉我不是血赚？就算我不卖我也不亏啊，那用户知道，你知道，别的别有用心的人（黑客、黄牛…）肯定也知道的。 那简单啊，我知道你什么时候抢，我搞个几十台机器搞点脚本，我也模拟出来十几万个人左右的请求，那我是不是意味着我基本上有80%的成功率了。 真实情况可能远远不止，因为机器请求的速度比人的手速往往快太多了，在贵州的敖丙我每年回家抢高铁票都是秒光的，我也不知道有没有黄牛的功劳，我要Diss你，黄牛。杰伦演唱会门票抢不到，我也Diss你。 Tip：科普下，小道消息了解到的，黄牛的抢票系统，比国内很多小公司的系统还吊很多，架构设计都是顶级的，我用顶配的服务加上顶配的架构设计，你还想看演唱会？还想回家？ 不过不用黄牛我回家都难，我们云贵川跟我一样要回家过年的仔太多了555！ 链接暴露： 前面几个问题大家可能都很好理解，一看到这个有的小伙伴可能会比较疑惑，啥是链接暴露呀？ 相信是个开发同学都对这个画面一点都不陌生吧，懂点行的仔都可以打开谷歌的开发者模式，然后看看你的网页代码，有的就有URL，但是我写VUE的时候是事件触发然后去调用文件里面的接口看源码看不到，但是我可以点击一下查看你的请求地址啊，不过你好像可以对按钮在秒杀前置灰。 不管怎么样子都有危险，撇开外面的所有的东西你都挡住了，你卖这个东西实在便宜得过分，有诱惑力，你能保证开发不动心？开发知道地址，在秒杀的时候自己提前请求。。。（开发：怎么TM又是我） 数据库： 每秒上万甚至十几万的QPS（每秒请求数）直接打到数据库，基本上都要把库打挂掉，而且你服务不单单是做秒杀的还涉及其他的业务，你没做降级、限流、熔断啥的，别的一起挂，小公司的话可能全站崩溃404。 反正不管你秒杀怎么挂，你别把别的搞挂了对吧，搞挂了就不是杀一个程序员能搞定的。 程序员：我TM好难啊！ ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/:4:2","tags":["redis"],"title":"秒杀系统设计","uri":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"问题都列出来了，那怎么设计，怎么解决这些问题就是接下去要考虑的了，我们对症下药。 服务单一职责： 设计个能抗住高并发的系统，我觉得还是得单一职责。 什么意思呢，大家都知道现在设计都是微服务的设计思想，然后再用分布式的部署方式 也就是我们下单是有个订单服务，用户登录管理等有个用户服务等等，那为啥我们不给秒杀也开个服务，我们把秒杀的代码业务逻辑放一起。 单独给他建立一个数据库，现在的互联网架构部署都是分库的，一样的就是订单服务对应订单库，秒杀我们也给他建立自己的秒杀库。 至于表就看大家怎么设计了，该设置索引的地方还是要设置索引的，建完后记得用explain看看SQL的执行计划。（不了解的小伙伴也没事，MySQL章节我会说的） 单一职责的好处就是就算秒杀没抗住，秒杀库崩了，服务挂了，也不会影响到其他的服务。（强行高可用） 秒杀链接加盐： 我们上面说了链接要是提前暴露出去可能有人直接访问url就提前秒杀了，那又有小伙伴要说了我做个时间的校验就好了呀，那我告诉你，知道链接的地址比起页面人工点击的还是有很大优势。 我知道url了，那我通过程序不断获取最新的北京时间，可以达到毫秒级别的，我就在00毫秒的时候请求，我敢说绝对比你人工点的成功率大太多了，而且我可以一毫秒发送N次请求，搞不好你卖100个产品我全拿了。 那这种情况怎么避免？ 简单，把URL动态化，就连写代码的人都不知道，你就通过MD5之类的加密算法加密随机的字符串去做url，然后通过前端代码获取url后台校验才能通过。 暖男我呢，又准备了一个简单的url加密给大家尝尝鲜，还不点个赞？ Redis集群： 之前不是说单机的Redis顶不住嘛，那简单多找几个兄弟啊，秒杀本来就是读多写少，那你们是不是瞬间想起来我之前跟你们提到过的，Redis集群，主从同步、读写分离，我们还搞点哨兵，开启持久化直接无敌高可用！ Nginx： Nginx大家想必都不陌生了吧，这玩意是高性能的web服务器，并发也随便顶几万不是梦，但是我们的Tomcat只能顶几百的并发呀，那简单呀负载均衡嘛，一台服务几百，那就多搞点，在秒杀的时候多租点流量机。 Tip：据我所知国内某大厂就是在去年春节活动期间租光了亚洲所有的服务器，小公司也很喜欢在双十一期间买流量机来顶住压力。 这样一对比是不是觉得你的集群能顶很多了。 恶意请求拦截也需要用到它，一般单个用户请求次数太夸张，不像人为的请求在网关那一层就得拦截掉了，不然请求多了他抢不抢得到是一回事，服务器压力上去了，可能占用网络带宽或者把服务器打崩、缓存击穿等等。 资源静态化： 秒杀一般都是特定的商品还有页面模板，现在一般都是前后端分离的，所以页面一般都是不会经过后端的，但是前端也要自己的服务器啊，那就把能提前放入cdn服务器的东西都放进去，反正把所有能提升效率的步骤都做一下，减少真正秒杀时候服务器的压力。 按钮控制： 大家有没有发现没到秒杀前，一般按钮都是置灰的，只有时间到了，才能点击。 这是因为怕大家在时间快到的最后几秒秒疯狂请求服务器，然后还没到秒杀的时候基本上服务器就挂了。 这个时候就需要前端的配合，定时去请求你的后端服务器，获取最新的北京时间，到时间点再给按钮可用状态。 按钮可以点击之后也得给他置灰几秒，不然他一样在开始之后一直点的。你敢说你们秒杀的时候不是这样的？ 限流： 限流这里我觉得应该分为前端限流和后端限流。 前端限流：这个很简单，一般秒杀不会让你一直点的，一般都是点击一下或者两下然后几秒之后才可以继续点击，这也是保护服务器的一种手段。 后端限流：秒杀的时候肯定是涉及到后续的订单生成和支付等操作，但是都只是成功的幸运儿才会走到那一步，那一旦100个产品卖光了，return了一个false，前端直接秒杀结束，然后你后端也关闭后续无效请求的介入了。 Tip：真正的限流还会有限流组件的加入例如：阿里的Sentinel、Hystrix等。我这里就不展开了，就说一下物理的限流。 库存预热： 秒杀的本质，就是对库存的抢夺，每个秒杀的用户来你都去数据库查询库存校验库存，然后扣减库存，撇开性能因数，你不觉得这样好繁琐，对业务开发人员都不友好，而且数据库顶不住啊。 开发：你tm总算为我着想一次了。 那怎么办？ 我们都知道数据库顶不住但是他的兄弟非关系型的数据库Redis能顶啊！ 那不简单了，我们要开始秒杀前你通过定时任务或者运维同学提前把商品的库存加载到Redis中去，让整个流程都在Redis里面去做，然后等秒杀介绍了，再异步的去修改库存就好了。 但是用了Redis就有一个问题了，我们上面说了我们采用主从，就是我们会去读取库存然后再判断然后有库存才去减库存，正常情况没问题，但是高并发的情况问题就很大了。 这里我就不画图了，我本来想画图的，想了半天我觉得语言可能更好表达一点。 **多品几遍！！！**就比如现在库存只剩下1个了，我们高并发嘛，4个服务器一起查询了发现都是还有1个，那大家都觉得是自己抢到了，就都去扣库存，那结果就变成了-3，是的只有一个是真的抢到了，别的都是超卖的。咋办？ ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/:4:3","tags":["redis"],"title":"秒杀系统设计","uri":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"Lua： 之前的文章就简单的提到了他，我今天就多一定点篇幅说一下吧。 Lua 脚本功能是 Reids在 2.6 版本的最大亮点， 通过内嵌对 Lua 环境的支持， Redis 解决了长久以来不能高效地处理 CAS （check-and-set）命令的缺点， 并且可以通过组合使用多个命令， 轻松实现以前很难实现或者不能高效实现的模式。 **Lua脚本是类似Redis事务，有一定的原子性，不会被其他命令插队，可以完成一些Redis事务性的操作。**这点是关键。 知道原理了，我们就写一个脚本把判断库存扣减库存的操作都写在一个脚本丢给Redis去做，那到0了后面的都Return False了是吧，一个失败了你修改一个开关，直接挡住所有的请求，然后再做后面的事情嘛。 限流\u0026降级\u0026熔断\u0026隔离： 这个为啥要做呢，不怕一万就怕万一，万一你真的顶不住了，限流，顶不住就挡一部分出去但是不能说不行，降级，降级了还是被打挂了，熔断，至少不要影响别的系统，隔离，你本身就独立的，但是你会调用其他的系统嘛，你快不行了你别拖累兄弟们啊。 削峰填谷： 一说到这个名词，很多小伙伴就知道了，对的MQ，你买东西少了你直接100个请求改库我觉得没问题，但是万一秒杀一万个，10万个呢？服务器挂了，程序员又要背锅的。 Tip：可能小伙伴说我们业务达不到这个量级，没必要。但是我想说我们写代码，就不应该写出有逻辑漏洞的代码，至少以后公司体量上去了，别人一看居然不用改代码，一看代码作者是敖丙？有点东西！ 你可以把它放消息队列，然后一点点消费去改库存就好了嘛，不过单个商品其实一次修改就够了，我这里说的是某个点多个商品一起秒杀的场景，像极了双十一零点。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/:4:4","tags":["redis"],"title":"秒杀系统设计","uri":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"总结 到这里我想我已经基本上把该考虑的点还有对应的解决方案也都说了一下，不知道还有没有没考虑到的，但是就算没考虑到我想我这个设计，应该也能撑住一个完整的秒杀流程。 （有大佬的话给敖丙点多的思路，去GitHub https://github.com/JavaFamily 上给我提，也有我的联系） 最后我就画个完整的流程图给大家收个尾吧！ Tip：这个链路还是比较简单的，很多细节的点全部画出来就太复杂了，我上面已经提到了所有的注意点了，大家都看看，真正的秒杀有比我这个简单的，也有比我这个复杂N倍的，之前的电商老东家就做的很高级，有机会也可以跟你们探讨，不过是面试嘛，我就给思路，让你理解比较关键的点。 秒杀这章我脑细胞死了很多，考虑了很多个点，最后还是出来了，忍不住给自己点赞！ （这章是真的不要白嫖，每次都看了不点赞，你们想白嫖我么？你们好坏喲，不过我好喜欢） ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/:5:0","tags":["redis"],"title":"秒杀系统设计","uri":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"总结 我们玩归玩，闹归闹，别拿面试开玩笑。 秒杀不一定是每个同学都会问到的，至少肯定没Redis基础那样常问，但是一旦问到，大家一定要回答到点上。 至少你得说出可能出现的情况，需要注意的情况，以及对于的解决思路和方案。 最后就是需要对整个链路比较熟悉，注意是一个完整的链路，前端怎么设计的呀，网关的作用呀，怎么解决Redis的并发竞争啊，数据的同步方式呀，MQ的作用啊。 （提到MQ又是一整条的知识链路，什么异步、削峰、解耦等等，所以面试，我们还是不打没有把握的胜仗） ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/:6:0","tags":["redis"],"title":"秒杀系统设计","uri":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"流着泪说再见 Redis系列到此是真的要跟大家说再见了，写了7篇文章，其实很多大佬的思路和片段真心赞，其实大家看出来了我的文章个人风格色彩特别浓厚，我个人在生活中就是这么说话的，也希望用这种风格把原本枯燥乏味的知识点让大家都像看小说一样津津有味的看下去，不知道大家什么感受，好的不好的都请给我留言。 我这个系列的我会写到我GitHub https://github.com/JavaFamily 图中所有的知识点，以后就麻烦大家多多关照了，我写作的时间都是业余时间，基本上周末和晚上的时间都贡献出来了，我也是个新人很多点也没接触到，也要看书看资料才能写出来，所以有时候还是希望大家多多包涵。 那我们下期见！ 下期写________________？ 不告诉你，哈哈！ ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/:6:1","tags":["redis"],"title":"秒杀系统设计","uri":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":"日常求赞 好了各位，以上就是这篇文章的全部内容了，能看到这里的人呀，都是人才。 我后面会每周都更新几篇《吊打面试官》系列和互联网常用技术栈相关的文章，非常感谢人才们能看到这里，如果这个文章写得还不错，觉得「敖丙」我有点东西的话 求点赞👍 求关注❤️ 求分享👥 对暖男我来说真的 非常有用！！！ 创作不易，各位的支持和认可，就是我创作的最大动力，我们下篇文章见！ 敖丙 | 文 【原创】【转载请联系本人】 如果本篇博客有任何错误，请批评指教，不胜感激 ！ 《吊打面试官》系列每周持续更新，可以关注我的公众号「JavaFamily」第一时间阅读和催更（公众号比博客早一到两篇哟），本文GitHub上已经收录https://github.com/JavaFamily，有一线大厂面试点思维导图，欢迎Star和完善，里面也有我个人联系方式有什么问题也可以直接找我，也有人才交流群，我们一起有点东西。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/:7:0","tags":["redis"],"title":"秒杀系统设计","uri":"/posts/middleware/redis/%E7%A7%92%E6%9D%80%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/"},{"categories":null,"content":" 你知道的越多，你不知道的越多 点赞再看，养成习惯 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/:0:0","tags":["redis"],"title":"缓存击穿、雪崩、穿透","uri":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/"},{"categories":null,"content":"前言 Redis在互联网技术存储方面使用如此广泛，几乎所有的后端技术面试官都要在Redis的使用和原理方面对小伙伴们进行360°的刁难。作为一个在互联网公司面一次拿一次offer的面霸（请允许我使用一下夸张的修辞手法），打败了无数竞争对手，每次都只能看到无数落寞的身影失望的离开，略感愧疚，在一个寂寞难耐的夜晚，我痛定思痛，决定开始写**《吊打面试官》**系列，希望能帮助各位读者以后面试势如破竹，对面试官进行360°的反击，吊打问你的面试官，让一同面试的同僚瞠目结舌，疯狂收割大厂offer！ ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/:1:0","tags":["redis"],"title":"缓存击穿、雪崩、穿透","uri":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/"},{"categories":null,"content":"一点感慨 本来都把稿子放到公众号保存了，洗澡的时候想了一下晚上的比赛，觉得还是打开电脑写点东西，跟文章内容没关系，只是一点个人的感慨，不知道多少小伙伴看了昨天SKT VS G2的比赛，又不知道多少小伙伴还记得Faker手抖的那一幕。 不知道你们看了是什么感受，我看到他手抖的时候我内心也抖了，世界赛我支持的都是LPL的队伍，但是我喜欢李哥这个人，那种对胜利的执著，这么多年了那种坚持自己的坚持，这么多利益诱惑在面前却只想要胜利，这样的人我好喜欢啊，我想很多人也喜欢。 可能就像很多网友说的那样，英雄迟暮，但是我觉得他还是有点东西，就像很多人说我们程序员只能吃年轻饭一样，但是如果你坚持自己的坚持，做个腹有诗书气自华的仔，我想最后肯定会得到自己的得到。 好了我也不煽情了，我们开始讲技术吧。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/:2:0","tags":["redis"],"title":"缓存击穿、雪崩、穿透","uri":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/"},{"categories":null,"content":"正文 上一期吊打系列我们提到了Redis的基础知识，还没看的小伙伴可以回顾一下 《吊打面试官》系列-Redis基础 那提到Redis我相信各位在面试，或者实际开发过程中对缓存雪崩，穿透，击穿也不陌生吧，就算没遇到过但是你肯定听过，那三者到底有什么区别，我们又应该怎么去防止这样的情况发生呢，我们有请下一位受害者。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/:3:0","tags":["redis"],"title":"缓存击穿、雪崩、穿透","uri":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/"},{"categories":null,"content":"面试开始 一个大腹便便，穿着格子衬衣的中年男子，拿着一个满是划痕的mac向你走来，看着快秃顶的头发，心想着肯定是尼玛顶级架构师吧！但是我们腹有诗书气自华，虚都不虚。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/:4:0","tags":["redis"],"title":"缓存击穿、雪崩、穿透","uri":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/"},{"categories":null,"content":"小伙子我看你的简历上写到了Redis，那么我们直接开门见山，直接怼常见的几个大问题，Redis雪崩了解么？ 帅气迷人的面试官您好，我了解的，目前电商首页以及热点数据都会去做缓存 ，一般缓存都是定时任务去刷新，或者是查不到之后去更新的，定时任务刷新就有一个问题。 举个简单的例子：如果所有首页的Key失效时间都是12小时，中午12点刷新的，我零点有个秒杀活动大量用户涌入，假设当时每秒 6000 个请求，本来缓存在可以扛住每秒 5000 个请求，但是缓存当时所有的Key都失效了。此时 1 秒 6000 个请求全部落数据库，数据库必然扛不住，它会报一下警，真实情况可能DBA都没反应过来就直接挂了。此时，如果没用什么特别的方案来处理这个故障，DBA 很着急，重启数据库，但是数据库立马又被新的流量给打死了。这就是我理解的缓存雪崩。 我刻意看了下我做过的项目感觉再吊的都不允许这么大的QPS直接打DB去，不过没慢SQL加上分库，大表分表可能还还算能顶，但是跟用了Redis的差距还是很大 同一时间大面积失效，那一瞬间Redis跟没有一样，那这个数量级别的请求直接打到数据库几乎是灾难性的，你想想如果打挂的是一个用户服务的库，那其他依赖他的库所有的接口几乎都会报错，如果没做熔断等策略基本上就是瞬间挂一片的节奏，你怎么重启用户都会把你打挂，等你能重启的时候，用户早就睡觉去了，并且对你的产品失去了信心，什么垃圾产品。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/:4:1","tags":["redis"],"title":"缓存击穿、雪崩、穿透","uri":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/"},{"categories":null,"content":"面试官摸了摸自己的头发，嗯还不错，那这种情况咋整？你都是怎么去应对的？ 处理缓存雪崩简单，在批量往Redis存数据的时候，把每个Key的失效时间都加个随机值就好了，这样可以保证数据不会在同一时间大面积失效，我相信，Redis这点流量还是顶得住的。 setRedis（Key，value，time + Math.random() * 10000）； 如果Redis是集群部署，将热点数据均匀分布在不同的Redis库中也能避免全部失效的问题，不过本渣我在生产环境中操作集群的时候，单个服务都是对应的单个Redis分片，是为了方便数据的管理，但是也同样有了可能会失效这样的弊端，失效时间随机是个好策略。 或者设置热点数据永远不过期，有更新操作就更新缓存就好了（比如运维更新了首页商品，那你刷下缓存就完事了，不要设置过期时间），电商首页的数据也可以用这个操作，保险。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/:4:2","tags":["redis"],"title":"缓存击穿、雪崩、穿透","uri":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/"},{"categories":null,"content":"那你了解缓存穿透和击穿么，可以说说他们跟雪崩的区别么？ 嗯，了解，我先说一下缓存穿透吧，缓存穿透是指缓存和数据库中都没有的数据，而用户不断发起请求，我们数据库的 id 都是1开始自增上去的，如发起为id值为 -1 的数据或 id 为特别大不存在的数据。这时的用户很可能是攻击者，攻击会导致数据库压力过大，严重会击垮数据库。 小点的单机系统，基本上用postman就能搞死，比如我自己买的阿里云服务 像这种你如果不对参数做校验，数据库id都是大于0的，我一直用小于0的参数去请求你，每次都能绕开Redis直接打到数据库，数据库也查不到，每次都这样，并发高点就容易崩掉了。 至于缓存击穿嘛，这个跟缓存雪崩有点像，但是又有一点不一样，缓存雪崩是因为大面积的缓存失效，打崩了DB，而缓存击穿不同的是缓存击穿是指一个Key非常热点，在不停的扛着大并发，大并发集中对这一个点进行访问，当这个Key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就像在一个完好无损的桶上凿开了一个洞。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/:4:3","tags":["redis"],"title":"缓存击穿、雪崩、穿透","uri":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/"},{"categories":null,"content":"面试官露出欣慰的眼光，那他们分别怎么解决 缓存穿透我会在接口层增加校验，比如用户鉴权校验，参数做校验，不合法的参数直接代码Return，比如：id 做基础校验，id \u003c=0的直接拦截等。 这里我想提的一点就是，我们在开发程序的时候都要有一颗“不信任”的心，就是不要相信任何调用方，比如你提供了API接口出去，你有这几个参数，那我觉得作为被调用方，任何可能的参数情况都应该被考虑到，做校验，因为你不相信调用你的人，你不知道他会传什么参数给你。 举个简单的例子，你这个接口是分页查询的，但是你没对分页参数的大小做限制，调用的人万一一口气查 Integer.MAX_VALUE 一次请求就要你几秒，多几个并发你不就挂了么？是公司同事调用还好大不了发现了改掉，但是如果是黑客或者竞争对手呢？在你双十一当天就调你这个接口会发生什么，就不用我说了吧。这是之前的Leader跟我说的，我觉得大家也都应该了解下。 从缓存取不到的数据，在数据库中也没有取到，这时也可以将对应Key的Value对写为null、位置错误、稍后重试这样的值具体取啥问产品，或者看具体的场景，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况也没法使用）。 这样可以防止攻击用户反复用同一个id暴力攻击，但是我们要知道正常用户是不会在单秒内发起这么多次请求的，那网关层Nginx本渣我也记得有配置项，可以让运维大大对单个IP每秒访问次数超出阈值的IP都拉黑。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/:4:4","tags":["redis"],"title":"缓存击穿、雪崩、穿透","uri":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/"},{"categories":null,"content":"那你还有别的办法么？ 还有我记得Redis还有一个高级用法**布隆过滤器（Bloom Filter）**这个也能很好的防止缓存穿透的发生，他的原理也很简单就是利用高效的数据结构和算法快速判断出你这个Key是否在数据库中存在，不存在你return就好了，存在你就去查了DB刷新KV再return。 那又有小伙伴说了如果黑客有很多个IP同时发起攻击呢？这点我一直也不是很想得通，但是一般级别的黑客没这么多肉鸡，再者正常级别的Redis集群都能抗住这种级别的访问的，小公司我想他们不会感兴趣的。把系统的高可用做好了，集群还是很能顶的。 缓存击穿的话，设置热点数据永远不过期。或者加上互斥锁就能搞定了 作为暖男，代码我肯定帮你们准备好了 ##面试结束 ### 嗯嗯还不错，三个点都回答得很好，今天也不早了，面试就先到这里，明天你再过来二面我继续问一下你关于Redis集群高可用，主从同步，哨兵等知识点的问题。 晕居然还有下一轮面试！（强行下一期的伏笔哈哈）但是为了offer还是得舔，嗯嗯，好的帅气面试官。 能回答得这么全面这么细节还是忍不住点赞 （**暗示点赞，每次都看了不点赞，你们想白嫖我么？你们好坏喲，不过我喜欢**） ## 总结 我们玩归玩，闹归闹，别拿面试开玩笑。 本文简单的介绍了，**Redis**的**雪崩**，**击穿**，**穿透**，三者其实都差不多，但是又有一些区别，在面试中其实这是问到缓存必问的，大家不要把三者搞混了，因为缓存雪崩、穿透和击穿，是缓存最大的问题，要么不出现，一旦出现就是致命性的问题，所以面试官一定会问你。 大家一定要理解是**怎么发生的**，以及是怎么去**避免**的，发生之后又怎么去**抢救**，你可以不是知道很深入，但是你不能一点都不去想，面试有时候不一定是对知识面的拷问，或许是对你的态度的拷问，如果你思路清晰，然后**知其然还知其所以然**那就很赞，还知道怎么预防那来上班吧。 ### 最后暖男我继续给你们做个小的技术总结： 一般避免以上情况发生我们从三个时间段去分析下： - 事前：**Redis** 高可用，主从+哨兵，**Redis cluster**，避免全盘崩溃。 - 事中：本地 **ehcache** 缓存 + **Hystrix** 限流+降级，避免** MySQL** 被打死。 事后：Redis 持久化 RDB+AOF，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。 上面的几点我会在吊打系列Redis篇全部讲一下这个月应该可以吧Redis更完，限流组件，可以设置每秒的请求，有多少能通过组件，剩余的未通过的请求，怎么办？走降级！可以返回一些默认的值，或者友情提示，或者空白的值。 好处： 数据库绝对不会死，限流组件确保了每秒只有多少个请求能通过。 只要数据库不死，就是说，对用户来说，3/5 的请求都是可以被处理的。 只要有 3/5 的请求可以被处理，就意味着你的系统没死，对用户来说，可能就是点击几次刷不出来页面，但是多点几次，就可以刷出来一次。 这个在目前主流的互联网大厂里面是最常见的，你是不是好奇，某明星爆出什么事情，你发现你去微博怎么刷都空白界面，但是有的人又直接进了，你多刷几次也出来了，现在知道了吧，那是做了降级，牺牲部分用户的体验换来服务器的安全，可还行？ 好了各位，以上就是这篇文章的全部内容了，能看到这里的人呀，都是人才，我后面会每周都更新几篇《吊打面试官》系列和Java技术栈相关的文章。如果你有什么想知道的，也可以留言给我，我一有时间就会写出来，我们共同进步。 非常感谢靓仔/靓女您能看到这里，如果这个文章写得还不错的话 求点赞 求关注 求分享 求留言 **（对我非常有用）**各位的支持和认可，就是我创作的最大动力，我们下篇文章见，拜了个拜！ 敖丙 | 文 【原创】 每周都会持续更新《吊打面试官》系列可以关注我的公众号第一时间阅读和催更，也可以在公众号回复【人才】加入人才交流群一起讨论面试题，就业和工作上有什么问题也可以直接滴滴我，我也是个新人，不过不影响我们一起进步，作为渣男，我给不了你工作，还给不了你温暖嘛？ ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/:4:5","tags":["redis"],"title":"缓存击穿、雪崩、穿透","uri":"/posts/middleware/redis/%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F/"},{"categories":null,"content":" 你知道的越多，你不知道的越多 点赞再看，养成习惯 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/:0:0","tags":["redis"],"title":"集群高可用、哨兵、持久化、LRU","uri":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/"},{"categories":null,"content":"前言 Redis在互联网技术存储方面使用如此广泛，几乎所有的后端技术面试官都要在Redis的使用和原理方面对小伙伴们进行360°的刁难。作为一个在互联网公司面一次拿一次offer的面霸（请允许我使用一下夸张的修辞手法），打败了无数竞争对手，每次都只能看到无数落寞的身影失望的离开，略感愧疚，在一个寂寞难耐的夜晚，我痛定思痛，决定开始写**《吊打面试官》**系列，希望能帮助各位读者以后面试势如破竹，对面试官进行360°的反击，吊打问你的面试官，让一同面试的同僚瞠目结舌，疯狂收割大厂**Offer**！ ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/:1:0","tags":["redis"],"title":"集群高可用、哨兵、持久化、LRU","uri":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/"},{"categories":null,"content":"絮叨 写这期其实比较纠结，我之前的写的比较通俗易懂，一是我都知道这些点，二是之前我在所在的电商公司对雪崩，击穿啥的还算有场景去接触。但是线上的Redis集群我实际操作经验很少，总不能在公司线上环境实践那些操作吧，所以最后看了下官网，还有一些资料（文章后面我都会贴出来），强行怼了这么篇出来。 最近双十一小忙，周末双十一值班目测没时间写，那我是暖男呀，我不能鸽啊，就有了这一篇，下一篇迟到你们不要喷我哈，而且下一篇还是Redis的终章还是得构思下，不熟悉的知识点我怕漏洞多，特意让以前的大牛同事看了下，所以有啥不对的地方大家及时留言Diss我，写这篇是真的难，诺下面就是我本人某天凌晨两点的拍的视频，多动症的仔。 之前说过系列第二篇到300赞我就发第三篇 咋样没骗你们吧，就很枯竭，不BB了，开搞。 不点个赞对不起我，这次不要白嫖我！ ##正文 上几期**《吊打面试官》**还没看的小伙伴可以回顾一下（明明就写了两期说的好像很多一样）！ 《吊打面试官》系列-Redis基础 《吊打面试官》系列-缓存雪崩、击穿、穿透 大家都知道一个技术的引入方便了开发，解决了各种问题，但是也会带来对应的问题，技术是把双刃剑嘛，集群的引入也会带来很多问题，如：集群的高可用怎么保证，数据怎么同步等等，我们话不多说，有请下一位受害者为我们展示。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/:2:0","tags":["redis"],"title":"集群高可用、哨兵、持久化、LRU","uri":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/"},{"categories":null,"content":"面试开始 三个大腹便便，穿着格子衬衣的中年男子，拿着三个满是划痕的mac向你走来，看着快秃顶的头发，心想着肯定是尼玛顶级架构师吧！而且还是三个，但是还好我看过敖丙写的《吊打面试官》系列，腹有诗书气自华，根本虚都不虚好伐。 小伙子你好，之前问过了你基础知识以及一些缓存的常见几个大问题了，那你能跟我聊聊为啥Redis那么快么？ 哦，帅气迷人的面试官您好，我们可以先看一下关系型数据库跟Redis本质上的区别。 Redis采用的是基于内存的采用的是单进程单线程模型的 KV 数据库，由C语言编写，官方提供的数据是可以达到100000+的QPS（每秒内查询次数）。 完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。它的，数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)； 数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的； 采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗； 使用多路I/O复用模型，非阻塞IO； 使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求； ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/:2:1","tags":["redis"],"title":"集群高可用、哨兵、持久化、LRU","uri":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/"},{"categories":null,"content":"我可以问一下啥是上下文切换么？ 我可以打个比方么：我记得有过一个小伙伴微信问过我上下文切换是啥，为啥可能会线程不安全，我是这么说的，就好比你看一本英文书，你看到第十页发现有个单词不会读，你加了个书签，然后去查字典，过了一会你又回来继续从书签那里读，ok到目前为止没啥问题。 如果是你一个人读肯定没啥问题，但是你去查的时候，别的小伙伴好奇你在看啥他就翻了一下你的书，然后溜了，哦豁，你再看的时候就发现书不是你看的那一页了。不知道到这里为止我有没有解释清楚，以及为啥会线程不安全，就是因为你一个人怎么看都没事，但是人多了换来换去的操作一本书数据就乱了。可能我的解释很粗糙，但是道理应该是一样的。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/:2:2","tags":["redis"],"title":"集群高可用、哨兵、持久化、LRU","uri":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/"},{"categories":null,"content":"那他是单线程的，我们现在服务器都是多核的，那不是很浪费？ 是的他是单线程的，但是，我们可以通过在单机开多个Redis实例嘛。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/:2:3","tags":["redis"],"title":"集群高可用、哨兵、持久化、LRU","uri":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/"},{"categories":null,"content":"既然提到了单机会有瓶颈，那你们是怎么解决这个瓶颈的？ 我们用到了集群的部署方式也就是Redis cluster，并且是主从同步读写分离，类似Mysql的主从同步，Redis cluster 支撑 N 个 Redis master node，每个master node都可以挂载多个 slave node。 这样整个 Redis 就可以横向扩容了。如果你要支撑更大数据量的缓存，那就横向扩容更多的 master 节点，每个 master 节点就能存放更多的数据了。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/:2:4","tags":["redis"],"title":"集群高可用、哨兵、持久化、LRU","uri":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/"},{"categories":null,"content":"哦？那问题就来了，他们之间是怎么进行数据交互的？以及Redis是怎么进行持久化的？Redis数据都在内存中，一断电或者重启不就木有了嘛？ 是的，持久化的话是Redis高可用中比较重要的一个环节，因为Redis数据在内存的特性，持久化必须得有，我了解到的持久化是有两种方式的。 RDB：RDB 持久化机制，是对 Redis 中的数据执行周期性的持久化。 AOF：AOF 机制对每条写入命令作为日志，以 append-only 的模式写入一个日志文件中，因为这个模式是只追加的方式，所以没有任何磁盘寻址的开销，所以很快，有点像Mysql中的binlog。 两种方式都可以把Redis内存中的数据持久化到磁盘上，然后再将这些数据备份到别的地方去，RDB更适合做冷备，AOF更适合做热备，比如我杭州的某电商公司有这两个数据，我备份一份到我杭州的节点，再备份一个到上海的，就算发生无法避免的自然灾害，也不会两个地方都一起挂吧，这灾备也就是异地容灾，地球毁灭他没办法。 tip：两种机制全部开启的时候，Redis在重启的时候会默认使用AOF去重新构建数据，因为AOF的数据是比RDB更完整的。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/:2:5","tags":["redis"],"title":"集群高可用、哨兵、持久化、LRU","uri":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/"},{"categories":null,"content":"那这两种机制各自优缺点是啥？ 我先说RDB吧 优点： 他会生成多个数据文件，每个数据文件分别都代表了某一时刻Redis里面的数据，这种方式，有没有觉得很适合做冷备，完整的数据运维设置定时任务，定时同步到远端的服务器，比如阿里的云服务，这样一旦线上挂了，你想恢复多少分钟之前的数据，就去远端拷贝一份之前的数据就好了。 RDB对Redis的性能影响非常小，是因为在同步数据的时候他只是fork了一个子进程去做持久化的，而且他在数据恢复的时候速度比AOF来的快。 缺点： RDB都是快照文件，都是默认五分钟甚至更久的时间才会生成一次，这意味着你这次同步到下次同步这中间五分钟的数据都很可能全部丢失掉。AOF则最多丢一秒的数据，数据完整性上高下立判。 还有就是RDB在生成数据快照的时候，如果文件很大，客户端可能会暂停几毫秒甚至几秒，你公司在做秒杀的时候他刚好在这个时候fork了一个子进程去生成一个大快照，哦豁，出大问题。 我们再来说说AOF ####优点： 上面提到了，RDB五分钟一次生成快照，但是AOF是一秒一次去通过一个后台的线程fsync操作，那最多丢这一秒的数据。 AOF在对日志文件进行操作的时候是以append-only的方式去写的，他只是追加的方式写数据，自然就少了很多磁盘寻址的开销了，写入性能惊人，文件也不容易破损。 AOF的日志是通过一个叫非常可读的方式记录的，这样的特性就适合做灾难性数据误删除的紧急恢复了，比如公司的实习生通过flushall清空了所有的数据，只要这个时候后台重写还没发生，你马上拷贝一份AOF日志文件，把最后一条flushall命令删了就完事了。 tip：我说的命令你们别真去线上系统操作啊，想试去自己买的服务器上装个Redis试，别到时候来说，敖丙真是个渣男，害我把服务器搞崩了，Redis官网上的命令都去看看，不要乱试！！！ ####缺点： 一样的数据，AOF文件比RDB还要大。 AOF开启后，Redis支持写的QPS会比RDB支持写的要低，他不是每秒都要去异步刷新一次日志嘛fsync，当然即使这样性能还是很高，我记得ElasticSearch也是这样的，异步刷新缓存区的数据去持久化，为啥这么做呢，不直接来一条怼一条呢，那我会告诉你这样性能可能低到没办法用的，大家可以思考下为啥哟。 ###那两者怎么选择？ 小孩子才做选择，我全都要，你单独用RDB你会丢失很多数据，你单独用AOF，你数据恢复没RDB来的快，真出什么时候第一时间用RDB恢复，然后AOF做数据补全，真香！冷备热备一起上，才是互联网时代一个高健壮性系统的王道。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/:2:6","tags":["redis"],"title":"集群高可用、哨兵、持久化、LRU","uri":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/"},{"categories":null,"content":"看不出来年纪轻轻有点东西的呀，对了我听你提到了高可用，Redis还有其他保证集群高可用的方式么？ ！！！晕 自己给自己埋个坑（其实是明早就准备好了，故意抛出这个词等他问，就怕他不问）。 假装思考一会（不要太久，免得以为你真的不会），哦我想起来了，还有哨兵集群sentinel。 哨兵必须用三个实例去保证自己的健壮性的，哨兵+主从并不能保证数据不丢失，但是可以保证集群的高可用。 为啥必须要三个实例呢？我们先看看两个哨兵会咋样。 master宕机了 s1和s2两个哨兵只要有一个认为你宕机了就切换了，并且会选举出一个哨兵去执行故障，但是这个时候也需要大多数哨兵都是运行的。 那这样有啥问题呢？M1宕机了，S1没挂那其实是OK的，但是整个机器都挂了呢？哨兵就只剩下S2个裸屌了，没有哨兵去允许故障转移了，虽然另外一个机器上还有R1，但是故障转移就是不执行。 经典的哨兵集群是这样的： M1所在的机器挂了，哨兵还有两个，两个人一看他不是挂了嘛，那我们就选举一个出来执行故障转移不就好了。 暖男我，小的总结下哨兵组件的主要功能： 集群监控：负责监控 Redis master 和 slave 进程是否正常工作。 消息通知：如果某个 Redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。 故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。 配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/:2:7","tags":["redis"],"title":"集群高可用、哨兵、持久化、LRU","uri":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/"},{"categories":null,"content":"我记得你还提到了主从同步，能说一下主从之间的数据怎么同步的么？ 面试官您的记性可真是一级棒呢，我都要忘了你还记得，我特么谢谢你，提到这个，就跟我前面提到的数据持久化的RDB和AOF有着比密切的关系了。 我先说下为啥要用主从这样的架构模式，前面提到了单机QPS是有上限的，而且Redis的特性就是必须支撑读高并发的，那你一台机器又读又写，这谁顶得住啊，不当人啊！但是你让这个master机器去写，数据同步给别的slave机器，他们都拿去读，分发掉大量的请求那是不是好很多，而且扩容的时候还可以轻松实现水平扩容。 回归正题，他们数据怎么同步的呢？ 你启动一台slave 的时候，他会发送一个psync命令给master ，如果是这个slave第一次连接到master，他会触发一个全量复制。master就会启动一个线程，生成RDB快照，还会把新的写请求都缓存在内存中，RDB文件生成后，master会将这个RDB发送给slave的，slave拿到之后做的第一件事情就是写进本地的磁盘，然后加载进内存，然后master会把内存里面缓存的那些新命名都发给slave。 ###数据传输的时候断网了或者服务器挂了怎么办啊？ 传输过程中有什么网络问题啥的，会自动重连的，并且连接之后会把缺少的数据补上的。 大家需要记得的就是，RDB快照的数据生成的时候，缓存区也必须同时开始接受新请求，不然你旧的数据过去了，你在同步期间的增量数据咋办？是吧？ ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/:2:8","tags":["redis"],"title":"集群高可用、哨兵、持久化、LRU","uri":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/"},{"categories":null,"content":"那说了这么多你能说一下他的内存淘汰机制么，来手写一下LRU代码？ 手写LRU？你是不是想直接跳起来说一句：Are U F**k Kidding me？ 这个问题是我在蚂蚁金服三面的时候亲身被问过的问题，不知道大家有没有被怼到过这个问题。 Redis的过期策略，是有定期删除+惰性删除两种。 定期好理解，默认100s就随机抽一些设置了过期时间的key，去检查是否过期，过期了就删了。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/:2:9","tags":["redis"],"title":"集群高可用、哨兵、持久化、LRU","uri":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/"},{"categories":null,"content":"为啥不扫描全部设置了过期时间的key呢？ 假如Redis里面所有的key都有过期时间，都扫描一遍？那太恐怖了，而且我们线上基本上也都是会设置一定的过期时间的。全扫描跟你去查数据库不带where条件不走索引全表扫描一样，100s一次，Redis累都累死了。 ###如果一直没随机到很多key，里面不就存在大量的无效key了？ 好问题，惰性删除，见名知意，惰性嘛，我不主动删，我懒，我等你来查询了我看看你过期没，过期就删了还不给你返回，没过期该怎么样就怎么样。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/:2:10","tags":["redis"],"title":"集群高可用、哨兵、持久化、LRU","uri":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/"},{"categories":null,"content":"最后就是如果的如果，定期没删，我也没查询，那可咋整？ 内存淘汰机制！ 官网上给到的内存淘汰机制是以下几个： noeviction:返回错误当内存限制达到并且客户端尝试执行会让更多内存被使用的命令（大部分的写入指令，但DEL和几个例外） allkeys-lru: 尝试回收最少使用的键（LRU），使得新添加的数据有空间存放。 volatile-lru: 尝试回收最少使用的键（LRU），但仅限于在过期集合的键,使得新添加的数据有空间存放。 allkeys-random: 回收随机的键使得新添加的数据有空间存放。 volatile-random: 回收随机的键使得新添加的数据有空间存放，但仅限于在过期集合的键。 volatile-ttl: 回收在过期集合的键，并且优先回收存活时间（TTL）较短的键,使得新添加的数据有空间存放。 如果没有键满足回收的前提条件的话，策略volatile-lru, volatile-random以及volatile-ttl就和noeviction 差不多了。 至于LRU我也简单提一下，手写实在是太长了，大家可以去Redis官网看看，我把近视LUR效果给大家看看 tip：Redis为什么不使用真实的LRU实现是因为这需要太多的内存。不过近似的LRU算法对于应用而言应该是等价的。使用真实的LRU算法与近似的算法可以通过下面的图像对比。 你可以看到三种点在图片中, 形成了三种带. 浅灰色带是已经被回收的对象。 灰色带是没有被回收的对象。 绿色带是被添加的对象。 在LRU实现的理论中，我们希望的是，在旧键中的第一半将会过期。Redis的LRU算法则是概率的过期旧的键。 你可以看到，在都是五个采样的时候Redis 3.0比Redis 2.8要好，Redis2.8中在最后一次访问之间的大多数的对象依然保留着。使用10个采样大小的Redis 3.0的近似值已经非常接近理论的性能。 注意LRU只是个预测键将如何被访问的模型。另外，如果你的数据访问模式非常接近幂定律，大部分的访问将集中在一个键的集合中，LRU的近似算法将处理得很好。 其实在大家熟悉的LinkedHashMap中也实现了Lru算法的，实现如下： 当容量超过100时，开始执行LRU策略：将最近最少未使用的 TimeoutInfoHolder 对象 evict 掉。 真实面试中会让你写LUR算法，你可别搞原始的那个，那真TM多，写不完的，你要么怼上面这个，要么怼下面这个，找一个数据结构实现下Java版本的LRU还是比较容易的，知道啥原理就好了。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/:2:11","tags":["redis"],"title":"集群高可用、哨兵、持久化、LRU","uri":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/"},{"categories":null,"content":"面试结束 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/:3:0","tags":["redis"],"title":"集群高可用、哨兵、持久化、LRU","uri":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/"},{"categories":null,"content":"小伙子，你确实有点东西，HRBP会联系你的，请务必保持你的手机畅通好么？ 好的谢谢面试官，面试官真好，我还想再面几次，噗此。 能回答得这么全面这么细节还是忍不住点赞 （暗示点赞，每次都看了不点赞，你们想白嫖我么？你们好坏喲，不过我好喜欢） ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/:3:1","tags":["redis"],"title":"集群高可用、哨兵、持久化、LRU","uri":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/"},{"categories":null,"content":"总结 好了，我们玩归玩，闹归闹，别拿面试开玩笑，我这么写是为了节目效果，大家面试请认真对待。 这一期是这期没前面好理解了对吧，我就在自己的服务器上启动了，然后再去官网看看命令一顿瞎操作的，查阅了部分资料，这里给大家推荐几本经典的Redis入门的书籍和我参考的资料。 [Redis中文官网](http://www.redis.cn/) 《Redis入门指南(第2版)》 《Redis实战》 《Redis设计与实现》 《大型网站技术架构——李智慧》 《Redis 设计与实现——黄健宏》 《Redis 深度历险——钱文品》 《亿级流量网站架构核心技术——张开涛》 《中华石杉——石杉》 不出意外的话这是Redis的倒数第二期，最后一期不知道写啥还没想好，我得好好想想，加上最近不是双十一嘛得加加班，你看看开头的我，多可怜，那还不点个赞？买个服务器？不确定下一期多久出，想早点看到更新的小伙伴可以去公众号催更，公众号提前一到两天更新。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/:4:0","tags":["redis"],"title":"集群高可用、哨兵、持久化、LRU","uri":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/"},{"categories":null,"content":"END 好了各位，以上就是这篇文章的全部内容了，能看到这里的人呀，都是人才，我后面会每周都更新几篇《吊打面试官》系列和Java技术栈相关的文章。如果你有什么想知道的，也可以留言给我，我一有时间就会写出来，我们共同进步。 非常感谢靓仔/靓女您能看到这里，如果这个文章写得还不错，觉得敖丙有点东西 求点赞 求关注 求分享 求留言 **（对我非常有用）**各位的支持和认可，就是我创作的最大动力，我们下篇文章见！ 敖丙 | 文 【原创】 每周都会持续更新《吊打面试官》系列可以关注我的公众号第一时间阅读和催更，公众号比博客提前一到两天更新，也可以在公众号回复【人才】加入人才交流群，里面都是人才长得好看说话还好听，进去就像回家了一样，就业和工作上有什么问题也可以直接滴滴我，我也是个新人，不过不影响我们一起进步。 ","date":"2022-03-26","objectID":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/:4:1","tags":["redis"],"title":"集群高可用、哨兵、持久化、LRU","uri":"/posts/middleware/redis/%E9%9B%86%E7%BE%A4%E9%AB%98%E5%8F%AF%E7%94%A8%E5%93%A8%E5%85%B5%E6%8C%81%E4%B9%85%E5%8C%96lru/"},{"categories":null,"content":"网络分层结构 计算机网络体系大致分为三种，OSI七层模型、TCP/IP四层模型和五层模型。一般面试的时候考察比较多的是五层模型。 TCP/IP五层模型：应用层、传输层、网络层、数据链路层、物理层。 应用层：为应用程序提供交互服务。在互联网中的应用层协议很多，如域名系统DNS、HTTP协议、SMTP协议等。 传输层：负责向两台主机进程之间的通信提供数据传输服务。传输层的协议主要有传输控制协议TCP和用户数据协议UDP。 网络层：选择合适的路由和交换结点，确保数据及时传送。主要包括IP协议。 数据链路层：在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。 物理层：实现相邻节点间比特流的透明传输，尽可能屏蔽传输介质和物理设备的差异。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:1:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"三次握手 假设发送端为客户端，接收端为服务端。开始时客户端和服务端的状态都是CLOSED。 第一次握手：客户端向服务端发起建立连接请求，客户端会随机生成一个起始序列号x，客户端向服务端发送的字段中包含标志位SYN=1，序列号seq=x。第一次握手前客户端的状态为CLOSE，第一次握手后客户端的状态为SYN-SENT。此时服务端的状态为LISTEN。 第二次握手：服务端在收到客户端发来的报文后，会随机生成一个服务端的起始序列号y，然后给客户端回复一段报文，其中包括标志位SYN=1，ACK=1，序列号seq=y，确认号ack=x+1。第二次握手前服务端的状态为LISTEN，第二次握手后服务端的状态为SYN-RCVD，此时客户端的状态为SYN-SENT。（其中SYN=1表示要和客户端建立一个连接，ACK=1表示确认序号有效） 第三次握手：客户端收到服务端发来的报文后，会再向服务端发送报文，其中包含标志位ACK=1，序列号seq=x+1，确认号ack=y+1。第三次握手前客户端的状态为SYN-SENT，第三次握手后客户端和服务端的状态都为ESTABLISHED。此时连接建立完成。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:2:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"两次握手可以吗？ 第三次握手主要为了防止已失效的连接请求报文段突然又传输到了服务端，导致产生问题。 比如客户端A发出连接请求，可能因为网络阻塞原因，A没有收到确认报文，于是A再重传一次连接请求。 连接成功，等待数据传输完毕后，就释放了连接。 然后A发出的第一个连接请求等到连接释放以后的某个时间才到达服务端B，此时B误认为A又发出一次新的连接请求，于是就向A发出确认报文段。 如果不采用三次握手，只要B发出确认，就建立新的连接了，此时A不会响应B的确认且不发送数据，则B一直等待A发送数据，浪费资源。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:3:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"四次挥手 A的应用进程先向其TCP发出连接释放报文段（FIN=1，seq=u），并停止再发送数据，主动关闭TCP连接，进入FIN-WAIT-1（终止等待1）状态，等待B的确认。 B收到连接释放报文段后即发出确认报文段（ACK=1，ack=u+1，seq=v），B进入CLOSE-WAIT（关闭等待）状态，此时的TCP处于半关闭状态，A到B的连接释放。 A收到B的确认后，进入FIN-WAIT-2（终止等待2）状态，等待B发出的连接释放报文段。 B发送完数据，就会发出连接释放报文段（FIN=1，ACK=1，seq=w，ack=u+1），B进入LAST-ACK（最后确认）状态，等待A的确认。 A收到B的连接释放报文段后，对此发出确认报文段（ACK=1，seq=u+1，ack=w+1），A进入TIME-WAIT（时间等待）状态。此时TCP未释放掉，需要经过时间等待计时器设置的时间2MSL（最大报文段生存时间）后，A才进入CLOSED状态。B收到A发出的确认报文段后关闭连接，若没收到A发出的确认报文段，B就会重传连接释放报文段。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:4:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"第四次挥手为什么要等待2MSL？ 保证A发送的最后一个ACK报文段能够到达B。这个ACK报文段有可能丢失，B收不到这个确认报文，就会超时重传连接释放报文段，然后A可以在2MSL时间内收到这个重传的连接释放报文段，接着A重传一次确认，重新启动2MSL计时器，最后A和B都进入到CLOSED状态，若A在TIME-WAIT状态不等待一段时间，而是发送完ACK报文段后立即释放连接，则无法收到B重传的连接释放报文段，所以不会再发送一次确认报文段，B就无法正常进入到CLOSED状态。 防止已失效的连接请求报文段出现在本连接中。A在发送完最后一个ACK报文段后，再经过2MSL，就可以使这个连接所产生的所有报文段都从网络中消失，使下一个新的连接中不会出现旧的连接请求报文段。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:5:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"为什么是四次挥手？ 因为当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。但是在关闭连接时，当Server端收到Client端发出的连接释放报文时，很可能并不会立即关闭SOCKET，所以Server端先回复一个ACK报文，告诉Client端我收到你的连接释放报文了。只有等到Server端所有的报文都发送完了，这时Server端才能发送连接释放报文，之后两边才会真正的断开连接。故需要四次挥手。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:6:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"TCP有哪些特点？ TCP是面向连接的运输层协议。 ，每一条TCP连接只能有两个端点。 TCP提供可靠交付的服务。 TCP提供全双工通信。 面向字节流。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:7:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"TCP和UDP的区别？ TCP面向连接；UDP是无连接的，即发送数据之前不需要建立连接。 TCP提供可靠的服务；UDP不保证可靠交付。 TCP面向字节流，把数据看成一连串无结构的字节流；UDP是面向报文的。 TCP有拥塞控制；UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如实时视频会议等）。 每一条TCP连接只能是的；UDP支持一对一、一对多、多对一和多对多的通信方式。 TCP首部开销20字节；UDP的首部开销小，只有8个字节。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:8:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"HTTP协议的特点？ HTTP允许传输任意类型的数据。传输的类型由Content-Type加以标记。 无状态。对于客户端每次发送的请求，服务器都认为是一个新的请求，上一次会话和下一次会话之间没有联系。 支持客户端/服务器模式。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:9:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"HTTP报文格式 HTTP请求由请求行、请求头部、空行和请求体四个部分组成。 请求行：包括请求方法，访问的资源URL，使用的HTTP版本。GET和POST是最常见的HTTP方法，除此以外还包括DELETE、HEAD、OPTIONS、PUT、TRACE。 请求头：格式为“属性名:属性值”，服务端根据请求头获取客户端的信息，主要有cookie、host、connection、accept-language、accept-encoding、user-agent。 请求体：用户的请求数据如用户名，密码等。 请求报文示例： POST /xxx HTTP/1.1 请求行 Accept:image/gif.image/jpeg, 请求头部 Accept-Language:zh-cn Connection:Keep-Alive Host:localhost User-Agent:Mozila/4.0(compatible;MSIE5.01;Window NT5.0) Accept-Encoding:gzip,deflate username=dabin 请求体 HTTP响应也由四个部分组成，分别是：状态行、响应头、空行和响应体。 状态行：协议版本，状态码及状态描述。 响应头：响应头字段主要有connection、content-type、content-encoding、content-length、set-cookie、Last-Modified，、Cache-Control、Expires。 响应体：服务器返回给客户端的内容。 响应报文示例： HTTP/1.1 200 OK Server:Apache Tomcat/5.0.12 Date:Mon,6Oct2003 13:23:42 GMT Content-Length:112 响应体 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:10:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"HTTP状态码有哪些？ ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:11:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"POST和GET的区别？ GET请求参数通过URL传递，POST的参数放在请求体中。 GET产生一个TCP数据包；POST产生两个TCP数据包。对于GET方式的请求，浏览器会把请求头和请求体一并发送出去；而对于POST，浏览器先发送请求头，服务器响应100 continue，浏览器再发送请求体。 GET请求会被浏览器主动缓存，而POST不会，除非手动设置。 GET请求只能进行url编码，而POST支持多种编码方式。 GET请求参数会被完整保留在浏览器历史记录里，而POST中的参数不会被保留。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:12:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"HTTP长连接和短连接？ HTTP1.0默认使用的是短连接。浏览器和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中断连接。 HTTP/1.1起，默认使用长连接。要使用长连接，客户端和服务器的HTTP首部的Connection都要设置为keep-alive，才能支持长连接。 HTTP长连接，指的是复用TCP连接。多个HTTP请求可以复用同一个TCP连接，这就节省了TCP连接建立和断开的消耗。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:13:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"HTTP1.1和 HTTP2.0的区别？ HTTP2.0相比HTTP1.1支持的特性： 新的二进制格式：HTTP1.1 基于文本格式传输数据；HTTP2.0采用二进制格式传输数据，解析更高效。 多路复用：在一个连接里，允许同时发送多个请求或响应，并且这些请求或响应能够并行的传输而不被阻塞，避免 HTTP1.1 出现的”队头堵塞”问题。 头部压缩，HTTP1.1的header带有大量信息，而且每次都要重复发送；HTTP2.0 把header从数据中分离，并封装成头帧和数据帧，使用特定算法压缩头帧，有效减少头信息大小。并且HTTP2.0在客户端和服务器端记录了之前发送的键值对，对于相同的数据，不会重复发送。比如请求a发送了所有的头信息字段，请求b则只需要发送差异数据，这样可以减少冗余数据，降低开销。 服务端推送：HTTP2.0允许服务器向客户端推送资源，无需客户端发送请求到服务器获取。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:14:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"HTTPS与HTTP的区别？ HTTP是超文本传输协议，信息是明文传输；HTTPS则是具有安全性的ssl加密传输协议。 HTTP和HTTPS用的端口不一样，HTTP端口是80，HTTPS是443。 HTTPS协议需要到CA机构申请证书，一般需要一定的费用。 HTTP运行在TCP协议之上；HTTPS运行在SSL协议之上，SSL运行在TCP协议之上。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:15:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"什么是数字证书？ 服务端可以向证书颁发机构CA申请证书，以避免中间人攻击（防止证书被篡改）。证书包含三部分内容：证书内容、证书签名算法和签名，签名是为了验证身份。 服务端把证书传输给浏览器，浏览器从证书里取公钥。证书可以证明该公钥对应本网站。 数字签名的制作过程： CA使用证书签名算法对证书内容进行hash运算。 对hash后的值用CA的私钥加密，得到数字签名。 浏览器验证过程： 获取证书，得到证书内容、证书签名算法和数字签名。 用CA机构的公钥对数字签名解密（由于是浏览器信任的机构，所以浏览器会保存它的公钥）。 用证书里的签名算法对证书内容进行hash运算。 比较解密后的数字签名和对证书内容做hash运算后得到的哈希值，相等则表明证书可信。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:16:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"HTTPS原理 首先是TCP三次握手，然后客户端发起一个HTTPS连接建立请求，客户端先发一个Client Hello的包，然后服务端响应Server Hello，接着再给客户端发送它的证书，然后双方经过密钥交换，最后使用交换的密钥加解密数据。 协商加密算法 。在Client Hello里面客户端会告知服务端自己当前的一些信息，包括客户端要使用的TLS版本，支持的加密算法，要访问的域名，给服务端生成的一个随机数（Nonce）等。需要提前告知服务器想要访问的域名以便服务器发送相应的域名的证书过来。 服务端响应Server Hello，告诉客户端服务端选中的加密算法。 接着服务端给客户端发来了2个证书。第二个证书是第一个证书的签发机构（CA）的证书。 客户端使用证书的认证机构CA公开发布的RSA公钥对该证书进行验证，下图表明证书认证成功。 验证通过之后，浏览器和服务器通过密钥交换算法产生共享的对称密钥。 开始传输数据，使用同一个对称密钥来加解密。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:17:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"DNS 的解析过程？ 浏览器搜索自己的DNS缓存 若没有，则搜索操作系统中的DNS缓存和hosts文件 若没有，则操作系统将域名发送至本地域名服务器，本地域名服务器查询自己的DNS缓存，查找成功则返回结果，否则依次向根域名服务器、顶级域名服务器、权限域名服务器发起查询请求，最终返回IP地址给本地域名服务器 本地域名服务器将得到的IP地址返回给操作系统，同时自己也将IP地址缓存起来 操作系统将 IP 地址返回给浏览器，同时自己也将IP地址缓存起来 浏览器得到域名对应的IP地址 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:18:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"浏览器中输入URL返回页面过程？ 解析域名，找到主机 IP。 浏览器利用 IP 直接与网站主机通信，三次握手，建立 TCP 连接。浏览器会以一个随机端口向服务端的 web 程序 80 端口发起 TCP 的连接。 建立 TCP 连接后，浏览器向主机发起一个HTTP请求。 服务器响应请求，返回响应数据。 浏览器解析响应内容，进行渲染，呈现给用户。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:19:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"什么是cookie和session？ 由于HTTP协议是无状态的协议，需要用某种机制来识具体的用户身份，用来跟踪用户的整个会话。常用的会话跟踪技术是cookie与session。 cookie就是由服务器发给客户端的特殊信息，而这些信息以文本文件的方式存放在客户端，然后客户端每次向服务器发送请求的时候都会带上这些特殊的信息。说得更具体一些：当用户使用浏览器访问一个支持cookie的网站的时候，用户会提供包括用户名在内的个人信息并且提交至服务器；接着，服务器在向客户端回传相应的超文本的同时也会发回这些个人信息，当然这些信息并不是存放在HTTP响应体中的，而是存放于HTTP响应头；当客户端浏览器接收到来自服务器的响应之后，浏览器会将这些信息存放在一个统一的位置。 自此，客户端再向服务器发送请求的时候，都会把相应的cookie存放在HTTP请求头再次发回至服务器。服务器在接收到来自客户端浏览器的请求之后，就能够通过分析存放于请求头的cookie得到客户端特有的信息，从而动态生成与该客户端相对应的内容。网站的登录界面中“请记住我”这样的选项，就是通过cookie实现的。 cookie工作流程： servlet创建cookie，保存少量数据，发送给浏览器。 浏览器获得服务器发送的cookie数据，将自动的保存到浏览器端。 下次访问时，浏览器将自动携带cookie数据发送给服务器。 session原理：首先浏览器请求服务器访问web站点时，服务器首先会检查这个客户端请求是否已经包含了一个session标识（sessionid），如果已经包含了一个sessionid则说明以前已经为此客户端创建过session，服务器就按照sessionid把这个session检索出来使用，如果客户端请求不包含session id，则服务器为此客户端创建一个session，并且生成一个与此session相关联的独一无二的sessionid存放到cookie中，这个sessionid将在本次响应中返回到客户端保存，这样在交互的过程中，浏览器端每次请求时，都会带着这个sessionid，服务器根据这个sessionid就可以找得到对应的session。以此来达到共享数据的目的。 这里需要注意的是，session不会随着浏览器的关闭而死亡，而是等待超时时间。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:20:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"Cookie和Session的区别？ 作用范围不同，Cookie 保存在客户端，Session 保存在服务器端。 有效期不同，Cookie 可设置为长时间保持，比如我们经常使用的默认登录功能，Session 一般失效时间较短，客户端关闭或者 Session 超时都会失效。 隐私策略不同，Cookie 存储在客户端，容易被窃取；Session 存储在服务端，安全性相对 Cookie 要好一些。 存储大小不同， 单个 Cookie 保存的数据不能超过 4K；对于 Session 来说存储没有上限，但出于对服务器的性能考虑，Session 内不要存放过多的数据，并且需要设置 Session 删除机制。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:21:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"什么是对称加密和非对称加密？ 对称加密：通信双方使用相同的密钥进行加密。特点是加密速度快，但是缺点是密钥泄露会导致密文数据被破解。常见的对称加密有AES和DES算法。 非对称加密：它需要生成两个密钥，公钥和私钥。公钥是公开的，任何人都可以获得，而私钥是私人保管的。公钥负责加密，私钥负责解密；或者私钥负责加密，公钥负责解密。这种加密算法安全性更高，但是计算量相比对称加密大很多，加密和解密都很慢。常见的非对称算法有RSA和DSA。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:22:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"滑动窗口机制 TCP 利用滑动窗口实现流量控制。流量控制是为了控制发送方发送速率，保证接收方来得及接收。 TCP会话的双方都各自维护一个发送窗口和一个接收窗口。接收窗口大小取决于应用、系统、硬件的限制。发送窗口则取决于对端通告的接收窗口。接收方发送的确认报文中的window字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将接收方的确认报文window字段设置为 0，则发送方不能发送数据。 TCP头包含window字段，16bit位，它代表的是窗口的字节容量，最大为65535。这个字段是接收端告诉发送端自己还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来。接收窗口的大小是约等于发送窗口的大小。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:23:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"详细讲一下拥塞控制？ 防止过多的数据注入到网络中。 几种拥塞控制方法：慢开始( slow-start )、拥塞避免( congestion avoidance )、快重传( fast retransmit )和快恢复( fast recovery )。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:24:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"慢开始 把拥塞窗口 cwnd 设置为一个最大报文段MSS的数值。而在每收到一个对新的报文段的确认后，把拥塞窗口增加至多一个MSS的数值。每经过一个传输轮次，拥塞窗口 cwnd 就加倍。 为了防止拥塞窗口cwnd增长过大引起网络拥塞，还需要设置一个慢开始门限ssthresh状态变量。 当 cwnd \u003c ssthresh 时，使用慢开始算法。 当 cwnd \u003e ssthresh 时，停止使用慢开始算法而改用拥塞避免算法。 当 cwnd = ssthresh 时，既可使用慢开始算法，也可使用拥塞控制避免算法。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:24:1","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"拥塞避免 让拥塞窗口cwnd缓慢地增大，每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍。这样拥塞窗口cwnd按线性规律缓慢增长。 无论在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞（其根据就是没有收到确认），就要把慢开始门限ssthresh设置为出现拥塞时的发送 方窗口值的一半（但不能小于2）。然后把拥塞窗口cwnd重新设置为1，执行慢开始算法。这样做的目的就是要迅速减少主机发送到网络中的分组数，使得发生 拥塞的路由器有足够时间把队列中积压的分组处理完毕。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:24:2","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"快重传 有时个别报文段会在网络中丢失，但实际上网络并未发生拥塞。如果发送方迟迟收不到确认，就会产生超时，就会误认为网络发生了拥塞。这就导致发送方错误地启动慢开始，把拥塞窗口cwnd又设置为1，因而降低了传输效率。 快重传算法可以避免这个问题。快重传算法首先要求接收方每收到一个失序的报文段后就立即发出重复确认，使发送方及早知道有报文段没有到达对方。 发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待重传计时器到期。由于发送方尽早重传未被确认的报文段，因此采用快重传后可以使整个网络吞吐量提高约20%。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:24:3","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"快恢复 当发送方连续收到三个重复确认，就会把慢开始门限ssthresh减半，接着把cwnd值设置为慢开始门限ssthresh减半后的数值，然后开始执行拥塞避免算法，使拥塞窗口缓慢地线性增大。 在采用快恢复算法时，慢开始算法只是在TCP连接建立时和网络出现超时时才使用。 采用这样的拥塞控制方法使得TCP的性能有明显的改进。 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:24:4","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"ARP协议 ARP解决了同一个局域网上的主机和路由器IP和MAC地址的解析。 每台主机都会在自己的ARP缓冲区中建立一个ARP列表，以表示IP地址和MAC地址的对应关系。 当源主机需要将一个数据包要发送到目的主机时，会首先检查自己 ARP列表中是否存在该 IP地址对应的MAC地址，如果有，就直接将数据包发送到这个MAC地址；如果没有，就向本地网段发起一个ARP请求的广播包，查询此目的主机对应的MAC地址。此ARP请求数据包里包括源主机的IP地址、硬件地址、以及目的主机的IP地址。 网络中所有的主机收到这个ARP请求后，会检查数据包中的目的IP是否和自己的IP地址一致。如果不相同就忽略此数据包；如果相同，该主机首先将发送端的MAC地址和IP地址添加到自己的ARP列表中，如果ARP表中已经存在该IP的信息，则将其覆盖，然后给源主机发送一个 ARP响应数据包，告诉对方自己是它需要查找的MAC地址。 源主机收到这个ARP响应数据包后，将得到的目的主机的IP地址和MAC地址添加到自己的ARP列表中，并利用此信息开始数据的传输。 如果源主机一直没有收到ARP响应数据包，表示ARP查询失败。 ref https://www.nowcoder.com/discuss/870064 ","date":"2022-03-25","objectID":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/:25:0","tags":["计网","八股"],"title":"计算机网络面试题","uri":"/posts/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"简介 腾讯云函数是一种serverless解决方案，使用云函数你就不需要关注运维，扩容，日志也是自动收集，非常的方便 ","date":"2022-03-24","objectID":"/posts/cloud/%E8%85%BE%E4%BA%91%E4%BA%91%E5%87%BD%E6%95%B0%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%9D%91/:1:0","tags":["cloud","白嫖"],"title":"腾云云函数部署及坑","uri":"/posts/cloud/%E8%85%BE%E4%BA%91%E4%BA%91%E5%87%BD%E6%95%B0%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%9D%91/"},{"categories":null,"content":"触发方式 定时触发 通过编写corntab进行触发，比如签到，你可以早上8点开始到下午5点，每隔5分钟就看看有没有签到 web方式触发 类似于http服务器，不同的是正常http服务器是一直后台运行，但是这个是只有请求来了才运行，请求完成后就自动销毁，当然，也不一定销毁，比如当你请求量很大的时候，他会一直保持后台运行 ","date":"2022-03-24","objectID":"/posts/cloud/%E8%85%BE%E4%BA%91%E4%BA%91%E5%87%BD%E6%95%B0%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%9D%91/:1:1","tags":["cloud","白嫖"],"title":"腾云云函数部署及坑","uri":"/posts/cloud/%E8%85%BE%E4%BA%91%E4%BA%91%E5%87%BD%E6%95%B0%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%9D%91/"},{"categories":null,"content":"使用语言 我这里使用的是go语言，要先编译成linux x64的可执行文件后，在压缩上传 ","date":"2022-03-24","objectID":"/posts/cloud/%E8%85%BE%E4%BA%91%E4%BA%91%E5%87%BD%E6%95%B0%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%9D%91/:2:0","tags":["cloud","白嫖"],"title":"腾云云函数部署及坑","uri":"/posts/cloud/%E8%85%BE%E4%BA%91%E4%BA%91%E5%87%BD%E6%95%B0%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%9D%91/"},{"categories":null,"content":"scf_bootstrap 所有云函数都最好在根目录下面有一个 scf_bootstrap 文件，这里面的内容主要是如何启动这个应用如 #!/bin/bash ./httpserver 值得注意的是这里有个坑，这个scf_bootstrap必须要使用 lf 换行符，别的都不行，如果使用别的，执行的时候就会报错如，非常的shit { \"errorCode\": -1, \"errorMessage\": \"Failed to initialize the container. Please confirm that the container can be started locally.\", \"statusCode\": 405 } 一定使用LF换行符 一定使用LF换行符 一定使用LF换行符 ","date":"2022-03-24","objectID":"/posts/cloud/%E8%85%BE%E4%BA%91%E4%BA%91%E5%87%BD%E6%95%B0%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%9D%91/:3:0","tags":["cloud","白嫖"],"title":"腾云云函数部署及坑","uri":"/posts/cloud/%E8%85%BE%E4%BA%91%E4%BA%91%E5%87%BD%E6%95%B0%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%9D%91/"},{"categories":null,"content":"部署 部署也是一言难尽，使用正常的压缩软件如7z进行压缩（windows） 部署，就会出现这个 /var/user/scf_bootstrap: line 3: ./httpserver: Permission denied 为啥呢，因为你使用的是windows进行压缩，和在linux下压缩的是不一样的 windows linux 可以看到windows的属性是A，猜测在解压以后就没有访问权限了 所以 必须使用linux进行压缩 必须使用linux进行压缩 必须使用linux进行压缩 ","date":"2022-03-24","objectID":"/posts/cloud/%E8%85%BE%E4%BA%91%E4%BA%91%E5%87%BD%E6%95%B0%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%9D%91/:4:0","tags":["cloud","白嫖"],"title":"腾云云函数部署及坑","uri":"/posts/cloud/%E8%85%BE%E4%BA%91%E4%BA%91%E5%87%BD%E6%95%B0%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%9D%91/"},{"categories":null,"content":"结语 虽然腾讯云函数有各种各样的坑，免费额度也少，但最后部署成功还是挺开心的，也希望大家都能早日部署成功 ","date":"2022-03-24","objectID":"/posts/cloud/%E8%85%BE%E4%BA%91%E4%BA%91%E5%87%BD%E6%95%B0%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%9D%91/:5:0","tags":["cloud","白嫖"],"title":"腾云云函数部署及坑","uri":"/posts/cloud/%E8%85%BE%E4%BA%91%E4%BA%91%E5%87%BD%E6%95%B0%E9%83%A8%E7%BD%B2%E5%8F%8A%E5%9D%91/"},{"categories":null,"content":"1. SQL的生命周期？ 应用服务器与数据库服务器建立一个连接 数据库进程拿到请求sql 解析并生成执行计划，执行 读取数据到内存并进行逻辑处理 通过步骤一的连接，发送结果到客户端 关掉连接，释放资源 ","date":"2022-03-23","objectID":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/:1:0","tags":null,"title":"Mysql优化面试","uri":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"2. 大表数据查询，怎么优化 优化shema、sql语句+索引； 第二加缓存，memcached, redis； 主从复制，读写分离； 垂直拆分，根据你模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统； 水平切分，针对数据量大的表，这一步最麻烦，最能考验技术水平，要选择一个合理的sharding key, 为了有好的查询效率，表结构也要改动，做一定的冗余，应用也要改，sql中尽量带sharding key，将数据定位到限定的表上去查，而不是扫描全部的表； ","date":"2022-03-23","objectID":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/:2:0","tags":null,"title":"Mysql优化面试","uri":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"3. 超大分页怎么处理？ 超大的分页一般从两个方向上来解决. 数据库层面,这也是我们主要集中关注的(虽然收效没那么大),类似于select * from table where age \u003e 20 limit 1000000,10这种查询其实也是有可以优化的余地的. 这条语句需要load1000000数据然后基本上全部丢弃,只取10条当然比较慢. 当时我们可以修改为select * from table where id in (select id from table where age \u003e 20 limit 1000000,10).这样虽然也load了一百万的数据,但是由于索引覆盖,要查询的所有字段都在索引中,所以速度会很快. 同时如果ID连续的好,我们还可以select * from table where id \u003e 1000000 limit 4,效率也是不错的,优化的可能性有许多种,但是核心思想都一样,就是减少load的数据. 从需求的角度减少这种请求…主要是不做类似的需求(直接跳转到几百万页之后的具体某一页.只允许逐页查看或者按照给定的路线走,这样可预测,可缓存)以及防止ID泄漏且连续被人恶意攻击. 解决超大分页,其实主要是靠缓存,可预测性的提前查到内容,缓存至redis等k-V数据库中,直接返回即可. 在阿里巴巴《Java开发手册》中,对超大分页的解决办法是类似于上面提到的第一种. 【推荐】利用延迟关联或者子查询优化超多分页场景。 说明：MySQL并不是跳过offset行，而是取offset+N行，然后返回放弃前offset行，返回N行，那当offset特别大的时候，效率就非常的低下，要么控制返回的总页数，要么对超过特定阈值的页数进行SQL改写。 正例：先快速定位需要获取的id段，然后再关联： SELECT a.* FROM 表1 a, (select id from 表1 where 条件 LIMIT 100000,20 ) b where a.id=b.id ","date":"2022-03-23","objectID":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/:3:0","tags":null,"title":"Mysql优化面试","uri":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"5. mysql 分页 LIMIT 子句可以被用于强制 SELECT 语句返回指定的记录数。LIMIT 接受一个或两个数字参数。参数必须是一个整数常量。如果给定两个参数，第一个参数指定第一个返回记录行的偏移量，第二个参数指定返回记录行的最大数目。初始记录行的偏移量是 0(而不是 1) mysql\u003e SELECT * FROM table LIMIT 5,10; // 检索记录行 6-15 为了检索从某一个偏移量到记录集的结束所有的记录行，可以指定第二个参数为 -1： mysql\u003e SELECT * FROM table LIMIT 95,-1; // 检索记录行 96-last. 如果只给定一个参数，它表示返回最大的记录行数目： mysql\u003e SELECT * FROM table LIMIT 5; //检索前 5 个记录行 换句话说，LIMIT n 等价于 LIMIT 0,n。 ","date":"2022-03-23","objectID":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/:4:0","tags":null,"title":"Mysql优化面试","uri":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"6. 慢查询日志 用于记录执行时间超过某个临界值的SQL日志，用于快速定位慢查询，为我们的优化做参考。 开启慢查询日志 配置项：slow_query_log 可以使用show variables like ‘slov_query_log’查看是否开启，如果状态值为OFF，可以使用set GLOBAL slow_query_log = on来开启，它会在datadir下产生一个xxx-slow.log的文件。 设置临界时间 配置项：long_query_time 查看：show VARIABLES like ‘long_query_time’，单位秒 设置：set long_query_time=0.5 实操时应该从长时间设置到短的时间，即将最慢的SQL优化掉 查看日志，一旦SQL超过了我们设置的临界时间就会被记录到xxx-slow.log中 ","date":"2022-03-23","objectID":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/:5:0","tags":null,"title":"Mysql优化面试","uri":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"7. 关心过业务系统里面的sql耗时吗？统计过慢查询吗？对慢查询都怎么优化过？ 在业务系统中，除了使用主键进行的查询，其他的我都会在测试库上测试其耗时，慢查询的统计主要由运维在做，会定期将业务中的慢查询反馈给我们。 慢查询的优化首先要搞明白慢的原因是什么？是查询条件没有命中索引？是load了不需要的数据列？还是数据量太大？ 所以优化也是针对这三个方向来的， 首先分析语句，看看是否load了额外的数据，可能是查询了多余的行并且抛弃掉了，可能是加载了许多结果中并不需要的列，对语句进行分析以及重写。 分析语句的执行计划，然后获得其使用索引的情况，之后修改语句或者修改索引，使得语句可以尽可能的命中索引。 如果对语句的优化已经无法进行，可以考虑表中的数据量是否太大，如果是的话可以进行横向或者纵向的分表。 ","date":"2022-03-23","objectID":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/:6:0","tags":null,"title":"Mysql优化面试","uri":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"8. 为什么要尽量设定一个主键？ 主键是数据库确保数据行在整张表唯一性的保障，即使业务上本张表没有主键，也建议添加一个自增长的ID列作为主键。设定了主键之后，在后续的删改查的时候可能更加快速以及确保操作数据范围安全。 ","date":"2022-03-23","objectID":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/:7:0","tags":null,"title":"Mysql优化面试","uri":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"9. 主键使用自增ID还是UUID？ 推荐使用自增ID，不要使用UUID。 因为在InnoDB存储引擎中，主键索引是作为聚簇索引存在的，也就是说，主键索引的B+树叶子节点上存储了主键索引以及全部的数据(按照顺序)，如果主键索引是自增ID，那么只需要不断向后排列即可，如果是UUID，由于到来的ID与原来的大小不确定，会造成非常多的数据插入，数据移动，然后导致产生很多的内存碎片，进而造成插入性能的下降。 总之，在数据量大一些的情况下，用自增主键性能会好一些。 关于主键是聚簇索引，如果没有主键，InnoDB会选择一个唯一键来作为聚簇索引，如果没有唯一键，会生成一个隐式的主键。 ","date":"2022-03-23","objectID":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/:8:0","tags":null,"title":"Mysql优化面试","uri":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"10. 字段为什么要求定义为not null？ null值会占用更多的字节，且会在程序中造成很多与预期不符的情况。 ","date":"2022-03-23","objectID":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/:9:0","tags":null,"title":"Mysql优化面试","uri":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"11. 如果要存储用户的密码散列，应该使用什么字段进行存储？ 密码散列，盐，用户身份证号等固定长度的字符串应该使用char而不是varchar来存储，这样可以节省空间且提高检索效率。 ","date":"2022-03-23","objectID":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/:10:0","tags":null,"title":"Mysql优化面试","uri":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"12. 优化查询过程中的数据访问 访问数据太多导致查询性能下降 确定应用程序是否在检索大量超过需要的数据，可能是太多行或列 确认MySQL服务器是否在分析大量不必要的数据行 避免犯如下SQL语句错误 查询不需要的数据。解决办法：使用limit解决 多表关联返回全部列。解决办法：指定列名 总是返回全部列。解决办法：避免使用SELECT * 重复查询相同的数据。解决办法：可以缓存数据，下次直接读取缓存 是否在扫描额外的记录。解决办法： 使用explain进行分析，如果发现查询需要扫描大量的数据，但只返回少数的行，可以通过如下技巧去优化： 使用索引覆盖扫描，把所有的列都放到索引中，这样存储引擎不需要回表获取对应行就可以返回结果。 改变数据库和表的结构，修改数据表范式 重写SQL语句，让优化器可以以更优的方式执行查询。 ","date":"2022-03-23","objectID":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/:11:0","tags":null,"title":"Mysql优化面试","uri":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"13. 优化长难的查询语句 一个复杂查询还是多个简单查询 MySQL内部每秒能扫描内存中上百万行数据，相比之下，响应数据给客户端就要慢得多 使用尽可能小的查询是好的，但是有时将一个大的查询分解为多个小的查询是很有必要的。 切分查询 将一个大的查询分为多个小的相同的查询 一次性删除1000万的数据要比一次删除1万，暂停一会的方案更加损耗服务器开销。 分解关联查询，让缓存的效率更高。 执行单个查询可以减少锁的竞争。 在应用层做关联更容易对数据库进行拆分。 查询效率会有大幅提升。 较少冗余记录的查询。 ","date":"2022-03-23","objectID":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/:12:0","tags":null,"title":"Mysql优化面试","uri":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"14. 优化特定类型的查询语句 count(*)会忽略所有的列，直接统计所有列数，不要使用count(列名) MyISAM中，没有任何where条件的count(*)非常快。 当有where条件时，MyISAM的count统计不一定比其它引擎快。 可以使用explain查询近似值，用近似值替代count(*) 增加汇总表 使用缓存 ","date":"2022-03-23","objectID":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/:13:0","tags":null,"title":"Mysql优化面试","uri":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"15. 优化关联查询 确定ON或者USING子句中是否有索引。 确保GROUP BY和ORDER BY只有一个表中的列，这样MySQL才有可能使用索引。 16. 优化子查询 用关联查询替代 优化GROUP BY和DISTINCT 这两种查询据可以使用索引来优化，是最有效的优化方法 关联查询中，使用标识列分组的效率更高 如果不需要ORDER BY，进行GROUP BY时加ORDER BY NULL，MySQL不会再进行文件排序。 WITH ROLLUP超级聚合，可以挪到应用程序处理 ","date":"2022-03-23","objectID":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/:14:0","tags":null,"title":"Mysql优化面试","uri":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"17. 优化LIMIT分页 LIMIT偏移量大的时候，查询效率较低 可以记录上次查询的最大ID，下次查询时直接根据该ID来查询 ","date":"2022-03-23","objectID":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/:15:0","tags":null,"title":"Mysql优化面试","uri":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"18. 优化UNION查询 UNION ALL的效率高于UNION ","date":"2022-03-23","objectID":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/:16:0","tags":null,"title":"Mysql优化面试","uri":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"19. 优化WHERE子句 解题方法 对于此类考题，先说明如何定位低效SQL语句，然后根据SQL语句可能低效的原因做排查，先从索引着手，如果索引没有问题，考虑以上几个方面，数据访问的问题，长难查询句的问题还是一些特定类型优化的问题，逐一回答。 SQL语句优化的一些方法？ 1.对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 2.应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如： select id from t where num is null -- 可以在num上设置默认值0，确保表中num列没有null值，然后这样查询： select id from t where num= 3.应尽量避免在 where 子句中使用!=或\u003c\u003e操作符，否则引擎将放弃使用索引而进行全表扫描。 4.应尽量避免在 where 子句中使用or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如： select id from t where num=10 or num=20 -- 可以这样查询： select id from t where num=10 union all select id from t where num=20 5.in 和 not in 也要慎用，否则会导致全表扫描，如： select id from t where num in(1,2,3) -- 对于连续的数值，能用 between 就不要用 in 了： select id from t where num between 1 and 3 6.下面的查询也将导致全表扫描：select id from t where name like ‘%李%’若要提高效率，可以考虑全文检索。 7.如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然 而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描： select id from t where num=@num -- 可以改为强制查询使用索引： select id from t with(index(索引名)) where num=@num 8.应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如： select id from t where num/2=100 -- 应改为: select id from t where num=100*2 9.应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如： select id from t where substring(name,1,3)=’abc’ -- name以abc开头的id应改为: select id from t where name like ‘abc%’ 10.不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。 ","date":"2022-03-23","objectID":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/:17:0","tags":null,"title":"Mysql优化面试","uri":"/posts/draft/mysql%E4%BC%98%E5%8C%96%E9%9D%A2%E8%AF%95/"},{"categories":null,"content":"事务的四大特性？* 事务特性ACID：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）。 原子性是指事务包含的所有操作要么全部成功，要么全部失败回滚。 一致性是指一个事务执行之前和执行之后都必须处于一致性状态。比如a与b账户共有1000块，两人之间转账之后无论成功还是失败，它们的账户总和还是1000。 隔离性。跟隔离级别相关，如read committed，一个事务只能读到已经提交的修改。 持久性是指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:1:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"数据库的三大范式 第一范式1NF 确保数据库表字段的原子性。 比如字段 userInfo: 广东省 10086' ，依照第一范式必须拆分成 userInfo: 广东省 userTel:10086两个字段。 第二范式2NF 首先要满足第一范式，另外包含两部分内容，一是表必须有一个主键；二是非主键列必须完全依赖于主键，而不能只依赖于主键的一部分。 举个例子。假定选课关系表为student_course(student_no, student_name, age, course_name, grade, credit)，主键为(student_no, course_name)。其中学分完全依赖于课程名称，姓名年龄完全依赖学号，不符合第二范式，会导致数据冗余（学生选n门课，姓名年龄有n条记录）、插入异常（插入一门新课，因为没有学号，无法保存新课记录）等问题。 可以拆分成三个表：学生：student(stuent_no, student_name, 年龄)；课程：course(course_name, credit)；选课关系：student_course_relation(student_no, course_name, grade)。 第三范式3NF 首先要满足第二范式，另外非主键列必须直接依赖于主键，不能存在传递依赖。即不能存在：非主键列 A 依赖于非主键列 B，非主键列 B 依赖于主键的情况。 假定学生关系表为Student(student_no, student_name, age, academy_id, academy_telephone)，主键为\"学号\"，其中学院id依赖于学号，而学院地点和学院电话依赖于学院id，存在传递依赖，不符合第三范式。 可以把学生关系表分为如下两个表：学生：(student_no, student_name, age, academy_id)；学院：(academy_id, academy_telephone)。 2NF和3NF的区别？ 2NF依据是非主键列是否完全依赖于主键，还是依赖于主键的一部分。 3NF依据是非主键列是直接依赖于主键，还是直接依赖于非主键。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:2:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"事务隔离级别有哪些？* 先了解下几个概念：脏读、不可重复读、幻读。 脏读是指在一个事务处理过程里读取了另一个未提交的事务中的数据。 不可重复读是指在对于数据库中的某行记录，一个事务范围内多次查询却返回了不同的数据值，这是由于在查询间隔，另一个事务修改了数据并提交了。 幻读是当某个事务在读取某个范围内的记录时，另外一个事务又在该范围内插入了新的记录，当之前的事务再次读取该范围的记录时，会产生幻行，就像产生幻觉一样，这就是发生了幻读。 不可重复读和脏读的区别是，脏读是某一事务读取了另一个事务未提交的脏数据，而不可重复读则是读取了前一事务提交的数据。 幻读和不可重复读都是读取了另一条已经提交的事务，不同的是不可重复读的重点是修改，幻读的重点在于新增或者删除。 事务隔离就是为了解决上面提到的脏读、不可重复读、幻读这几个问题。 MySQL数据库为我们提供的四种隔离级别： Serializable (串行化)：通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。 Repeatable read (可重复读)：MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行，解决了不可重复读的问题。 Read committed (读已提交)：一个事务只能看见已经提交事务所做的改变。可避免脏读的发生。 Read uncommitted (读未提交)：所有事务都可以看到其他未提交事务的执行结果。 查看隔离级别： select @@transaction_isolation; 设置隔离级别： set session transaction isolation level read uncommitted; ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:3:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"索引 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:4:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"什么是索引？ 索引是存储引擎用于提高数据库表的访问速度的一种数据结构。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:4:1","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"索引的优缺点？ 优点： 加快数据查找的速度 为用来排序或者是分组的字段添加索引，可以加快分组和排序的速度 加快表与表之间的连接 缺点： 建立索引需要占用物理空间 会降低表的增删改的效率，因为每次对表记录进行增删改，需要进行动态维护索引，导致增删改时间变长 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:4:2","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"索引的作用？ 数据是存储在磁盘上的，查询数据时，如果没有索引，会加载所有的数据到内存，依次进行检索，读取磁盘次数较多。有了索引，就不需要加载所有数据，因为B+树的高度一般在2-4层，最多只需要读取2-4次磁盘，查询速度大大提升。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:4:3","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"什么情况下需要建索引？ 经常用于查询的字段 经常用于连接的字段建立索引，可以加快连接的速度 经常需要排序的字段建立索引，因为索引已经排好序，可以加快排序查询速度 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:4:4","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"什么情况下不建索引？ where条件中用不到的字段不适合建立索引 表记录较少 需要经常增删改 参与列计算的列不适合建索引 区分度不高的字段不适合建立索引，如性别等 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:4:5","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"索引的数据结构 索引的数据结构主要有B+树和哈希表，对应的索引分别为B+树索引和哈希索引。InnoDB引擎的索引类型有B+树索引和哈希索引，默认的索引类型为B+树索引。 B+树索引 B+ 树是基于B 树和叶子节点顺序访问指针进行实现，它具有B树的平衡性，并且通过顺序访问指针来提高区间查询的性能。 在 B+ 树中，节点中的 key 从左到右递增排列，如果某个指针的左右相邻 key 分别是 keyi 和 keyi+1，则该指针指向节点的所有 key 大于等于 keyi 且小于等于 keyi+1。 进行查找操作时，首先在根节点进行二分查找，找到key所在的指针，然后递归地在指针所指向的节点进行查找。直到查找到叶子节点，然后在叶子节点上进行二分查找，找出key所对应的数据项。 MySQL 数据库使用最多的索引类型是BTREE索引，底层基于B+树数据结构来实现。 mysql\u003e show index from blog\\G; *************************** 1. row *************************** Table: blog Non_unique: 0 Key_name: PRIMARY Seq_in_index: 1 Column_name: blog_id Collation: A Cardinality: 4 Sub_part: NULL Packed: NULL Null: Index_type: BTREE Comment: Index_comment: Visible: YES Expression: NULL 哈希索引 哈希索引是基于哈希表实现的，对于每一行数据，存储引擎会对索引列进行哈希计算得到哈希码，并且哈希算法要尽量保证不同的列值计算出的哈希码值是不同的，将哈希码的值作为哈希表的key值，将指向数据行的指针作为哈希表的value值。这样查找一个数据的时间复杂度就是O(1)，一般多用于精确查找。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:4:6","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"Hash索引和B+树索引的区别？ 哈希索引不支持排序，因为哈希表是无序的。 哈希索引不支持范围查找。 哈希索引不支持模糊查询及多列索引的最左前缀匹配。 因为哈希表中会存在哈希冲突，所以哈希索引的性能是不稳定的，而B+树索引的性能是相对稳定的，每次查询都是从根节点到叶子节点。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:4:7","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"为什么B+树比B树更适合实现数据库索引？ 由于B+树的数据都存储在叶子结点中，叶子结点均为索引，方便扫库，只需要扫一遍叶子结点即可，但是B树因为其分支结点同样存储着数据，我们要找到具体的数据，需要进行一次中序遍历按序来扫，所以B+树更加适合在区间查询的情况，而在数据库中基于范围的查询是非常频繁的，所以通常B+树用于数据库索引。 B+树的节点只存储索引key值，具体信息的地址存在于叶子节点的地址中。这就使以页为单位的索引中可以存放更多的节点。减少更多的I/O支出。 B+树的查询效率更加稳定，任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:4:8","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"索引有什么分类？ 1、主键索引：名为primary的唯一非空索引，不允许有空值。 2、唯一索引：索引列中的值必须是唯一的，但是允许为空值。唯一索引和主键索引的区别是：唯一约束的列可以为null且可以存在多个null值。唯一索引的用途：唯一标识数据库表中的每条记录，主要是用来防止数据重复插入。创建唯一索引的SQL语句如下： ALTER TABLE table_name ADD CONSTRAINT constraint_name UNIQUE KEY(column_1,column_2,…); 3、组合索引：在表中的多个字段组合上创建的索引，只有在查询条件中使用了这些字段的左边字段时，索引才会被使用，使用组合索引时需遵循最左前缀原则。 4、全文索引：只有在MyISAM引擎上才能使用，只能在CHAR、VARCHAR和TEXT类型字段上使用全文索引。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:4:9","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"什么是最左匹配原则？* 如果 SQL 语句中用到了组合索引中的最左边的索引，那么这条 SQL 语句就可以利用这个组合索引去进行匹配。当遇到范围查询(\u003e、\u003c、between、like)就会停止匹配，后面的字段不会用到索引。 对(a,b,c)建立索引，查询条件使用 a/ab/abc 会走索引，使用 bc 不会走索引。 对(a,b,c,d)建立索引，查询条件为a = 1 and b = 2 and c \u003e 3 and d = 4，那么a、b和c三个字段能用到索引，而d无法使用索引。因为遇到了范围查询。 如下图，对(a, b) 建立索引，a 在索引树中是全局有序的，而 b 是全局无序，局部有序（当a相等时，会根据b进行排序）。直接执行b = 2这种查询条件无法使用索引。 当a的值确定的时候，b是有序的。例如a = 1时，b值为1，2是有序的状态。当a = 2时候，b的值为1，4也是有序状态。 当执行a = 1 and b = 2时a和b字段能用到索引。而执行a \u003e 1 and b = 2时，a字段能用到索引，b字段用不到索引。因为a的值此时是一个范围，不是固定的，在这个范围内b值不是有序的，因此b字段无法使用索引。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:4:10","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"什么是聚集索引？ InnoDB使用表的主键构造主键索引树，同时叶子节点中存放的即为整张表的记录数据。聚集索引叶子节点的存储是逻辑上连续的，使用双向链表连接，叶子节点按照主键的顺序排序，因此对于主键的排序查找和范围查找速度比较快。 聚集索引的叶子节点就是整张表的行记录。InnoDB 主键使用的是聚簇索引。聚集索引要比非聚集索引查询效率高很多。 对于InnoDB来说，聚集索引一般是表中的主键索引，如果表中没有显示指定主键，则会选择表中的第一个不允许为NULL的唯一索引。如果没有主键也没有合适的唯一索引，那么InnoDB内部会生成一个隐藏的主键作为聚集索引，这个隐藏的主键长度为6个字节，它的值会随着数据的插入自增。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:4:11","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"什么是覆盖索引？ select的数据列只用从索引中就能够取得，不需要回表进行二次查询，也就是说查询列要被所使用的索引覆盖。对于innodb表的二级索引，如果索引能覆盖到查询的列，那么就可以避免对主键索引的二次查询。 不是所有类型的索引都可以成为覆盖索引。覆盖索引要存储索引列的值，而哈希索引、全文索引不存储索引列的值，所以MySQL使用b+树索引做覆盖索引。 对于使用了覆盖索引的查询，在查询前面使用explain，输出的extra列会显示为using index。 比如user_like 用户点赞表，组合索引为(user_id, blog_id)，user_id和blog_id都不为null。 explain select blog_id from user_like where user_id = 13; explain结果的Extra列为Using index，查询的列被索引覆盖，并且where筛选条件符合最左前缀原则，通过索引查找就能直接找到符合条件的数据，不需要回表查询数据。 explain select user_id from user_like where blog_id = 1; explain结果的Extra列为Using where; Using index， 查询的列被索引覆盖，where筛选条件不符合最左前缀原则，无法通过索引查找找到符合条件的数据，但可以通过索引扫描找到符合条件的数据，也不需要回表查询数据。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:4:12","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"索引的设计原则？ 索引列的区分度越高，索引的效果越好。比如使用性别这种区分度很低的列作为索引，效果就会很差。 尽量使用短索引，对于较长的字符串进行索引时应该指定一个较短的前缀长度，因为较小的索引涉及到的磁盘I/O较少，查询速度更快。 索引不是越多越好，每个索引都需要额外的物理空间，维护也需要花费时间。 利用最左前缀原则。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:4:13","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"索引什么时候会失效？* 导致索引失效的情况： 对于组合索引，不是使用组合索引最左边的字段，则不会使用索引 以%开头的like查询如%abc，无法使用索引；非%开头的like查询如abc%，相当于范围查询，会使用索引 查询条件中列类型是字符串，没有使用引号，可能会因为类型不同发生隐式转换，使索引失效 判断索引列是否不等于某个值时 对索引列进行运算 查询条件使用or连接，也会导致索引失效 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:4:14","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"什么是前缀索引？ 有时需要在很长的字符列上创建索引，这会造成索引特别大且慢。使用前缀索引可以避免这个问题。 前缀索引是指对文本或者字符串的前几个字符建立索引，这样索引的长度更短，查询速度更快。 创建前缀索引的关键在于选择足够长的前缀以保证较高的索引选择性。索引选择性越高查询效率就越高，因为选择性高的索引可以让MySQL在查找时过滤掉更多的数据行。 建立前缀索引的方式： // email列创建前缀索引 ALTER TABLE table_name ADD KEY(column_name(prefix_length)); ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:4:15","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"常见的存储引擎有哪些？ MySQL中常用的四种存储引擎分别是： MyISAM、InnoDB、MEMORY、ARCHIVE。MySQL 5.5版本后默认的存储引擎为InnoDB。 InnoDB存储引擎 InnoDB是MySQL默认的事务型存储引擎，使用最广泛，基于聚簇索引建立的。InnoDB内部做了很多优化，如能够自动在内存中创建自适应hash索引，以加速读操作。 优点：支持事务和崩溃修复能力；引入了行级锁和外键约束。 缺点：占用的数据空间相对较大。 适用场景：需要事务支持，并且有较高的并发读写频率。 MyISAM存储引擎 数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，可以使用MyISAM引擎。MyISAM会将表存储在两个文件中，数据文件.MYD和索引文件.MYI。 优点：访问速度快。 缺点：MyISAM不支持事务和行级锁，不支持崩溃后的安全恢复，也不支持外键。 适用场景：对事务完整性没有要求；表的数据都会只读的。 MEMORY存储引擎 MEMORY引擎将数据全部放在内存中，访问速度较快，但是一旦系统奔溃的话，数据都会丢失。 MEMORY引擎默认使用哈希索引，将键的哈希值和指向数据行的指针保存在哈希索引中。 优点：访问速度较快。 缺点： 哈希索引数据不是按照索引值顺序存储，无法用于排序。 不支持部分索引匹配查找，因为哈希索引是使用索引列的全部内容来计算哈希值的。 只支持等值比较，不支持范围查询。 当出现哈希冲突时，存储引擎需要遍历链表中所有的行指针，逐行进行比较，直到找到符合条件的行。 ARCHIVE存储引擎 ARCHIVE存储引擎非常适合存储大量独立的、作为历史记录的数据。ARCHIVE提供了压缩功能，拥有高效的插入速度，但是这种引擎不支持索引，所以查询性能较差。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:5:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"MyISAM和InnoDB的区别？* 是否支持行级锁 : MyISAM 只有表级锁，而InnoDB 支持行级锁和表级锁，默认为行级锁。 是否支持事务和崩溃后的安全恢复： MyISAM 不提供事务支持。而InnoDB提供事务支持，具有事务、回滚和崩溃修复能力。 是否支持外键： MyISAM不支持，而InnoDB支持。 是否支持MVCC ：MyISAM不支持，InnoDB支持。应对高并发事务，MVCC比单纯的加锁更高效。 MyISAM不支持聚集索引，InnoDB支持聚集索引。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:6:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"MVCC 实现原理？* MVCC(Multiversion concurrency control) 就是同一份数据保留多版本的一种方式，进而实现并发控制。在查询的时候，通过read view和版本链找到对应版本的数据。 作用：提升并发性能。对于高并发场景，MVCC比行级锁开销更小。 MVCC 实现原理如下： MVCC 的实现依赖于版本链，版本链是通过表的三个隐藏字段实现。 DB_TRX_ID：当前事务id，通过事务id的大小判断事务的时间顺序。 DB_ROLL_PRT：回滚指针，指向当前行记录的上一个版本，通过这个指针将数据的多个版本连接在一起构成undo log版本链。 DB_ROLL_ID：主键，如果数据表没有主键，InnoDB会自动生成主键。 每条表记录大概是这样的： 使用事务更新行记录的时候，就会生成版本链，执行过程如下： 用排他锁锁住该行； 将该行原本的值拷贝到undo log，作为旧版本用于回滚； 修改当前行的值，生成一个新版本，更新事务id，使回滚指针指向旧版本的记录，这样就形成一条版本链。 下面举个例子方便大家理解。 1、初始数据如下，其中DB_ROW_ID和DB_ROLL_PTR为空。 2、事务A对该行数据做了修改，将age修改为12，效果如下： 3、之后事务B也对该行记录做了修改，将age修改为8，效果如下： 4、此时undo log有两行记录，并且通过回滚指针连在一起。 接下来了解下read view的概念。 read view可以理解成将数据在每个时刻的状态拍成“照片”记录下来。在获取某时刻t的数据时，到t时间点拍的“照片”上取数据。 在read view内部维护一个活跃事务链表，表示生成read view的时候还在活跃的事务。这个链表包含在创建read view之前还未提交的事务，不包含创建read view之后提交的事务。 不同隔离级别创建read view的时机不同。 read committed：每次执行select都会创建新的read_view，保证能读取到其他事务已经提交的修改。 repeatable read：在一个事务范围内，第一次select时更新这个read_view，以后不会再更新，后续所有的select都是复用之前的read_view。这样可以保证事务范围内每次读取的内容都一样，即可重复读。 read view的记录筛选方式 前提：DATA_TRX_ID 表示每个数据行的最新的事务ID；up_limit_id表示当前快照中的最先开始的事务；low_limit_id表示当前快照中的最慢开始的事务，即最后一个事务。 如果DATA_TRX_ID \u003c up_limit_id：说明在创建read view时，修改该数据行的事务已提交，该版本的记录可被当前事务读取到。 如果DATA_TRX_ID \u003e= low_limit_id：说明当前版本的记录的事务是在创建read view之后生成的，该版本的数据行不可以被当前事务访问。此时需要通过版本链找到上一个版本，然后重新判断该版本的记录对当前事务的可见性。 如果up_limit_id \u003c= DATA_TRX_ID \u003c low_limit_i： 需要在活跃事务链表中查找是否存在ID为DATA_TRX_ID的值的事务。 如果存在，因为在活跃事务链表中的事务是未提交的，所以该记录是不可见的。此时需要通过版本链找到上一个版本，然后重新判断该版本的可见性。 如果不存在，说明事务trx_id 已经提交了，这行记录是可见的。 总结：InnoDB 的MVCC是通过 read view 和版本链实现的，版本链保存有历史版本记录，通过read view 判断当前版本的数据是否可见，如果不可见，再从版本链中找到上一个版本，继续进行判断，直到找到一个可见的版本。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:7:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"快照读和当前读 表记录有两种读取方式。 快照读：读取的是快照版本。普通的SELECT就是快照读。通过mvcc来进行并发控制的，不用加锁。 当前读：读取的是最新版本。UPDATE、DELETE、INSERT、SELECT … LOCK IN SHARE MODE、SELECT … FOR UPDATE是当前读。 快照读情况下，InnoDB通过mvcc机制避免了幻读现象。而mvcc机制无法避免当前读情况下出现的幻读现象。因为当前读每次读取的都是最新数据，这时如果两次查询中间有其它事务插入数据，就会产生幻读。 下面举个例子说明下： 1、首先，user表只有两条记录，具体如下： 2、事务a和事务b同时开启事务start transaction； 3、事务a插入数据然后提交； insert into user(user_name, user_password, user_mail, user_state) values(‘tyson’, ‘a’, ‘a’, 0); 4、事务b执行全表的update； update user set user_name = ‘a’; 5、事务b然后执行查询，查到了事务a中插入的数据。（下图左边是事务b，右边是事务a。事务开始之前只有两条记录，事务a插入一条数据之后，事务b查询出来是三条数据） 以上就是当前读出现的幻读现象。 那么MySQL是如何避免幻读？ 在快照读情况下，MySQL通过mvcc来避免幻读。 在当前读情况下，MySQL通过next-key来避免幻读（加行锁和间隙锁来实现的）。 next-key包括两部分：行锁和间隙锁。行锁是加在索引上的锁，间隙锁是加在索引之间的。 Serializable隔离级别也可以避免幻读，会锁住整张表，并发性极低，一般不会使用。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:8:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"共享锁和排他锁 SELECT 的读取锁定主要分为两种方式：共享锁和排他锁。 select * from table where id\u003c6 lock in share mode;–共享锁 select * from table where id\u003c6 for update;–排他锁 这两种方式主要的不同在于LOCK IN SHARE MODE多个事务同时更新同一个表单时很容易造成死锁。 申请排他锁的前提是，没有线程对该结果集的任何行数据使用排它锁或者共享锁，否则申请会受到阻塞。在进行事务操作时，MySQL会对查询结果集的每行数据添加排它锁，其他线程对这些数据的更改或删除操作会被阻塞（只能读操作），直到该语句的事务被commit语句或rollback语句结束为止。 SELECT... FOR UPDATE 使用注意事项： for update 仅适用于innodb，且必须在事务范围内才能生效。 根据主键进行查询，查询条件为like或者不等于，主键字段产生表锁。 根据非索引字段进行查询，会产生表锁。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:9:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"大表怎么优化？ 某个表有近千万数据，查询比较慢，如何优化？ 当MySQL单表记录数过大时，数据库的性能会明显下降，一些常见的优化措施如下： 限定数据的范围。比如：用户在查询历史信息的时候，可以控制在一个月的时间范围内； 读写分离： 经典的数据库拆分方案，主库负责写，从库负责读； 通过分库分表的方式进行优化，主要有垂直拆分和水平拆分。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:10:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"bin log/redo log/undo log MySQL日志主要包括查询日志、慢查询日志、事务日志、错误日志、二进制日志等。其中比较重要的是 bin log（二进制日志）和 redo log（重做日志）和 undo log（回滚日志）。 bin log bin log是MySQL数据库级别的文件，记录对MySQL数据库执行修改的所有操作，不会记录select和show语句，主要用于恢复数据库和同步数据库。 redo log redo log是innodb引擎级别，用来记录innodb存储引擎的事务日志，不管事务是否提交都会记录下来，用于数据恢复。当数据库发生故障，innoDB存储引擎会使用redo log恢复到发生故障前的时刻，以此来保证数据的完整性。将参数innodb_flush_log_at_tx_commit设置为1，那么在执行commit时会将redo log同步写到磁盘。 undo log 除了记录redo log外，当进行数据修改时还会记录undo log，undo log用于数据的撤回操作，它保留了记录修改前的内容。通过undo log可以实现事务回滚，并且可以根据undo log回溯到某个特定的版本的数据，实现MVCC。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:11:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"bin log和redo log有什么区别？ bin log会记录所有日志记录，包括InnoDB、MyISAM等存储引擎的日志；redo log只记录innoDB自身的事务日志。 bin log只在事务提交前写入到磁盘，一个事务只写一次；而在事务进行过程，会有redo log不断写入磁盘。 bin log是逻辑日志，记录的是SQL语句的原始逻辑；redo log是物理日志，记录的是在某个数据页上做了什么修改。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:12:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"讲一下MySQL架构？ MySQL主要分为 Server 层和存储引擎层： Server 层：主要包括连接器、查询缓存、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块 binglog 日志模块。 存储引擎： 主要负责数据的存储和读取。server 层通过api与存储引擎进行通信。 Server 层基本组件 连接器： 当客户端连接 MySQL 时，server层会对其进行身份认证和权限校验。 查询缓存: 执行查询语句的时候，会先查询缓存，先校验这个 sql 是否执行过，如果有缓存这个 sql，就会直接返回给客户端，如果没有命中，就会执行后续的操作。 分析器: 没有命中缓存的话，SQL 语句就会经过分析器，主要分为两步，词法分析和语法分析，先看 SQL 语句要做什么，再检查 SQL 语句语法是否正确。 优化器： 优化器对查询进行优化，包括重写查询、决定表的读写顺序以及选择合适的索引等，生成执行计划。 执行器： 首先执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会根据执行计划去调用引擎的接口，返回结果。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:13:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"分库分表 当单表的数据量达到1000W或100G以后，优化索引、添加从库等可能对数据库性能提升效果不明显，此时就要考虑对其进行切分了。切分的目的就在于减少数据库的负担，缩短查询的时间。 数据切分可以分为两种方式：垂直划分和水平划分。 垂直划分 垂直划分数据库是根据业务进行划分，例如购物场景，可以将库中涉及商品、订单、用户的表分别划分出成一个库，通过降低单库的大小来提高性能。同样的，分表的情况就是将一个大表根据业务功能拆分成一个个子表，例如商品基本信息和商品描述，商品基本信息一般会展示在商品列表，商品描述在商品详情页，可以将商品基本信息和商品描述拆分成两张表。 优点：行记录变小，数据页可以存放更多记录，在查询时减少I/O次数。 缺点： 主键出现冗余，需要管理冗余列； 会引起表连接JOIN操作，可以通过在业务服务器上进行join来减少数据库压力； 依然存在单表数据量过大的问题。 水平划分 水平划分是根据一定规则，例如时间或id序列值等进行数据的拆分。比如根据年份来拆分不同的数据库。每个数据库结构一致，但是数据得以拆分，从而提升性能。 优点：单库（表）的数据量得以减少，提高性能；切分出的表结构相同，程序改动较少。 缺点： 分片事务一致性难以解决 跨节点join性能差，逻辑复杂 数据分片在扩容时需要迁移 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:14:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"什么是分区表？ 分区表是一个独立的逻辑表，但是底层由多个物理子表组成。 当查询条件的数据分布在某一个分区的时候，查询引擎只会去某一个分区查询，而不是遍历整个表。在管理层面，如果需要删除某一个分区的数据，只需要删除对应的分区即可。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:15:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"分区表类型 按照范围分区。 CREATETABLEtest_range_partition(idINTauto_increment,createdateDATETIME,primarykey(id,createdate))PARTITIONBYRANGE(TO_DAYS(createdate))(PARTITIONp201801VALUESLESSTHAN(TO_DAYS('20180201')),PARTITIONp201802VALUESLESSTHAN(TO_DAYS('20180301')),PARTITIONp201803VALUESLESSTHAN(TO_DAYS('20180401')),PARTITIONp201804VALUESLESSTHAN(TO_DAYS('20180501')),PARTITIONp201805VALUESLESSTHAN(TO_DAYS('20180601')),PARTITIONp201806VALUESLESSTHAN(TO_DAYS('20180701')),PARTITIONp201807VALUESLESSTHAN(TO_DAYS('20180801')),PARTITIONp201808VALUESLESSTHAN(TO_DAYS('20180901')),PARTITIONp201809VALUESLESSTHAN(TO_DAYS('20181001')),PARTITIONp201810VALUESLESSTHAN(TO_DAYS('20181101')),PARTITIONp201811VALUESLESSTHAN(TO_DAYS('20181201')),PARTITIONp201812VALUESLESSTHAN(TO_DAYS('20190101')));在/var/lib/mysql/data/可以找到对应的数据文件，每个分区表都有一个使用#分隔命名的表文件： -rw-r----- 1 MySQL MySQL 65 Mar 14 21:47 db.opt -rw-r----- 1 MySQL MySQL 8598 Mar 14 21:50 test\\_range\\_partition.frm -rw-r----- 1 MySQL MySQL 98304 Mar 14 21:50 test\\_range\\_partition#P#p201801.ibd -rw-r----- 1 MySQL MySQL 98304 Mar 14 21:50 test\\_range\\_partition#P#p201802.ibd -rw-r----- 1 MySQL MySQL 98304 Mar 14 21:50 test\\_range\\_partition#P#p201803.ibd list分区 对于List分区，分区字段必须是已知的，如果插入的字段不在分区时枚举值中，将无法插入。 createtabletest_list_partiotion(idintauto_increment,data_typetinyint,primarykey(id,data_type))partitionbylist(data_type)(partitionp0valuesin(0,1,2,3,4,5,6),partitionp1valuesin(7,8,9,10,11,12),partitionp2valuesin(13,14,15,16,17));hash分区 可以将数据均匀地分布到预先定义的分区中。 createtabletest_hash_partiotion(idintauto_increment,create_datedatetime,primarykey(id,create_date))partitionbyhash(year(create_date))partitions10;","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:16:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"分区的问题？ 打开和锁住所有底层表的成本可能很高。当查询访问分区表时，MySQL 需要打开并锁住所有的底层表，这个操作在分区过滤之前发生，所以无法通过分区过滤来降低此开销，会影响到查询速度。可以通过批量操作来降低此类开销，比如批量插入、LOAD DATA INFILE和一次删除多行数据。 维护分区的成本可能很高。例如重组分区，会先创建一个临时分区，然后将数据复制到其中，最后再删除原分区。 所有分区必须使用相同的存储引擎。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:17:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"查询语句执行流程？ 查询语句的执行流程如下：权限校验、查询缓存、分析器、优化器、权限校验、执行器、引擎。 举个例子，查询语句如下： select * from user where id \u003e 1 and name = ‘大彬’; 首先检查权限，没有权限则返回错误； MySQL8.0以前会查询缓存，缓存命中则直接返回，没有则执行下一步； 词法分析和语法分析。提取表名、查询条件，检查语法是否有错误； 两种执行方案，先查 id \u003e 1 还是 name = '大彬'，优化器根据自己的优化算法选择执行效率最好的方案； 校验权限，有权限就调用数据库引擎接口，返回引擎的执行结果。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:18:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"更新语句执行过程？ 更新语句执行流程如下：分析器、权限校验、执行器、引擎、redo log（prepare状态）、binlog、redo log（commit状态） 举个例子，更新语句如下： update user set name = ‘大彬’ where id = 1; 先查询到 id 为1的记录，有缓存会使用缓存。 拿到查询结果，将 name 更新为大彬，然后调用引擎接口，写入更新数据，innodb 引擎将数据保存在内存中，同时记录redo log，此时redo log进入 prepare状态。 执行器收到通知后记录binlog，然后调用引擎接口，提交redo log为commit状态。 更新完成。 为什么记录完redo log，不直接提交，而是先进入prepare状态？ 假设先写redo log直接提交，然后写binlog，写完redo log后，机器挂了，binlog日志没有被写入，那么机器重启后，这台机器会通过redo log恢复数据，但是这个时候binlog并没有记录该数据，后续进行机器备份的时候，就会丢失这一条数据，同时主从同步也会丢失这一条数据。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:19:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"exist和in的区别？ exists用于对外表记录做筛选。exists会遍历外表，将外查询表的每一行，代入内查询进行判断。当exists里的条件语句能够返回记录行时，条件就为真，返回外表当前记录。反之如果exists里的条件语句不能返回记录行，条件为假，则外表当前记录被丢弃。 select a.\\* from A awhere exists (select 1 from B b where a.id=b.id) in是先把后边的语句查出来放到临时表中，然后遍历临时表，将临时表的每一行，代入外查询去查找。 select * from Awhere id in (select id from B) 子查询的表比较大的时候，使用exists可以有效减少总的循环次数来提升速度；当外查询的表比较大的时候，使用in可以有效减少对外查询表循环遍历来提升速度。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:20:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"MySQL中int(10)和char(10)的区别？　 int(10)中的10表示的是显示数据的长度，而char(10)表示的是存储数据的长度。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:21:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"truncate、delete与drop区别？ 相同点： truncate和不带where子句的delete、以及drop都会删除表内的数据。 drop、truncate都是DDL语句（数据定义语言），执行后会自动提交。 不同点： truncate 和 delete 只删除数据不删除表的结构；drop 语句将删除表的结构被依赖的约束、触发器、索引； 一般来说，执行速度: drop \u003e truncate \u003e delete。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:22:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"having和where区别？ 二者作用的对象不同，where子句作用于表和视图，having作用于组。 where在数据分组前进行过滤，having在数据分组后进行过滤。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:23:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"什么是MySQL主从同步？ 主从同步使得数据可以从一个数据库服务器复制到其他服务器上，在复制数据时，一个服务器充当主服务器（master），其余的服务器充当从服务器（slave）。 因为复制是异步进行的，所以从服务器不需要一直连接着主服务器，从服务器甚至可以通过拨号断断续续地连接主服务器。通过配置文件，可以指定复制所有的数据库，某个数据库，甚至是某个数据库上的某个表。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:24:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"为什么要做主从同步？ 读写分离，使数据库能支撑更大的并发。 在主服务器上生成实时数据，而在从服务器上分析这些数据，从而提高主服务器的性能。 数据备份，保证数据的安全。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:25:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"乐观锁和悲观锁是什么？ 数据库中的并发控制是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。乐观锁和悲观锁是并发控制主要采用的技术手段。 悲观锁：假定会发生并发冲突，在查询完数据的时候就把事务锁起来，直到提交事务。实现方式：使用数据库中的锁机制。 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否数据是否被修改过。给表增加version字段，在修改提交之前检查version与原来取到的version值是否相等，若相等，表示数据没有被修改，可以更新，否则，数据为脏数据，不能更新。实现方式：乐观锁一般使用版本号机制或CAS算法实现。 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:26:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"用过processlist吗？ show processlist 或 show full processlist 可以查看当前 MySQL 是否有压力，正在运行的SQL，有没有慢SQL正在执行。返回参数如下： id：线程ID，可以用kill id杀死某个线程 db：数据库名称 user：数据库用户 host：数据库实例的IP command：当前执行的命令，比如Sleep，Query，Connect等 time：消耗时间，单位秒 state：执行状态，主要有以下状态： Sleep，线程正在等待客户端发送新的请求 Locked，线程正在等待锁 Sending data，正在处理SELECT查询的记录，同时把结果发送给客户端 Kill，正在执行kill语句，杀死指定线程 Connect，一个从节点连上了主节点 Quit，线程正在退出 Sorting for group，正在为GROUP BY做排序 Sorting for order，正在为ORDER BY做排序 info：正在执行的SQL语句 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:27:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"存储过程 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:28:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"DML、DDL、DCL区别 一、DML DML（data manipulation language）数据操纵语言： 　就是我们最经常用到的 SELECT、UPDATE、INSERT、DELETE。 主要用来对数据库的数据进行一些操作。 SELECT列名称FROM表名称UPDATE表名称SET列名称=新值WHERE列名称=某值INSERTINTOtable_name(列1,列2,...)VALUES(值1,值2,....)DELETEFROM表名称WHERE列名称=值二、DDL DDL（data definition language）数据库定义语言： 　其实就是我们在创建表的时候用到的一些sql，比如说：CREATE、ALTER、DROP等。DDL主要是用在定义或改变表的结构，数据类型，表之间的链接和约束等初始化工作上 CREATETABLE表名称(列名称1数据类型,列名称2数据类型,列名称3数据类型,....)ALTERTABLEtable_nameALTERCOLUMNcolumn_namedatatypeDROPTABLE表名称DROPDATABASE数据库名称三、DCL DCL（Data Control Language）数据库控制语言： 　是用来设置或更改数据库用户或角色权限的语句，包括（grant,deny,revoke等）语句。这个比较少用到。 ref https://www.nowcoder.com/discuss/837435 ","date":"2022-03-23","objectID":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/:29:0","tags":["数据库","八股"],"title":"Mysql面试题","uri":"/posts/middleware/mysql/mysql%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":null,"content":"系统调用 在我们运行的用户程序中，凡是与系统级别的资源有关的操作（例如文件管理、进程控制、内存管理等）都必须通过系统调用方式向OS提出服务请求，并由OS代为完成 平常我门的进程几乎都是用户态，读取用户数据，当涉及到系统操作，计算机资源的时候就要用到系统调用了 系统调用的功能与其作用一样——涉及计算机资源的操作 设备管理：完成设备的请求/释放以及设备的启动 文件管理：完成文件的读写、删除、创建等功能 进程控制：完成进程的创建、撤销、阻塞以及唤醒等功能 内存管理：完成内存的分配、回收以及获取作业占用内存区大小和地址等功能 ","date":"2022-03-18","objectID":"/posts/linux/%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8-%E7%94%A8%E6%88%B7%E6%80%81%E5%92%8C%E5%86%85%E6%A0%B8%E6%80%81/:1:0","tags":["操作系统","八股"],"title":"系统调用 用户态和内核态","uri":"/posts/linux/%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8-%E7%94%A8%E6%88%B7%E6%80%81%E5%92%8C%E5%86%85%E6%A0%B8%E6%80%81/"},{"categories":null,"content":"进程在系统上的运行分为2个级别 用户态（user mode）：用户态运行的进程可以直接读取用户程序的数据 内核态（kernel mode）：系统态运行的程序可以访问计算机的任何资源，不受限制 平常我们运行的程序都是用户态的，如果想要将进程运行在系统态则需要利用系统调用 ","date":"2022-03-18","objectID":"/posts/linux/%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8-%E7%94%A8%E6%88%B7%E6%80%81%E5%92%8C%E5%86%85%E6%A0%B8%E6%80%81/:1:1","tags":["操作系统","八股"],"title":"系统调用 用户态和内核态","uri":"/posts/linux/%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8-%E7%94%A8%E6%88%B7%E6%80%81%E5%92%8C%E5%86%85%E6%A0%B8%E6%80%81/"},{"categories":null,"content":"Bloom Filter 概念 布隆过滤器（英语：Bloom Filter）是1970年由一个叫布隆的小伙子提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。 面试关联：一般都会在回答缓存穿透，或者海量数据去重这个时候引出来，加分项哟 Bloom Filter 原理 布隆过滤器的原理是，当一个元素被加入集合时，通过K个散列函数将这个元素映射成一个位数组中的K个点，把它们置为1。检索时，我们只要看看这些点是不是都是1就（大约）知道集合中有没有它了：如果这些点有任何一个0，则被检元素一定不在；如果都是1，则被检元素很可能在。这就是布隆过滤器的基本思想。 Bloom Filter跟单哈希函数Bit-Map不同之处在于：Bloom Filter使用了k个哈希函数，每个字符串跟k个bit对应。从而降低了冲突的概率。 简单的说一下就是我们先把我们数据库的数据都加载到我们的过滤器中，比如数据库的id现在有：1、2、3 那就用id：1 为例子他在上图中经过三次hash之后，把三次原本值0的地方改为1 下次我进来查询如果id也是1 那我就把1拿去三次hash 发现跟上面的三个位置完全一样，那就能证明过滤器中有1的 反之如果不一样就说明不存在了 那应用的场景在哪里呢？一般我们都会用来防止缓存击穿（如果不知道缓存击穿是啥的小伙伴不要着急，我已经帮你准备好了，传送门 ↓ ） 简单来说就是你数据库的id都是1开始然后自增的，那我知道你接口是通过id查询的，我就拿负数去查询，这个时候，会发现缓存里面没这个数据，我又去数据库查也没有，一个请求这样，100个，1000个，10000个呢？你的DB基本上就扛不住了，如果在缓存里面加上这个，是不是就不存在了，你判断没这个数据就不去查了，直接return一个数据为空不就好了嘛。 这玩意这么好使那有啥缺点么？有的，我们接着往下看 Bloom Filter的缺点 bloom filter之所以能做到在时间和空间上的效率比较高，是因为牺牲了判断的准确率、删除的便利性 存在误判，可能要查到的元素并没有在容器中，但是hash之后得到的k个位置上值都是1。如果bloom filter中存储的是黑名单，那么可以通过建立一个白名单来存储可能会误判的元素。 删除困难。一个放入容器的元素映射到bit数组的k个位置上是1，删除的时候不能简单的直接置为0，可能会影响其他元素的判断。可以采用Counting Bloom Filter Bloom Filter 实现 布隆过滤器有许多实现与优化，Guava中就提供了一种Bloom Filter的实现。 在使用bloom filter时，绕不过的两点是预估数据量n以及期望的误判率fpp， 在实现bloom filter时，绕不过的两点就是hash函数的选取以及bit数组的大小。 对于一个确定的场景，我们预估要存的数据量为n，期望的误判率为fpp，然后需要计算我们需要的Bit数组的大小m，以及hash函数的个数k，并选择hash函数 ","date":"2022-03-18","objectID":"/posts/middleware/redis/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8bloomfilter/:0:0","tags":["redis"],"title":"布隆过滤器(BloomFilter)","uri":"/posts/middleware/redis/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8bloomfilter/"},{"categories":null,"content":"(1)Bit数组大小选择 　根据预估数据量n以及误判率fpp，bit数组大小的m的计算方式：","date":"2022-03-18","objectID":"/posts/middleware/redis/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8bloomfilter/:1:0","tags":["redis"],"title":"布隆过滤器(BloomFilter)","uri":"/posts/middleware/redis/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8bloomfilter/"},{"categories":null,"content":"(2)哈希函数选择 ​ 由预估数据量n以及bit数组长度m，可以得到一个hash函数的个数k：​ 哈希函数的选择对性能的影响应该是很大的，一个好的哈希函数要能近似等概率的将字符串映射到各个Bit。选择k个不同的哈希函数比较麻烦，一种简单的方法是选择一个哈希函数，然后送入k个不同的参数。 哈希函数个数k、位数组大小m、加入的字符串数量n的关系可以参考Bloom Filters - the math，Bloom_filter-wikipedia 要使用BloomFilter，需要引入guava包： \u003cdependency\u003e \u003cgroupId\u003ecom.google.guava\u003c/groupId\u003e \u003cartifactId\u003eguava\u003c/artifactId\u003e \u003cversion\u003e23.0\u003c/version\u003e \u003c/dependency\u003e 测试分两步： 1、往过滤器中放一百万个数，然后去验证这一百万个数是否能通过过滤器 2、另外找一万个数，去检验漏网之鱼的数量 /** * 测试布隆过滤器(可用于redis缓存穿透) * * @author 敖丙 */ public class TestBloomFilter { private static int total = 1000000; private static BloomFilter\u003cInteger\u003e bf = BloomFilter.create(Funnels.integerFunnel(), total); // private static BloomFilter\u003cInteger\u003e bf = BloomFilter.create(Funnels.integerFunnel(), total, 0.001); public static void main(String[] args) { // 初始化1000000条数据到过滤器中 for (int i = 0; i \u003c total; i++) { bf.put(i); } // 匹配已在过滤器中的值，是否有匹配不上的 for (int i = 0; i \u003c total; i++) { if (!bf.mightContain(i)) { System.out.println(\"有坏人逃脱了~~~\"); } } // 匹配不在过滤器中的10000个值，有多少匹配出来 int count = 0; for (int i = total; i \u003c total + 10000; i++) { if (bf.mightContain(i)) { count++; } } System.out.println(\"误伤的数量：\" + count); } } 运行结果： 运行结果表示，遍历这一百万个在过滤器中的数时，都被识别出来了。一万个不在过滤器中的数，误伤了320个，错误率是0.03左右。 看下BloomFilter的源码： public static \u003cT\u003e BloomFilter\u003cT\u003e create(Funnel\u003c? super T\u003e funnel, int expectedInsertions) { return create(funnel, (long) expectedInsertions); } public static \u003cT\u003e BloomFilter\u003cT\u003e create(Funnel\u003c? super T\u003e funnel, long expectedInsertions) { return create(funnel, expectedInsertions, 0.03); // FYI, for 3%, we always get 5 hash functions } public static \u003cT\u003e BloomFilter\u003cT\u003e create( Funnel\u003c? super T\u003e funnel, long expectedInsertions, double fpp) { return create(funnel, expectedInsertions, fpp, BloomFilterStrategies.MURMUR128_MITZ_64); } static \u003cT\u003e BloomFilter\u003cT\u003e create( Funnel\u003c? super T\u003e funnel, long expectedInsertions, double fpp, Strategy strategy) { ...... } BloomFilter一共四个create方法，不过最终都是走向第四个。看一下每个参数的含义： funnel：数据类型(一般是调用Funnels工具类中的) expectedInsertions：期望插入的值的个数 fpp 错误率(默认值为0.03) strategy 哈希算法(我也不懂啥意思)Bloom Filter的应用 在最后一个create方法中，设置一个断点： 上面的numBits，表示存一百万个int类型数字，需要的位数为7298440，700多万位。理论上存一百万个数，一个int是4字节32位，需要481000000=3200万位。如果使用HashMap去存，按HashMap50%的存储效率，需要6400万位。可以看出BloomFilter的存储空间很小，只有HashMap的1/10左右 上面的numHashFunctions，表示需要5个函数去存这些数字 使用第三个create方法，我们设置下错误率： private static BloomFilter\u003cInteger\u003e bf = BloomFilter.create(Funnels.integerFunnel(), total, 0.0003); 再运行看看： 此时误伤的数量为4，错误率为0.04%左右。 当错误率设为0.0003时，所需要的位数为16883499，1600万位，需要12个函数 和上面对比可以看出，错误率越大，所需空间和时间越小，错误率越小，所需空间和时间约大 常见的几个应用场景： cerberus在收集监控数据的时候, 有的系统的监控项量会很大, 需要检查一个监控项的名字是否已经被记录到db过了, 如果没有的话就需要写入db. 爬虫过滤已抓到的url就不再抓，可用bloom filter过滤 垃圾邮件过滤。如果用哈希表，每存储一亿个 email地址，就需要 1.6GB的内存（用哈希表实现的具体办法是将每一个 email地址对应成一个八字节的信息指纹，然后将这些信息指纹存入哈希表，由于哈希表的存储效率一般只有 50%，因此一个 email地址需要占用十六个字节。一亿个地址大约要 1.6GB，即十六亿字节的内存）。因此存贮几十亿个邮件地址可能需要上百 GB的内存。而Bloom Filter只需要哈希表 1/8到 1/4 的大小就能解决同样的问题。 觉得有用的话欢迎 关注 点赞 分享 【敖丙】| 文 ","date":"2022-03-18","objectID":"/posts/middleware/redis/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8bloomfilter/:2:0","tags":["redis"],"title":"布隆过滤器(BloomFilter)","uri":"/posts/middleware/redis/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8bloomfilter/"},{"categories":null,"content":"死锁的概念 死锁（Deadlock）：是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。称此时系统处于死锁状态或系统产生了死锁。 称这些永远在互相等待的进程为死锁进程。 所占用的资源或者需要它们进行某种合作的其它进程就会相继陷入死锁，最终可能导致整个系统处于瘫痪状态。 ","date":"2022-03-18","objectID":"/posts/linux/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%94%81/:1:0","tags":["操作系统","八股"],"title":"操作系统锁","uri":"/posts/linux/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%94%81/"},{"categories":null,"content":"死锁产生的条件 互斥条件 不可剥夺条件 占有并请求条件 循环等待条件 ","date":"2022-03-18","objectID":"/posts/linux/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%94%81/:2:0","tags":["操作系统","八股"],"title":"操作系统锁","uri":"/posts/linux/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%94%81/"},{"categories":null,"content":"死锁的破坏 ","date":"2022-03-18","objectID":"/posts/linux/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%94%81/:3:0","tags":["操作系统","八股"],"title":"操作系统锁","uri":"/posts/linux/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%94%81/"},{"categories":null,"content":"破坏互斥条件 ","date":"2022-03-18","objectID":"/posts/linux/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%94%81/:3:1","tags":["操作系统","八股"],"title":"操作系统锁","uri":"/posts/linux/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%94%81/"},{"categories":null,"content":"破坏不可剥夺条件 ","date":"2022-03-18","objectID":"/posts/linux/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%94%81/:3:2","tags":["操作系统","八股"],"title":"操作系统锁","uri":"/posts/linux/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%94%81/"},{"categories":null,"content":"破环占有并请求条件 ","date":"2022-03-18","objectID":"/posts/linux/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%94%81/:3:3","tags":["操作系统","八股"],"title":"操作系统锁","uri":"/posts/linux/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%94%81/"},{"categories":null,"content":"破坏循环等待条件 ref https://www.cnblogs.com/wkfvawl/p/11598647.html ","date":"2022-03-18","objectID":"/posts/linux/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%94%81/:3:4","tags":["操作系统","八股"],"title":"操作系统锁","uri":"/posts/linux/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%94%81/"},{"categories":null,"content":"Go 交叉编译 go 语言再啥平台都支持交叉编译，值得注意的是 cgo，在一些 linux 发行版中用的从语言库 go 并不支持，所以不能使用 cgo，再一些情况下 cgo 是可以提升运行速度的 ","date":"2022-03-15","objectID":"/posts/go/go%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/:1:0","tags":["Go"],"title":"Go交叉编译","uri":"/posts/go/go%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/"},{"categories":null,"content":"使用 ","date":"2022-03-15","objectID":"/posts/go/go%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/:2:0","tags":["Go"],"title":"Go交叉编译","uri":"/posts/go/go%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/"},{"categories":null,"content":"选择需要编译的系统 编译 linux go env -w GOOS=linux 编译 windows go env -w GOOS=windows ###选择需要编译的 cpu 架构 go env -w GOARCH=amd64 go env -w GOARCH=arm64 ","date":"2022-03-15","objectID":"/posts/go/go%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/:2:1","tags":["Go"],"title":"Go交叉编译","uri":"/posts/go/go%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/"},{"categories":null,"content":"支持的平台 GOOS GOARCH aix ppc64 android 386 android amd64 android arm android arm64 darwin amd64 darwin arm64 dragonfly amd64 freebsd 386 freebsd amd64 freebsd arm illumos amd64 ios arm64 js wasm linux 386 linux amd64 linux arm linux arm64 linux ppc64 linux ppc64le linux mips linux mipsle linux mips64 linux mips64le linux riscv64 linux s390x netbsd 386 netbsd amd64 netbsd arm openbsd 386 openbsd amd64 openbsd arm openbsd arm64 plan9 386 plan9 amd64 plan9 arm solaris amd64 windows 386 windows amd64 windows arm windows arm64 ","date":"2022-03-15","objectID":"/posts/go/go%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/:3:0","tags":["Go"],"title":"Go交叉编译","uri":"/posts/go/go%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/"},{"categories":null,"content":"前言 最近超星改网页改的厉害，导致很多网页脚本无法运行 遂使用命令行，绕过网页直接刷 ","date":"2022-03-13","objectID":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E6%89%8B%E6%9C%BA%E5%91%BD%E4%BB%A4%E8%A1%8C/:1:0","tags":["学习通"],"title":"学习通脚本安装（手机命令行）","uri":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E6%89%8B%E6%9C%BA%E5%91%BD%E4%BB%A4%E8%A1%8C/"},{"categories":null,"content":"刷前提醒 ","date":"2022-03-13","objectID":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E6%89%8B%E6%9C%BA%E5%91%BD%E4%BB%A4%E8%A1%8C/:2:0","tags":["学习通"],"title":"学习通脚本安装（手机命令行）","uri":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E6%89%8B%E6%9C%BA%E5%91%BD%E4%BB%A4%E8%A1%8C/"},{"categories":null,"content":"刷前提醒 开倍数，使用脚本有风险，出现的结果本人一概不负责 脚本仅供学习使用 这个暂时刷题功能不完善，只能刷视频，建议刷完视频后在用网页脚本刷题 只支持安卓手机 ","date":"2022-03-13","objectID":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E6%89%8B%E6%9C%BA%E5%91%BD%E4%BB%A4%E8%A1%8C/:3:0","tags":["学习通"],"title":"学习通脚本安装（手机命令行）","uri":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E6%89%8B%E6%9C%BA%E5%91%BD%E4%BB%A4%E8%A1%8C/"},{"categories":null,"content":"快速使用 点击这里下载命令行软件，安装，安装完成后打开 复制下面这一行代码，到命令行那里，粘贴，回车 curl -L -o cx.sh https://gh.fakev.cn/lyj0309/chaoxing-xuexitong-autoflush/raw/master/android.sh \u0026\u0026 chmod +x cx.sh \u0026\u0026 ./cx.sh 等待命令加载完毕，即可使用 如需再次使用，执行上面那一行代码即可 ","date":"2022-03-13","objectID":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E6%89%8B%E6%9C%BA%E5%91%BD%E4%BB%A4%E8%A1%8C/:4:0","tags":["学习通"],"title":"学习通脚本安装（手机命令行）","uri":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E6%89%8B%E6%9C%BA%E5%91%BD%E4%BB%A4%E8%A1%8C/"},{"categories":null,"content":"前言 最近超星改网页改的厉害，导致很多网页脚本无法运行 遂使用命令行，绕过网页直接刷 ","date":"2022-03-11","objectID":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E5%91%BD%E4%BB%A4%E8%A1%8C/:1:0","tags":["学习通"],"title":"学习通脚本安装（命令行）","uri":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E5%91%BD%E4%BB%A4%E8%A1%8C/"},{"categories":null,"content":"刷前提醒 开倍数，使用脚本有风险，出现的结果本人一概不负责 脚本仅供学习使用 这个暂时刷题功能不完善，只能刷视频，建议刷完视频后在用网页脚本刷题 只支持win64位 ","date":"2022-03-11","objectID":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E5%91%BD%E4%BB%A4%E8%A1%8C/:2:0","tags":["学习通"],"title":"学习通脚本安装（命令行）","uri":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E5%91%BD%E4%BB%A4%E8%A1%8C/"},{"categories":null,"content":"快速使用 点这里下载exe文件，打开，输入账号密码即可开始 脚本截图 ","date":"2022-03-11","objectID":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E5%91%BD%E4%BB%A4%E8%A1%8C/:3:0","tags":["学习通"],"title":"学习通脚本安装（命令行）","uri":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E5%91%BD%E4%BB%A4%E8%A1%8C/"},{"categories":null,"content":"参考文献 https://github.com/lyj0309/chaoxing-xuexitong-autoflush ","date":"2022-03-11","objectID":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E5%91%BD%E4%BB%A4%E8%A1%8C/:4:0","tags":["学习通"],"title":"学习通脚本安装（命令行）","uri":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E5%91%BD%E4%BB%A4%E8%A1%8C/"},{"categories":null,"content":"go提供了一个读取库encoding package main import ( \"encoding/csv\" \"log\" \"os\" ) //go语言读写csv文件 func main() { //创建一个io对象 filename:=\"Person1.csv\" f := ReadCsv(filename) //WriterCSV(filename) } //csv文件读取 func ReadCsv(filepath string) *[][]string { //打开文件(只读模式)，创建io.read接口实例 opencast,err:=os.Open(filepath) if err!=nil{ log.Println(\"csv文件打开失败！\") } defer opencast.Close() //创建csv读取接口实例 reader:=csv.NewReader(opencast) //获取一行内容，一般为第一行内容 //read,_:=reader.Read() //返回切片类型：[chen hai wei] //log.Println(read) //读取所有内容 ReadAll,err:=reader.ReadAll()//返回切片类型：[[s s ds] [a a a]] log.Println(ReadAll) return \u0026ReadAll /* 说明： 1、读取csv文件返回的内容为切片类型，可以通过遍历的方式使用或Slicer[0]方式获取具体的值。 2、同一个函数或线程内，两次调用Read()方法时，第二次调用时得到的值为每二行数据，依此类推。 3、大文件时使用逐行读取，小文件直接读取所有然后遍历，两者应用场景不一样，需要注意。 */ } //csv文件写入 func WriterCSV(path string) { //OpenFile读取文件，不存在时则创建，使用追加模式 File,err:=os.OpenFile(path,os.O_RDWR|os.O_APPEND|os.O_CREATE,0666) if err!=nil{ log.Println(\"文件打开失败！\") } defer File.Close() //创建写入接口 WriterCsv:=csv.NewWriter(File) str:=[]string{\"chen1\",\"hai1\",\"wei1\"} //需要写入csv的数据，切片类型 //写入一条数据，传入数据为切片(追加模式) err1:=WriterCsv.Write(str) if err1!=nil{ log.Println(\"WriterCsv写入文件失败\") } WriterCsv.Flush() //刷新，不刷新是无法写入的 log.Println(\"数据写入成功...\") } ","date":"2022-03-02","objectID":"/posts/go/go-csv/:0:0","tags":["Go"],"title":"Go Csv","uri":"/posts/go/go-csv/"},{"categories":null,"content":"文本操作 ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:0:0","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"cat 查看文件 cat filename 写入文件 cat \u003e\u003e filename \u003c\u003c EOF #EOF是标记符，写什么无所谓，但EOF为大家默认 \u003e hahaha \u003e hehehe EOF #再次输入标记符表示结束，如果最初的标记符不是EOF，那么这里就需要和第一行一致 追加内容 cat \u003c\u003c EOF \u003e file1 111 222 333 EOF ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:1:0","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"grep 　Linux系统中grep命令是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来（匹配到的标红）。grep全称是Global Regular Expression Print，表示全局正则表达式版本，它的使用权限是所有用户。 　grep的工作方式是这样的，它在一个或多个文件中搜索字符串模板。如果模板包括空格，则必须被引用，模板后的所有字符串被看作文件名。搜索的结果被送到标准输出，不影响原文件内容。 　grep可用于shell脚本，因为grep通过返回一个状态值来说明搜索的状态，如果模板搜索成功，则返回0，如果搜索不成功，则返回1，如果搜索的文件不存在，则返回2。我们利用这些返回值就可进行一些自动化的文本处理工作。 egrep = grep -E：扩展的正则表达式 （除了 \\\u003c , \\\u003e , \\b 使用其他正则都可以去掉\\） ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:2:0","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"sed ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:3:0","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"3.1 认识sed 　sed 是一种流编辑器，它一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”（patternspace ），接着用sed 命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。然后读入下行，执行下一个循环。如果没有使诸如‘D’ 的特殊命令，那会在两个循环之间清空模式空间，但不会清空保留空间。这样不断重复，直到文件末尾。文件内容并没有改变，除非你使用重定向存储输出或-i。 　功能：主要用来自动编辑一个或多个文件, 简化对文件的反复操作  ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:3:1","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"3.2 使用sed 3.2.1 命令格式 sed [options] ‘[地址定界] command’ file(s) 　3.2.2 常用选项options  -n：不输出模式空间内容到屏幕，即不自动打印，只打印匹配到的行  **-e：**多点编辑，对每行处理时，可以有多个Script  -f：把Script写到文件当中，在执行sed时-f 指定文件路径，如果是多个Script，换行写  -r：支持扩展的正则表达式  -i：直接将处理的结果写入文件  -i.bak：在将处理的结果写入文件之前备份一份  3.2.3 地址定界  不给地址：对全文进行处理  单地址：  #: 指定的行  /pattern/：被此处模式所能够匹配到的每一行  地址范围：  #,#  #,+#  /pat1/,/pat2/  #,/pat1/  ~：步进  sed -n**‘1~2p’** 只打印奇数行 （1~2 从第1行，一次加2行）  sed -n ‘2~2p’ 只打印偶数行  3.2.4 编辑命令command  d：删除模式空间匹配的行，并立即启用下一轮循环  p：打印当前模式空间内容，追加到默认输出之后  a：在指定行后面追加文本，支持使用\\n实现多行追加  i：在行前面插入文本，支持使用\\n实现多行追加  c：替换行为单行或多行文本，支持使用\\n实现多行追加  w：保存模式匹配的行至指定文件  r：读取指定文件的文本至模式空间中匹配到的行后  =：为模式空间中的行打印行号  !：模式空间中匹配行取反处理  s///：查找替换，支持使用其它分隔符，如：s@@@，s###；  加g表示行内全局替换；  在替换时，可以加一下命令，实现大小写转换  \\l：把下个字符转换成小写。  \\L：把replacement字母转换成小写，直到\\U或\\E出现。  \\u：把下个字符转换成大写。  \\U：把replacement字母转换成大写，直到\\L或\\E出现。  \\E：停止以\\L或\\U开始的大小写转换 ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:3:2","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"实例 [root@along ~]# cat demo aaa bbbb AABBCCDD [root@along ~]# sed \"/aaa/p\" demo #匹配到的行会打印一遍，不匹配的行也会打印 aaa aaa bbbb AABBCCDD [root@along ~]# sed -n \"/aaa/p\" demo #-n不显示没匹配的行 aaa [root@along ~]# sed -e \"s/a/A/\" -e \"s/b/B/\" demo #-e多点编辑 Aaa Bbbb AABBCCDD ########################### [root@along ~]# sed -n \"p\" demo #不指定行，打印全文 aaa bbbb AABBCCDD [root@along ~]# sed \"2s/b/B/g\" demo #替换第2行的b-\u003eB aaa BBBB AABBCCDD [root@along ~]# sed -n \"/aaa/p\" demo aaa [root@along ~]# sed -n \"1,2p\" demo #打印1-2行 aaa bbbb [root@along ~]# sed -n \"/aaa/,/DD/p\" demo aaa bbbb AABBCCDD [root@along ~]# sed -n \"2,/DD/p\" demo bbbb AABBCCDD [root@along ~]# sed \"1~2s/[aA]/E/g\" demo #将奇数行的a或A替换为E EEE bbbb EEBBCCDD ################ [root@along ~]# cat demo aaa bbbb AABBCCDD [root@along ~]# sed \"2d\" demo #删除第2行 aaa AABBCCDD [root@along ~]# sed -n \"2p\" demo #打印第2行 bbbb [root@along ~]# sed \"2a123\" demo #在第2行后加123 aaa bbbb 123 AABBCCDD [root@along ~]# sed \"1i123\" demo #在第1行前加123 123 aaa bbbb AABBCCDD [root@along ~]# sed \"3c123\\n456\" demo #替换第3行内容 aaa bbbb 123 456 [root@along ~]# sed -n \"3w/root/demo3\" demo #保存第3行的内容到demo3文件中 [root@along ~]# cat demo3 AABBCCDD [root@along ~]# sed \"1r/root/demo3\" demo #读取demo3的内容到第1行后 aaa AABBCCDD bbbb AABBCCDD [root@along ~]# sed -n \"=\" demo #=打印行号 1 2 3 [root@along ~]# sed -n '2!p' demo #打印除了第2行的内容 aaa AABBCCDD [root@along ~]# sed 's@[a-z]@\\u\u0026@g' demo #将全文的小写字母替换为大写字母 AAA BBBB AABBCCDD ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:3:3","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"awk 杂项 ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:4:0","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"环境变量 主要有两种环境变量，一种是含PATH的环境变量，一种则直接是键值对 含PATH的环境变量一般用于软件的导入，比如导入一个文件夹下面所有文件然后可以直接通过命令行访问，比如把mingw/src文件夹导入从而可以使用gcc 键值对环境变量则可以用来进行配置的读取，如NODEJS_ORG_MIRROR=https://npm.taobao.org/mirrors/node这个环境变量，则是告诉node使用淘宝镜像，就如同全局变量 ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:5:0","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"读取环境变量 export命令显示当前系统定义的所有环境变量 echo $PATH命令输出当前的PATH环境变量的值 ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:5:1","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"配置环境变量 临时变量（当前命令行有用） PATH变量 ：使用export命令直接修改PATH的值，配置MySQL进入环境变量的方法: export PATH=/home/uusama/mysql/bin:$PATH 非PATH变量： export GO_VERSION=1.17.6 永久变量 主要是修改配置文件的方式 一共有四个文件，修改方式则是把设置临时变量的句子加到文件中 /etc/profile： 此文件为系统的每个用户设置环境信息,当用户第一次登录时,该文件被执行。是系统全局针对终端环境的设置，它是login时最先被系统加载的，是它调用了/etc/bashrc，以及/etc/profile.d目录下的*.sh文件，如果有一个软件包，系统上只安装一份，供所有开发者使用，建议在/etc/profile.d下创建一个新的xxx.sh，配置环境变量。 ~/.bashrc:是用户相关的终端（shell）的环境设置，通常打开一个新终端时，默认会load里面的设置，在这里的设置不影响其它人。如果一个服务器多个开发者使用，大家都需要有自己的sdk安装和设置，那么最好就是设置它。 /etc/bashrc: 是系统全局针对终端环境的设置，修改了它，会影响所有用户的终端环境，这里一般配置终端如何与用户进行交互的增强功能等（比如sudo提示、命令找不到提示安装什么包等），新开的终端，已经load了这个配置，最后才load用户自己的 ~/.bashrc。 ~/.bash_profile:每个用户都可使用该文件输入专用于自己使用的shell信息,当用户登录时,该文件仅仅执行一次!默认情况下,他设置一些环境变量,执行用户的.bashrc文件. ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:5:2","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"swap 创建一个swap文件,大小2G，count代表大小。 cd /var sudo dd if=/dev/zero of=swapfile bs=1024 count=2000000 chmod 0600 swapfile sudo mkswap swapfile #挂载： sudo swapon /var/swapfile #卸载： # sudo swapoff /var/swapfile 开机自己挂载 开机自动挂载SWAP分区， 编辑 /etc/fstab，末行添加： cat \u003c\u003c EOF \u003e /etc/fstab /var/swapfile swap swap defaults 0 0 EOF ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:6:0","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"压缩 ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:7:0","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"zip 将 /home/html/ 这个目录下所有文件和文件夹打包为当前目录下的 html.zip： zip -q -r html.zip /home/html zip *就可以压缩当前文件夹了，与别的命令有点不同，这个命令不能识别./或.必须要* ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:7:1","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"tar 压缩 a.c文件为test.tar.gz touch a.c tar -czvf test.tar.gz a.c # a.c 解压文件 tar -xzvf test.tar.gz #a.c ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:7:2","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"监控 ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:8:0","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"查看网络流量 nload ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:8:1","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"任务管理器 top htop ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:8:2","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"查看端口占用 lsof -i:端口号 netstat -ntlp //查看当前所有tcp端口 netstat -ntulp | grep 80 //查看所有80端口使用情况 netstat -ntulp | grep 3306 //查看所有3306端口使用情况 ","date":"2022-02-18","objectID":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:8:3","tags":null,"title":"Linux基本命令","uri":"/posts/draft/linux%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":null,"content":"在 kibana.yml配置文件中加一行 docker exec -ti efk-kibana-1 sh -c “cat « EOF \u003e config/kibana.yml i18n.locale: \"zh-CN\" EOF \" docker exec -it -c efk-kibana-1 cat i18n.locale: \"zh-CN\" ","date":"2022-02-18","objectID":"/posts/middleware/elastic/kibana%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:0:0","tags":["elastic"],"title":"Kibana改中文","uri":"/posts/middleware/elastic/kibana%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"Elasticsearch8.0基本使用 https://www.elastic.co/guide/en/elasticsearch/reference/8.0/getting-started.html 官方文档 Quick start ","date":"2022-02-18","objectID":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:0:0","tags":["elastic"],"title":"Elasticsearch8.0基本使用","uri":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"简介 首先介绍几个概念，为了方便理解，这里拿数据库做类比 索引相当于表 mappings properties 相当于传统数据库中的表定义 ","date":"2022-02-18","objectID":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:1:0","tags":["elastic"],"title":"Elasticsearch8.0基本使用","uri":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"举个例子 假设我现在要创建一张叫做topic的表，有2个字段，topic和answer,类型是text 不同的是elastic不会中文分词，所以要添加分词器，如ik分词器，所以要添加analyzer这个字段 PUT topic 请求体 { \"mappings\": { \"properties\": { \"topic\": { \"type\": \"text\", \"analyzer\": \"ik_smart\" }, \"answer\": { \"type\": \"text\", \"analyzer\": \"ik_smart\" } } } } ","date":"2022-02-18","objectID":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:1:1","tags":["elastic"],"title":"Elasticsearch8.0基本使用","uri":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"es配置用户名密码 修改配置文件./config/elasticsearch.yml docker exec es bash cat « EOF \u003e ./config/elasticsearch.yml xpack.security.enabled: true xpack.security.transport.ssl.enabled: true EOF xpack.security.enabled: true xpack.security.transport.ssl.enabled: true 执行 ./bin/elasticsearch-setup-passwords interactive 设置kibana.yml elasticsearch.username: \"elastic\" elasticsearch.password: \"xxx\" ","date":"2022-02-18","objectID":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:2:0","tags":["elastic"],"title":"Elasticsearch8.0基本使用","uri":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"创建索引 PUT {{name}} 请求体 { \"mappings\": { \"properties\": { \"topic\": { \"type\": \"text\", \"analyzer\": \"ik_smart\" }, \"answer\": { \"type\": \"text\", \"analyzer\": \"ik_smart\" } } } } ","date":"2022-02-18","objectID":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:3:0","tags":["elastic"],"title":"Elasticsearch8.0基本使用","uri":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"添加数据 ","date":"2022-02-18","objectID":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:4:0","tags":["elastic"],"title":"Elasticsearch8.0基本使用","uri":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"单个数据 POST topic/_doc { \"topic\": \"1+1=2\", \"answer\": \"正确\" } #or POST logs-my_app-default/_doc { \"@timestamp\": \"2099-05-06T16:21:15.000Z\", \"event\": { \"original\": \"192.0.2.42 - - [06/May/2099:16:21:15 +0000] \\\"GET /images/bg.jpg HTTP/1.0\\\" 200 24736\" } } ","date":"2022-02-18","objectID":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:4:1","tags":["elastic"],"title":"Elasticsearch8.0基本使用","uri":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"多个数据 PUT logs-my_app-default/_bulk { \"create\": { } } { \"@timestamp\": \"2099-05-07T16:24:32.000Z\", \"event\": { \"original\": \"192.0.2.242 - - [07/May/2020:16:24:32 -0500] \\\"GET /images/hm_nbg.jpg HTTP/1.0\\\" 304 0\" } } { \"create\": { } } { \"@timestamp\": \"2099-05-08T16:25:42.000Z\", \"event\": { \"original\": \"192.0.2.255 - - [08/May/2099:16:25:42 +0000] \\\"GET /favicon.ico HTTP/1.0\\\" 200 3638\" } } #or POST topic/_bulk {\"index\":{\"_index\":\"topic\",\"_id\":\"1\"}} {\"topic\":\"乌鲁木齐惊现彩虹\",\"answer\":\"今日午后一场大雨过后，乌鲁木齐天空上出现一道彩虹\"} ","date":"2022-02-18","objectID":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:4:2","tags":["elastic"],"title":"Elasticsearch8.0基本使用","uri":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"搜索数据 GET logs-my_app-default/_search { \"query\": { \"match_all\": { } }, \"fields\": [ \"@timestamp\" ], \"_source\": false, \"sort\": [ { \"@timestamp\": \"desc\" } ] } #or GET topic/_search { \"size\": 6, \"query\": { \"multi_match\": { \"query\": \"1+1\", \"fields\": [ \"topic^2\", \"answer\" ] } } } ","date":"2022-02-18","objectID":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:4:3","tags":["elastic"],"title":"Elasticsearch8.0基本使用","uri":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"删除数据 DELETE _data_stream/logs-my_app-default ","date":"2022-02-18","objectID":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:4:4","tags":["elastic"],"title":"Elasticsearch8.0基本使用","uri":"/posts/middleware/elastic/elastic%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":null,"content":"es安装 esver=8.0.0 # 设置系统内存 sudo sysctl -w vm.max_map_count=262144 # docker pull elasticsearch:$esver docker run --name es --net elastic -p 9200:9200 -p 9300:9300 -it docker.elastic.co/elasticsearch/elasticsearch:8.0.0 es就启动了 访问https://ip:9200就能看到，是需要密码的，密码可以从命令行看到，如果是arm则要手动生成 ","date":"2022-02-18","objectID":"/posts/middleware/elastic/elastic%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8ik%E5%88%86%E8%AF%8D%E5%99%A8/:1:0","tags":["elastic"],"title":"Elasticsearch8.0安装及使用ik分词器","uri":"/posts/middleware/elastic/elastic%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8ik%E5%88%86%E8%AF%8D%E5%99%A8/"},{"categories":null,"content":"设置密码 docker exec -it es /usr/share/elasticsearch/bin/elasticsearch-reset-password ","date":"2022-02-18","objectID":"/posts/middleware/elastic/elastic%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8ik%E5%88%86%E8%AF%8D%E5%99%A8/:1:1","tags":["elastic"],"title":"Elasticsearch8.0安装及使用ik分词器","uri":"/posts/middleware/elastic/elastic%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8ik%E5%88%86%E8%AF%8D%E5%99%A8/"},{"categories":null,"content":"分词器安装 esver=8.0.0 #安装解压 apt install unzip -y yum install unzip -y # 下载分词器 curl -L -o ik.zip https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v$esver/elasticsearch-analysis-ik-$esver.zip # wget gh.dlpu.workers.dev/https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v$esver/elasticsearch-analysis-ik-$esver.zip mkdir ik unzip -d ik ik.zip #将ik移动到容器中 docker cp ik es:/usr/share/elasticsearch/plugins rm -rf ik.zip ik docker restart es ","date":"2022-02-18","objectID":"/posts/middleware/elastic/elastic%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8ik%E5%88%86%E8%AF%8D%E5%99%A8/:2:0","tags":["elastic"],"title":"Elasticsearch8.0安装及使用ik分词器","uri":"/posts/middleware/elastic/elastic%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8ik%E5%88%86%E8%AF%8D%E5%99%A8/"},{"categories":null,"content":"yarn // 查询源 yarn config get registry // 更换国内源 yarn config set registry https://registry.npm.taobao.org/ // 恢复官方源 yarn config set registry https://registry.yarnpkg.com // 删除注册表 yarn config delete registry ","date":"2022-02-15","objectID":"/posts/front/npm%E5%92%8Cyarn%E6%8D%A2%E6%BA%90/:1:0","tags":["国内","前端"],"title":"Npm和yarn换源","uri":"/posts/front/npm%E5%92%8Cyarn%E6%8D%A2%E6%BA%90/"},{"categories":null,"content":"npm 注意 npm 更换国内镜像源之后，将无法再使用 npm search 命令，需要恢复为官方源才可以使用，如果恢复官方源后还不可使用，运行删除注册表命令后重试即可。 // 查询源 npm config get registry // 更换国内源 npm config set registry https://registry.npm.taobao.org/ // 恢复官方源 npm config set registry https://registry.npmjs.org // 删除注册表 npm config delete registry ","date":"2022-02-15","objectID":"/posts/front/npm%E5%92%8Cyarn%E6%8D%A2%E6%BA%90/:2:0","tags":["国内","前端"],"title":"Npm和yarn换源","uri":"/posts/front/npm%E5%92%8Cyarn%E6%8D%A2%E6%BA%90/"},{"categories":null,"content":"前言 railway是一个免费的PaaS平台https://railway.app,每月有5美元免费额度，如果添加支付方式则有5美元免费额度 ","date":"2022-02-12","objectID":"/posts/cloud/%E9%83%A8%E7%BD%B2cloudreve%E8%87%B3railway.app/:1:0","tags":null,"title":"部署cloudreve至railway.app","uri":"/posts/cloud/%E9%83%A8%E7%BD%B2cloudreve%E8%87%B3railway.app/"},{"categories":null,"content":"生成自己的github仓库 演示仓库 https://github.com/lyj0309/pan ","date":"2022-02-12","objectID":"/posts/cloud/%E9%83%A8%E7%BD%B2cloudreve%E8%87%B3railway.app/:2:0","tags":null,"title":"部署cloudreve至railway.app","uri":"/posts/cloud/%E9%83%A8%E7%BD%B2cloudreve%E8%87%B3railway.app/"},{"categories":null,"content":"目录说明 cr 这个文件是cloudreve的linux可执行文件,我用的3.4版本 Dockerfile，railway会通过这个自动构建镜像 FROMalpineENV PUID=1000 ENV PGID=1000 ENV TZ=\"Asia/Shanghai\"LABEL MAINTAINER=\"lyj0309\"WORKDIR/app# ADD config.ini .ADD cr .RUN echo \"\u003e\u003e\u003e\u003e\u003e\u003e update dependencies\" \\ \u0026\u0026 apk update \\ \u0026\u0026 apk add tzdata gcompat\\ \u0026\u0026 echo \"\u003e\u003e\u003e\u003e\u003e\u003e set up timezone\" \\ \u0026\u0026 cp /usr/share/zoneinfo/${TZ} /etc/localtime \\ \u0026\u0026 echo ${TZ} \u003e /etc/timezone \\ \u0026\u0026 echo \"\u003e\u003e\u003e\u003e\u003e\u003e fix premission\" \\ \u0026\u0026 chmod +x /app/crEXPOSE5212# ENTRYPOINT [\"/app/cr\",\"-c\",\"/app/config.ini\"] ENTRYPOINT [\"/app/cr\"] 神奇的是，似乎由于滥用，railway已经屏蔽了一切有关cloudreve的东西 注意： 仓库名字不要包含cloudreve 仓库文件名不要包含cloudreve 仓库文件内容不要包含cloudreve ","date":"2022-02-12","objectID":"/posts/cloud/%E9%83%A8%E7%BD%B2cloudreve%E8%87%B3railway.app/:2:1","tags":null,"title":"部署cloudreve至railway.app","uri":"/posts/cloud/%E9%83%A8%E7%BD%B2cloudreve%E8%87%B3railway.app/"},{"categories":null,"content":"安装 在railway中导入仓库 接着会自动构建app，构建成功后，在运行日志里面查看初始密码 系统应该会提示修改端口，如果没有则自己修改 端口为5212 ","date":"2022-02-12","objectID":"/posts/cloud/%E9%83%A8%E7%BD%B2cloudreve%E8%87%B3railway.app/:3:0","tags":null,"title":"部署cloudreve至railway.app","uri":"/posts/cloud/%E9%83%A8%E7%BD%B2cloudreve%E8%87%B3railway.app/"},{"categories":null,"content":"检验 railway会自动送你个域名访问 想添加自定义域名也是可以的 ","date":"2022-02-12","objectID":"/posts/cloud/%E9%83%A8%E7%BD%B2cloudreve%E8%87%B3railway.app/:4:0","tags":null,"title":"部署cloudreve至railway.app","uri":"/posts/cloud/%E9%83%A8%E7%BD%B2cloudreve%E8%87%B3railway.app/"},{"categories":null,"content":"市面上有很多博客的生成框架，hugo, wordpress, hexo 我选择hugo有以下几点 我是gopher hugo生成网站快 配置简单 官方网站https://gohugo.io/ ","date":"2022-02-11","objectID":"/posts/hugo%E5%AE%89%E8%A3%85/:0:0","tags":["other"],"title":"Hugo安装与使用","uri":"/posts/hugo%E5%AE%89%E8%A3%85/"},{"categories":null,"content":"安装 ","date":"2022-02-11","objectID":"/posts/hugo%E5%AE%89%E8%A3%85/:1:0","tags":["other"],"title":"Hugo安装与使用","uri":"/posts/hugo%E5%AE%89%E8%A3%85/"},{"categories":null,"content":"win 这里推荐使用choco安装, 直接 choco install hugo即可 ps: 最好把代理打开，choco会自动走代理 ","date":"2022-02-11","objectID":"/posts/hugo%E5%AE%89%E8%A3%85/:1:1","tags":["other"],"title":"Hugo安装与使用","uri":"/posts/hugo%E5%AE%89%E8%A3%85/"},{"categories":null,"content":"生成站点 hugo new site quickstart 这时候就会生成一个名叫quickstart的文件夹，里面包含了hugo站点的一些东西 ","date":"2022-02-11","objectID":"/posts/hugo%E5%AE%89%E8%A3%85/:1:2","tags":["other"],"title":"Hugo安装与使用","uri":"/posts/hugo%E5%AE%89%E8%A3%85/"},{"categories":null,"content":"添加主题 github上面有许许多多的主题，直接去上面搜索即可 https://github.com/search?q=hugo+theme 找到心仪的主题，下载或clone到theme文件夹 ","date":"2022-02-11","objectID":"/posts/hugo%E5%AE%89%E8%A3%85/:1:3","tags":["other"],"title":"Hugo安装与使用","uri":"/posts/hugo%E5%AE%89%E8%A3%85/"},{"categories":null,"content":"添加页面 hugo new /posts/{name}.md name是你文章的名字，支持中文，运行后会在文件夹下面生成md文件，直接编写即可 ps:现在生成的页面是草稿页面，当你写完后去掉draft: true这一行即可变成正式页面 ","date":"2022-02-11","objectID":"/posts/hugo%E5%AE%89%E8%A3%85/:1:4","tags":["other"],"title":"Hugo安装与使用","uri":"/posts/hugo%E5%AE%89%E8%A3%85/"},{"categories":null,"content":"配置主题 在hugo站点的跟文件夹下面有一个config.toml的配置文件，把里面的theme改成你下载的主题 每个主题的配置文件都不一样，你需要仔细浏览主题的教程 ","date":"2022-02-11","objectID":"/posts/hugo%E5%AE%89%E8%A3%85/:1:5","tags":["other"],"title":"Hugo安装与使用","uri":"/posts/hugo%E5%AE%89%E8%A3%85/"},{"categories":null,"content":"生成预览 hugo server -D即可生成预览，包含草稿 hugo server不包含草稿 运行后会生成一个地址，访问即可预览 ","date":"2022-02-11","objectID":"/posts/hugo%E5%AE%89%E8%A3%85/:1:6","tags":["other"],"title":"Hugo安装与使用","uri":"/posts/hugo%E5%AE%89%E8%A3%85/"},{"categories":null,"content":"生成静态文件 hugo -D包含草稿 hugo 不包含草稿 运行后，在目录下面会生吃一个public的文件夹，里面即使成品 ","date":"2022-02-11","objectID":"/posts/hugo%E5%AE%89%E8%A3%85/:1:7","tags":["other"],"title":"Hugo安装与使用","uri":"/posts/hugo%E5%AE%89%E8%A3%85/"},{"categories":null,"content":"前言 市面上有许许多多的静态网站部署，vercel,netify,aws,azure,github pages··· 我经过多方考虑，最终选择了azure的静态网站部署。下面是一张对照表 ps: 都有免费额度,azure我用的是外币卡注册的，应该学生账号也可以 azure vercel github pages 地区\u0026速度 可选香港，速度起飞 美国aws，速度一般般 速度不慢，但github.io经常阻断 免费流量 100G 100G 自定义域名 2个 无限制 无限制 自动ssl 有 有 有 serverless Azure Functions 有 无 最大应用大小 250mb 15,000个文件 无 vercel azure firebase 综上，除了地区和速度之外，其他的也差不多，而且，香港的一般也不会阻断，所以我选择azure ","date":"2022-02-11","objectID":"/posts/hugo%E5%85%8D%E8%B4%B9%E9%83%A8%E7%BD%B2azure/:1:0","tags":["other","白嫖"],"title":"Hugo免费部署azure","uri":"/posts/hugo%E5%85%8D%E8%B4%B9%E9%83%A8%E7%BD%B2azure/"},{"categories":null,"content":"安装hugo和生成与写博客 点这里 ","date":"2022-02-11","objectID":"/posts/hugo%E5%85%8D%E8%B4%B9%E9%83%A8%E7%BD%B2azure/:2:0","tags":["other","白嫖"],"title":"Hugo免费部署azure","uri":"/posts/hugo%E5%85%8D%E8%B4%B9%E9%83%A8%E7%BD%B2azure/"},{"categories":null,"content":"部署至azure 官方文档 我采用的是github储存 ","date":"2022-02-11","objectID":"/posts/hugo%E5%85%8D%E8%B4%B9%E9%83%A8%E7%BD%B2azure/:3:0","tags":["other","白嫖"],"title":"Hugo免费部署azure","uri":"/posts/hugo%E5%85%8D%E8%B4%B9%E9%83%A8%E7%BD%B2azure/"},{"categories":null,"content":"将站点推送至github 创建github储存库 git remote add origin https://github.com/\u003cYOUR_USER_NAME\u003e/hugo-static-app git push --set-upstream origin main ","date":"2022-02-11","objectID":"/posts/hugo%E5%85%8D%E8%B4%B9%E9%83%A8%E7%BD%B2azure/:3:1","tags":["other","白嫖"],"title":"Hugo免费部署azure","uri":"/posts/hugo%E5%85%8D%E8%B4%B9%E9%83%A8%E7%BD%B2azure/"},{"categories":null,"content":"部署 打开Azure控制台 https://portal.azure.com/#create/Microsoft.StaticApp 新建web应用 主要注意两点 地区选择east aisa(香港) 使用github登录，然后选择你的库 点击创建 这时候azure就会自动生成一个github action，在.github\\workflows\\xxx.yml,然后会自动运行，就部署上去了 ","date":"2022-02-11","objectID":"/posts/hugo%E5%85%8D%E8%B4%B9%E9%83%A8%E7%BD%B2azure/:3:2","tags":["other","白嫖"],"title":"Hugo免费部署azure","uri":"/posts/hugo%E5%85%8D%E8%B4%B9%E9%83%A8%E7%BD%B2azure/"},{"categories":null,"content":"自定义域名 在你的控制台下面有个自定义域的按钮，点击，在点击添加即可 ","date":"2022-02-11","objectID":"/posts/hugo%E5%85%8D%E8%B4%B9%E9%83%A8%E7%BD%B2azure/:4:0","tags":["other","白嫖"],"title":"Hugo免费部署azure","uri":"/posts/hugo%E5%85%8D%E8%B4%B9%E9%83%A8%E7%BD%B2azure/"},{"categories":null,"content":"注意事项 在我的网站实际部署中，因为采用了atomic-algolia这个插件，所以在根目录下有package.json文件，而azure添加的那个github action文件azure-static-web-appsxxx.yml实际上并没有指定编译环境，完全是由oryx这个编译器猜的，所以这玩意猜我是npm项目，而我是hugo,导致了error 解决办法就是在github action文件加一句，rm package.json ","date":"2022-02-11","objectID":"/posts/hugo%E5%85%8D%E8%B4%B9%E9%83%A8%E7%BD%B2azure/:5:0","tags":["other","白嫖"],"title":"Hugo免费部署azure","uri":"/posts/hugo%E5%85%8D%E8%B4%B9%E9%83%A8%E7%BD%B2azure/"},{"categories":null,"content":"//459. 重复的子字符串 func repeatedSubstringPattern(s string) bool { l := len(s) for i := 1; i \u003c= l/2; i++ { if l%i == 0 { subs := s[:i] t := true for k := 0; k \u003c l/i; k++ { if s[k*i:k*i+i] != subs { t = false break } } if t { return true } } } return false } //1447. 最简分数 func simplifiedFractions(n int) (res []string) { for i := 1; i \u003c n; i++ { for k := n; k \u003e 1; k-- { if i \u003e= k { break } //判断化简 for f := 2; f \u003c= i; f++ { if i%f == 0 \u0026\u0026 k%f == 0 { goto this } } res = append(res, strconv.Itoa(i)+\"/\"+strconv.Itoa(k)) this: } } return } //268. 丢失的数字 func missingNumber(nums []int) int { arr := make([]bool, len(nums)) for _, num := range nums { arr[num] = true } for i, b := range arr { if b == false { return i } } return 0 } //258. 各位相加 func addDigits(num int) int { for { arr := getBit(num) num = 0 for _, i := range arr { num += i } if num \u003c 10 { return num } } } //242. 有效的字母异位词 func isAnagram(s string, t string) bool { m := map[int32]int{} for _, i := range s { m[i]++ } for _, i := range t { m[i]-- } for _, i := range m { if i != 0 { return false } } return true } //228. 汇总区间 func summaryRanges(nums []int) (res []string) { if len(nums) == 0 { return } k := 0 for i := 0; i \u003c len(nums)-1; i++ { if nums[i+1]-nums[i] != 1 { //生成单个 r := strconv.Itoa(nums[i]) if k != i { r = strconv.Itoa(nums[k]) + \"-\u003e\" + r } res = append(res, r) k = i + 1 } } if k == len(nums)-1 { res = append(res, strconv.Itoa(nums[len(nums)-1])) } else { res = append(res, strconv.Itoa(nums[k])+\"-\u003e\"+strconv.Itoa(nums[len(nums)-1])) } return } type MyStack struct { arr []int } //225. 用队列实现栈 func Constructor() MyStack { return MyStack{} } func (s *MyStack) Push(x int) { s.arr = append(s.arr, x) } func (s *MyStack) Pop() int { a := s.arr[len(s.arr)] s.arr = s.arr[:len(s.arr)-1] return a } func (s *MyStack) Top() int { return s.arr[len(s.arr)] } func (s *MyStack) Empty() bool { return len(s.arr) == 0 } //219. 存在重复元素 II func containsNearbyDuplicate(nums []int, k int) bool { m := make(map[int]int) for i, num := range nums { if i-m[num] \u003c= k \u0026\u0026 m[num] != 0 { return true } m[num] = i } return false } func myPow(x float64, n int) float64 { return math.Pow(x, float64(n)) } func majorityElement(nums []int) int { m := make(map[int]int) for _, num := range nums { m[num]++ if m[num] \u003e (len(nums) / 2) { return num } } return 0 } //168. Excel表列名称 func convertToTitle(columnNumber int) string { ans := []byte{} for columnNumber \u003e 0 { a0 := (columnNumber-1)%26 + 1 ans = append(ans, 'A'+byte(a0-1)) columnNumber = (columnNumber - a0) / 26 } for i, n := 0, len(ans); i \u003c n/2; i++ { ans[i], ans[n-1-i] = ans[n-1-i], ans[i] } return string(ans) } //1996. 游戏中弱角色的数量 func numberOfWeakCharacters(properties [][]int) (ans int) { sort.Slice(properties, func(i, j int) bool { p, q := properties[i], properties[j] return p[0] \u003c q[0] || p[0] == q[0] \u0026\u0026 p[1] \u003e q[1] }) var st []int for _, p := range properties { for len(st) \u003e 0 \u0026\u0026 st[len(st)-1] \u003c p[1] { st = st[:len(st)-1] ans++ } st = append(st, p[1]) } return } //171. Excel 表列序号 func titleToNumber(columnTitle string) (res int) { l := len(columnTitle) k := 1 for i := l; i \u003e 0; i-- { res += int(columnTitle[i-1]-64) * k k *= 26 } return } //202. 快乐数 func isHappy(n int) bool { m := make(map[int]bool) for { arr := getBit(n) n = 0 for _, i := range arr { n += i * i } if m[n] == true { return false } m[n] = true if n \u003c= 3 { if n == 1 { return true } else { return false } } } } //217. 存在重复元素 func containsDuplicate(nums []int) bool { if len(nums) \u003e 0 { for k, v := range nums { for _, vv := range nums[k+1:] { if v == vv { return true } } } } return false } ","date":"2022-02-10","objectID":"/posts/go/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/:0:0","tags":null,"title":"力扣刷题","uri":"/posts/go/%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98/"},{"categories":null,"content":"前言 想把博客作为自己的笔记，用过有道云笔记和onenote，hugo美中不足的地方就是搜索不了笔记，直到后面我发现其实是可以搜索的，搜索方式也是多种多样，主要有3种 elastic lunr algolia 易搭建性 难 简单 中等 收费 自建服务器或者一些提供商（有免费额度） 免费 收费（有免费额度） 搜索速度 快 中等（基于浏览器，每次需要下载所有索引） 快 ","date":"2022-02-09","objectID":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/:1:0","tags":null,"title":"Hugo使用添加algolia搜索","uri":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/"},{"categories":null,"content":"配置algolia账号 ","date":"2022-02-09","objectID":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/:2:0","tags":null,"title":"Hugo使用添加algolia搜索","uri":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/"},{"categories":null,"content":"注册账号 官网https://www.algolia.com/ ","date":"2022-02-09","objectID":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/:2:1","tags":null,"title":"Hugo使用添加algolia搜索","uri":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/"},{"categories":null,"content":"创建应用 记住计划选择free,1w的请求和1w的记录应该是够的 ","date":"2022-02-09","objectID":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/:2:2","tags":null,"title":"Hugo使用添加algolia搜索","uri":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/"},{"categories":null,"content":"创建索引 ","date":"2022-02-09","objectID":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/:2:3","tags":null,"title":"Hugo使用添加algolia搜索","uri":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/"},{"categories":null,"content":"获取api key 右上角的用户-\u003esetting api key 其中，Application ID 和 Search-Only API Key 是hugo所需要的 ","date":"2022-02-09","objectID":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/:2:4","tags":null,"title":"Hugo使用添加algolia搜索","uri":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/"},{"categories":null,"content":"配置hugo 我是用的是loveIt主题，在这个主题的配置文件中有配置搜索这个选项 # 搜索配置 [params.search] enable = true # 搜索引擎的类型 (\"lunr\", \"algolia\") type = \"algolia\" # 文章内容最长索引长度 contentLength = 4000 # 搜索框的占位提示语 placeholder = \"\" # 最大结果数目 maxResultLength = 10 # 结果内容片段长度 snippetLength = 50 # 搜索结果中高亮部分的 HTML 标签 highlightTag = \"em\" # 是否在搜索索引中使用基于 baseURL 的绝对路径 absoluteURL = false [params.search.algolia] index = \"blog\" appID = \"xxx\" searchKey = \"xxx\" 其中appID和searchKey是上一步提到的 ","date":"2022-02-09","objectID":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/:3:0","tags":null,"title":"Hugo使用添加algolia搜索","uri":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/"},{"categories":null,"content":"索引的上传 至此，你的hugo已经可以使用algolia搜索了，但是还有一个问题，就是algolia并没有数据，即索引，我们需要生成索引，并上传给algolia，告诉他我们有哪些文章内容。 ","date":"2022-02-09","objectID":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/:4:0","tags":null,"title":"Hugo使用添加algolia搜索","uri":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/"},{"categories":null,"content":"非自动方式（不推荐） 运行hugo后，在public文件夹下面会生成一个index.json文件，这个即是索引文件，打开algolia网站，上传即可 ","date":"2022-02-09","objectID":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/:4:1","tags":null,"title":"Hugo使用添加algolia搜索","uri":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/"},{"categories":null,"content":"自动方式（推荐） github上面有个npm包，用来自动上传索引https://github.com/chrisdmacrae/atomic-algolia使用这个包即可 在你的根目录下新建package.json文件写入如下 { \"scripts\": { \"algolia\": \"atomic-algolia\" } } 添加github action文件到.github\\workclows name:GitHub Pageson:push:branches:- main # Set a branch name to trigger deploymentpull_request:jobs:deploy:runs-on:ubuntu-20.04concurrency:group:${{ github.workflow }}-${{ github.ref }}steps:- uses:actions/checkout@v2with:submodules:true# Fetch Hugo themes (true OR recursive)fetch-depth:0# Fetch all history for .GitInfo and .Lastmod- name:Setup Hugouses:peaceiris/actions-hugo@v2with:hugo-version:'0.85.0'- name:Buildrun:hugo --minify- name:Use Node.jsuses:actions/setup-node@v1with:node-version:'12.x'- name:Install automic-algoliarun:| npm install atomic-algolianpm run algoliaenv:ALGOLIA_APP_ID:${{ secrets.ALGOLIA_APP_ID }}ALGOLIA_ADMIN_KEY:${{ secrets.ALGOLIA_ADMIN_KEY }}ALGOLIA_INDEX_NAME:${{ secrets.ALGOLIA_INDEX_NAME }}ALGOLIA_INDEX_FILE:\"./public/index.json\"添加action的secrcts git push即可 ","date":"2022-02-09","objectID":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/:4:2","tags":null,"title":"Hugo使用添加algolia搜索","uri":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/"},{"categories":null,"content":"检验 所有配置好后，在站点右上有个搜索图标，点击搜索查看结果 响应速度还是非常快的 ","date":"2022-02-09","objectID":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/:5:0","tags":null,"title":"Hugo使用添加algolia搜索","uri":"/posts/hugo%E4%BD%BF%E7%94%A8%E6%B7%BB%E5%8A%A0algolia%E6%90%9C%E7%B4%A2/"},{"categories":null,"content":"实际上，刷机子的原理无非过一会儿发一个包看看能开通不，为了便携且贯彻落实白嫖精神，我使用的是railway部署的，一个免费的PaaS平台https://railway.app,每月有5美元免费额度 ","date":"2022-02-07","objectID":"/posts/cloud/%E7%94%B2%E9%AA%A8%E6%96%87%E4%BA%91%E8%87%AA%E5%8A%A8%E5%88%B7arm/:0:0","tags":["白嫖","cloud"],"title":"甲骨文云自动刷arm(使用railway.app)","uri":"/posts/cloud/%E7%94%B2%E9%AA%A8%E6%96%87%E4%BA%91%E8%87%AA%E5%8A%A8%E5%88%B7arm/"},{"categories":null,"content":"Github地址 https://github.com/lemoex/oci-help ","date":"2022-02-07","objectID":"/posts/cloud/%E7%94%B2%E9%AA%A8%E6%96%87%E4%BA%91%E8%87%AA%E5%8A%A8%E5%88%B7arm/:1:0","tags":["白嫖","cloud"],"title":"甲骨文云自动刷arm(使用railway.app)","uri":"/posts/cloud/%E7%94%B2%E9%AA%A8%E6%96%87%E4%BA%91%E8%87%AA%E5%8A%A8%E5%88%B7arm/"},{"categories":null,"content":"部署至railway ","date":"2022-02-07","objectID":"/posts/cloud/%E7%94%B2%E9%AA%A8%E6%96%87%E4%BA%91%E8%87%AA%E5%8A%A8%E5%88%B7arm/:2:0","tags":["白嫖","cloud"],"title":"甲骨文云自动刷arm(使用railway.app)","uri":"/posts/cloud/%E7%94%B2%E9%AA%A8%E6%96%87%E4%BA%91%E8%87%AA%E5%8A%A8%E5%88%B7arm/"},{"categories":null,"content":"建立自己的git仓库 按照网址上的教程配置好oci-help.ini和pem密钥 然后把源码clone下来，把.ini和pem都考进去 再添加个文件，Dockerfile FROMgolang:1.17.6 as builderENV GO111MODULE=on # CGO_ENABLED alpine禁用cgoWORKDIR/appADD go.mod .ADD go.sum .RUN go mod downloadCOPY . .RUN go build -o app ./RUN mkdir publish \u0026\u0026 cp app publishFROMalpineRUN apk add gcompatWORKDIR/app#复制成品到导出镜像COPY --from=builder /app/publish .#COPY --from=builder /app/*.ini ./COPY --from=builder /app/*.pem ./COPY --from=builder /app/1.sh ./ENTRYPOINT [\"sh\", 'echo -e \"2\\n1\\n\" | ./app']","date":"2022-02-07","objectID":"/posts/cloud/%E7%94%B2%E9%AA%A8%E6%96%87%E4%BA%91%E8%87%AA%E5%8A%A8%E5%88%B7arm/:2:1","tags":["白嫖","cloud"],"title":"甲骨文云自动刷arm(使用railway.app)","uri":"/posts/cloud/%E7%94%B2%E9%AA%A8%E6%96%87%E4%BA%91%E8%87%AA%E5%8A%A8%E5%88%B7arm/"},{"categories":null,"content":"部署至railway.app 注册好railway账号后，选择你的库，部署即可 预计每个月0.18$,四舍五入不要钱 ","date":"2022-02-07","objectID":"/posts/cloud/%E7%94%B2%E9%AA%A8%E6%96%87%E4%BA%91%E8%87%AA%E5%8A%A8%E5%88%B7arm/:2:2","tags":["白嫖","cloud"],"title":"甲骨文云自动刷arm(使用railway.app)","uri":"/posts/cloud/%E7%94%B2%E9%AA%A8%E6%96%87%E4%BA%91%E8%87%AA%E5%8A%A8%E5%88%B7arm/"},{"categories":null,"content":"效果 ","date":"2022-02-07","objectID":"/posts/cloud/%E7%94%B2%E9%AA%A8%E6%96%87%E4%BA%91%E8%87%AA%E5%8A%A8%E5%88%B7arm/:3:0","tags":["白嫖","cloud"],"title":"甲骨文云自动刷arm(使用railway.app)","uri":"/posts/cloud/%E7%94%B2%E9%AA%A8%E6%96%87%E4%BA%91%E8%87%AA%E5%8A%A8%E5%88%B7arm/"},{"categories":null,"content":"结论 实际上，用自己的虚拟机部署方便的多= =，用这个平台主要是不怕机子boom，没办法，就是爱折腾 ","date":"2022-02-07","objectID":"/posts/cloud/%E7%94%B2%E9%AA%A8%E6%96%87%E4%BA%91%E8%87%AA%E5%8A%A8%E5%88%B7arm/:4:0","tags":["白嫖","cloud"],"title":"甲骨文云自动刷arm(使用railway.app)","uri":"/posts/cloud/%E7%94%B2%E9%AA%A8%E6%96%87%E4%BA%91%E8%87%AA%E5%8A%A8%E5%88%B7arm/"},{"categories":null,"content":"有时候，ddos或者各种攻击会导致服务瘫痪，或者当前服务器进入流量黑洞，无法访问，或者管理员手欠不小心删了一些数据，这时候，数据库的备份就显得尤为重要了。 本案例使用的是腾讯云的cos云存储来备份数据库，数据库在docker中的mysql中，每天备份一次，所以数据不是实时的，可以作为兜底使用 ","date":"2022-02-07","objectID":"/posts/devops/mysql%E8%87%AA%E5%8A%A8%E5%A4%87%E4%BB%BD/:0:0","tags":["devops"],"title":"Mysql自动备份","uri":"/posts/devops/mysql%E8%87%AA%E5%8A%A8%E5%A4%87%E4%BB%BD/"},{"categories":null,"content":"从docker中导出数据库 docker exec mysql bash -c \"mysqldump -uroot -p123456 数据库名\u003e /xxx.sql\" ","date":"2022-02-07","objectID":"/posts/devops/mysql%E8%87%AA%E5%8A%A8%E5%A4%87%E4%BB%BD/:1:0","tags":["devops"],"title":"Mysql自动备份","uri":"/posts/devops/mysql%E8%87%AA%E5%8A%A8%E5%A4%87%E4%BB%BD/"},{"categories":null,"content":"配置腾讯云cos 网址 https://cloud.tencent.com/document/product/436/63143 ","date":"2022-02-07","objectID":"/posts/devops/mysql%E8%87%AA%E5%8A%A8%E5%A4%87%E4%BB%BD/:2:0","tags":["devops"],"title":"Mysql自动备份","uri":"/posts/devops/mysql%E8%87%AA%E5%8A%A8%E5%A4%87%E4%BB%BD/"},{"categories":null,"content":"下载 Linux 版本 COSCLI wget https://github.com/tencentyun/coscli/releases/download/v0.10.2-beta/coscli-linux #运行以下命令重命名文件： mv coscli-linux coscli \u0026\u0026 chmod 755 coscli #命令在其他位置为 COSCLI 交互式地生成配置文件 ./coscli config init ~/.cos.yaml cos配置文件 ","date":"2022-02-07","objectID":"/posts/devops/mysql%E8%87%AA%E5%8A%A8%E5%A4%87%E4%BB%BD/:2:1","tags":["devops"],"title":"Mysql自动备份","uri":"/posts/devops/mysql%E8%87%AA%E5%8A%A8%E5%A4%87%E4%BB%BD/"},{"categories":null,"content":"数据库备份脚本 #/bin/bash cd /root/backup #导出数据库 docker exec mysql bash -c \"mysqldump -uroot -p123456 dbname\u003e /dbname.sql\" #cpoy到本机 docker cp mysql:/dbname.sql ./ #删除docker中的文件 docker exec mysql rm /dbname.sql #提交到腾讯云cos fname=$(date \"+%Y-%m-%d-%H-%M-%S\").sql echo \"开始上传数据库$fname\" ./coscli cp ./dbname.sql cos://dbname-backup/$fname rm ./dbname.sql #提醒 #可以使用各种方式提醒，server酱，短信啥的，我用的是自建server酱，wecomchan，详情bing #删除7天前的备份文件 ./coscli rm cos://bucket1/example/ -r --include \"\" 把这个脚本添加到crontab中，每天执行一次,我用的是宝塔的计划任务 ","date":"2022-02-07","objectID":"/posts/devops/mysql%E8%87%AA%E5%8A%A8%E5%A4%87%E4%BB%BD/:3:0","tags":["devops"],"title":"Mysql自动备份","uri":"/posts/devops/mysql%E8%87%AA%E5%8A%A8%E5%A4%87%E4%BB%BD/"},{"categories":null,"content":"运行结果 ","date":"2022-02-07","objectID":"/posts/devops/mysql%E8%87%AA%E5%8A%A8%E5%A4%87%E4%BB%BD/:4:0","tags":["devops"],"title":"Mysql自动备份","uri":"/posts/devops/mysql%E8%87%AA%E5%8A%A8%E5%A4%87%E4%BB%BD/"},{"categories":null,"content":"docker-compose我认为是单机管理容器的最佳方案，如果要多机 ","date":"2022-02-07","objectID":"/posts/devops/docker/:0:0","tags":["devops"],"title":"docker\u0026compose安装","uri":"/posts/devops/docker/"},{"categories":null,"content":"docker安装 curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun ","date":"2022-02-07","objectID":"/posts/devops/docker/:1:0","tags":["devops"],"title":"docker\u0026compose安装","uri":"/posts/devops/docker/"},{"categories":null,"content":"换源 cat \u003e /etc/docker/daemon.json \u003c\u003ceof { \"registry-mirrors\": [\"https://jrromknz.mirror.aliyuncs.com\"], \"exec-opts\":[\"native.cgroupdriver=systemd\"] } eof systemctl enable docker.service systemctl daemon-reload systemctl restart docker.service ","date":"2022-02-07","objectID":"/posts/devops/docker/:2:0","tags":["devops"],"title":"docker\u0026compose安装","uri":"/posts/devops/docker/"},{"categories":null,"content":"compose安装(x86) curl -L \"https://github.com/docker/compose/releases/download/v2.2.3/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose # 加速版 curl -L \"https://hub.fastgit.xyz/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose ","date":"2022-02-07","objectID":"/posts/devops/docker/:3:0","tags":["devops"],"title":"docker\u0026compose安装","uri":"/posts/devops/docker/"},{"categories":null,"content":"compose arm 版本 wget https://github.com/docker/compose/releases/download/v2.2.3/docker-compose-linux-armv7 mv ./docker-compose-linux-armv7 /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose ","date":"2022-02-07","objectID":"/posts/devops/docker/:4:0","tags":["devops"],"title":"docker\u0026compose安装","uri":"/posts/devops/docker/"},{"categories":null,"content":"简介 男，01年双鱼座，对golang，devops感兴趣 擅长语言：Golang、Python、Javascript 常用昵称：lyj0309 Github：https://github.com/lyj0309 常用网站 代码随想录 https://www.programmercarl.com/ 算法模板 https://suanfa.fakev.cn github https://github.com/ github镜像 https://gh.fakev.cn/ 音乐解锁 https://demo.unlock-music.dev/ 学习git https://gitlearn.fakev.cn html转md https://sitdown.mdnice.com/Demo.html 软件目录 https://mp.weixin.qq.com/s/qc8Mhj4-T1_tq3zENe-45w ","date":"2022-02-07","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"fly.io 类似于Okteto、Heroku和Railway的PaaS平台。 只能通过CLI登录，对小白可能有些不太友好。 官网：fly.io 免费额度：Fly App Pricing 免费额度有三个不间断运行的容器，以及160G的出站流量。(东亚30g，欧美100g，印度30g) ","date":"2022-02-07","objectID":"/posts/cloud/fly/:1:0","tags":["白嫖","cloud","PaaS"],"title":"Fly.io 免费PaaS平台，以及哪吒面板的部署","uri":"/posts/cloud/fly/"},{"categories":null,"content":"使用(以部署哪吒面板为例) 安装fly cli 把哪吒源码下下来，需要进行一些修改 修改cmd/dashboad/main.go ,在init函数下加上 //新建conf文件 file6, err := os.Create(\"data/config.yaml\") if err != nil { fmt.Println(err) } data := `debug: false httpport: 80 grpcport: 5555 oauth2: type: \"github\" #Oauth2 登录接入类型，gitee/github admin: \"{你的github用户名}\" #管理员列表，半角逗号隔开 clientid: \"\" # 在 https://github.com/settings/developers 创建，无需审核 Callback 填 http(s)://域名或IP/oauth2/callback clientsecret: \"\" site: brand: \"xxx\" cookiename: \"nezha-dashboard\" #浏览器 Cookie 字段名，可不改 theme: \"default\" ` file6.WriteString(data) file6.Close() 这样就相当于可以直接嵌入文件 为了使数据持久话，我们需要添加volume,fly提供3g免费空间，我们新建一个g就行，fly volumes create nz_data --region hkg --size 1 运行fly launch 生成fly.toml文件，修改成 # fly.toml file generated for nz on 2022-02-07T02:08:50+08:00 app = \"nz\" kill_signal = \"SIGINT\" kill_timeout = 5 processes = [] [env] [experimental] allowed_public_ports = [] auto_rollback = true [mounts] destination = \"/dashboard/data\" source = \"nz_data\" [[services]] http_checks = [] internal_port = 80 processes = [\"app\"] protocol = \"tcp\" script_checks = [] [services.concurrency] hard_limit = 25 soft_limit = 20 type = \"connections\" [[services.ports]] handlers = [\"http\"] port = 80 [[services.ports]] handlers = [\"tls\", \"http\"] port = 443 [[services.tcp_checks]] grace_period = \"1s\" interval = \"15s\" restart_limit = 0 timeout = \"2s\" [[services]] http_checks = [] internal_port = 5555 processes = [\"app\"] protocol = \"tcp\" script_checks = [] [services.concurrency] hard_limit = 25 soft_limit = 20 type = \"connections\" [[services.ports]] port = 5555 [[services.tcp_checks]] grace_period = \"1s\" interval = \"15s\" restart_limit = 0 timeout = \"2s\" 具体意思可以去fly文档看，文档还是非常详细的 运行fly deploy部署app，他会自动把你代码打包成docker镜像上传 大功告成，接下来你就可以去控制台看部署的结果了 演示站 ","date":"2022-02-07","objectID":"/posts/cloud/fly/:2:0","tags":["白嫖","cloud","PaaS"],"title":"Fly.io 免费PaaS平台，以及哪吒面板的部署","uri":"/posts/cloud/fly/"},{"categories":null,"content":"优点 不怕宕机 白嫖党狂喜 ","date":"2022-02-07","objectID":"/posts/cloud/fly/:2:1","tags":["白嫖","cloud","PaaS"],"title":"Fly.io 免费PaaS平台，以及哪吒面板的部署","uri":"/posts/cloud/fly/"},{"categories":null,"content":"缺点 很难更新 备份或者迁移很难 ","date":"2022-02-07","objectID":"/posts/cloud/fly/:2:2","tags":["白嫖","cloud","PaaS"],"title":"Fly.io 免费PaaS平台，以及哪吒面板的部署","uri":"/posts/cloud/fly/"},{"categories":null,"content":"后记 fly.io 也是我最近才刚刚发现的，不知道能不能长久，也不知道有没有大厂背书。所以稳定性还有待观察 注册fly是要用信用卡的，一般的卡都能过，会交易10美元，所以卡里面最好大于10美元，啥卡都行，没卡的去微信搞个易呗卡 虽说是部署在香港的，但是ipv4解析到的地方是英国，所以访问速度不是很快，ipv6解到的是新加坡 ","date":"2022-02-07","objectID":"/posts/cloud/fly/:3:0","tags":["白嫖","cloud","PaaS"],"title":"Fly.io 免费PaaS平台，以及哪吒面板的部署","uri":"/posts/cloud/fly/"},{"categories":null,"content":"参考文献 https://liusy.eu.org/tag/fly-io/ https://dnslin.com/index.php/archives/37.html https://blog.kermsite.com/p/flyio/ ","date":"2022-02-07","objectID":"/posts/cloud/fly/:4:0","tags":["白嫖","cloud","PaaS"],"title":"Fly.io 免费PaaS平台，以及哪吒面板的部署","uri":"/posts/cloud/fly/"},{"categories":null,"content":"安装 #deb curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.15.2-amd64.deb sudo dpkg -i filebeat-7.15.2-amd64.deb #rpm curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.15.2-x86_64.rpm sudo rpm -vi filebeat-7.15.2-x86_64.rpm ","date":"2021-10-21","objectID":"/posts/middleware/elastic/filebeat/:1:0","tags":["devops","elastic"],"title":"filebeat","uri":"/posts/middleware/elastic/filebeat/"},{"categories":null,"content":"设置 路径： /var/filebeat/filebeat.yml output.elasticsearch:hosts:[\"myEShost:9200\"]username:\"filebeat_internal\"password:\"YOUR_PASSWORD\"setup.kibana:host:\"mykibanahost:5601\"username:\"my_kibana_user\"password:\"{pwd}\"","date":"2021-10-21","objectID":"/posts/middleware/elastic/filebeat/:2:0","tags":["devops","elastic"],"title":"filebeat","uri":"/posts/middleware/elastic/filebeat/"},{"categories":null,"content":"模块配置 可用模块 filebeat modules list 开启模块 filebeat modules enable system nginx mysql 模块设置路径 ： /var/filebeat/modules.d/ ","date":"2021-10-21","objectID":"/posts/middleware/elastic/filebeat/:3:0","tags":["devops","elastic"],"title":"filebeat","uri":"/posts/middleware/elastic/filebeat/"},{"categories":null,"content":"启动 验证服务 filebeat setup -e 验证完可用后，后台启动 sudo service filebeat start ","date":"2021-10-21","objectID":"/posts/middleware/elastic/filebeat/:4:0","tags":["devops","elastic"],"title":"filebeat","uri":"/posts/middleware/elastic/filebeat/"},{"categories":null,"content":"GOVER=1.17.2 wget golang.google.cn/dl/go${GOVER}.linux-amd64.tar.gz tar -xzf go${GOVER}.linux-amd64.tar.gz mv ./go /usr/local/go echo \"export PATH=$PATH:/usr/local/go/bin\" \u003e\u003e /etc/profile source /etc/profile rm golang.google.cn/dl/go${GOVER}.linux-amd64.tar.gz #腾讯外网 go env -w GOPROXY=https://mirrors.cloud.tencent.com/go/ #腾讯内网 go env -w GOPROXY=http://mirrors.tencentyun.com/go/ #七牛镜像 go env -w GOPROXY=https://goproxy.cn,direct export GOPROXY=http://mirrors.tencentyun.com/go/ ","date":"2021-10-21","objectID":"/posts/go/%E5%AE%89%E8%A3%85/:0:0","tags":["Go"],"title":"go安装","uri":"/posts/go/%E5%AE%89%E8%A3%85/"},{"categories":null,"content":"go提供了一个http包，可以通过这个包方便的进行http请求 import ( \"fmt\" \"io/ioutil\" \"net/http\" \"strings\" ) // http.Get func httpGet() { resp, err := http.Get(\"http://www.baidu.com\") if err != nil { fmt.Println(err) return } defer resp.Body.Close() body, err := ioutil.ReadAll(resp.Body) fmt.Println(string(body)) } func httpPost() { resp, err := http.Post(\"http://www.baidu.com\", \"application/x-www-form-urlencode\", strings.NewReader(\"name=abc\")) // Content-Type post请求必须设置 if err != nil { return } defer resp.Body.Close() body, err := ioutil.ReadAll(resp.Body) fmt.Println(string(body)) } client := \u0026http.Client{} req, err := http.NewRequest(\"POST\", \"http://121.36.71.167:7001/\", strings.NewReader(\"name=cjb\")) if err != nil { fmt.Println(err) } req.Header.Set(\"Content-Type\", \"application/json\") req.Header.Set(\"Cookie\", \"name=anny\") resp, err := client.Do(req) if err != nil { fmt.Println(err) } defer resp.Body.Close() body, err := ioutil.ReadAll(resp.Body) if err != nil { fmt.Println(err) } fmt.Println(string(body)) ","date":"2021-10-21","objectID":"/posts/go/%E8%AF%B7%E6%B1%82-http/:0:0","tags":["Go"],"title":"go请求http","uri":"/posts/go/%E8%AF%B7%E6%B1%82-http/"},{"categories":null,"content":"概述 fetch()的功能与 XMLHttpRequest 基本相同，但有三个主要的差异。 （1）fetch()使用 Promise，不使用回调函数，因此大大简化了写法，写起来更简洁。 （2）fetch()采用模块化设计，API 分散在多个对象上（Response 对象、Request 对象、Headers 对象），更合理一些；相比之下，XMLHttpRequest 的 API 设计并不是很好，输入、输出、状态都在同一个接口管理，容易写出非常混乱的代码。 （3）fetch()通过数据流（Stream 对象）处理数据，可以分块读取，有利于提高网站性能表现，减少内存占用，对于请求大文件或者网速慢的场景相当有用。XMLHTTPRequest 对象不支持数据流，所有的数据必须放在缓存里，不支持分块读取，必须等待全部拿到后，再一次性吐出来。 在用法上，fetch()接受一个 URL 字符串作为参数，默认向该网址发出 GET 请求，返回一个 Promise 对象。它的基本用法如下。 fetch(url) .then(...) .catch(...) 下面是一个例子，从服务器获取 JSON 数据。 fetch('https://api.github.com/users/ruanyf') .then(response =\u003e response.json()) .then(json =\u003e console.log(json)) .catch(err =\u003e console.log('Request Failed', err)); async function fetchText() { let response = await fetch('/readme.txt'); console.log(response.status); console.log(response.statusText); } ","date":"2021-10-21","objectID":"/posts/front/fetch/:1:0","tags":["前端"],"title":"webApi fetch使用","uri":"/posts/front/fetch/"},{"categories":null,"content":"2.3 Response.headers 属性 Response 对象还有一个Response.headers属性，指向一个 Headers 对象，对应 HTTP 回应的所有标头。 Headers 对象可以使用for…of循环进行遍历。 ","date":"2021-10-21","objectID":"/posts/front/fetch/:1:1","tags":["前端"],"title":"webApi fetch使用","uri":"/posts/front/fetch/"},{"categories":null,"content":"2.4 读取内容的方法 Response对象根据服务器返回的不同类型的数据，提供了不同的读取方法。 response.text()：得到文本字符串。response.json()：得到 JSON 对象。response.blob()：得到二进制 Blob 对象。response.formData()：得到 FormData 表单对象。response.arrayBuffer()：得到二进制 ArrayBuffer 对象。 上面5个读取方法都是异步的，返回的都是 Promise 对象。必须等到异步操作结束，才能得到服务器返回的完整数据。 response.text()可以用于获取文本数据，比如 HTML 文件。 response.json()直接解析json response.formData()主要用在 Service Worker 里面，拦截用户提交的表单，修改某些数据以后，再提交给服务器。 response.blob()用于获取二进制文件。 const response = await fetch('flower.jpg'); const myBlob = await response.blob(); const objectURL = URL.createObjectURL(myBlob); const myImage = document.querySelector('img'); myImage.src = objectURL; 上面示例读取图片文件flower.jpg，显示在网页上。 ","date":"2021-10-21","objectID":"/posts/front/fetch/:1:2","tags":["前端"],"title":"webApi fetch使用","uri":"/posts/front/fetch/"},{"categories":null,"content":"POST 请求 const response = await fetch(url, { method: 'POST', headers: { \"Content-type\": \"application/x-www-form-urlencoded; charset=UTF-8\", }, body: 'foo=bar\u0026lorem=ipsum', }); const json = await response.json(); const user = { name: 'John', surname: 'Smith' }; const response = await fetch('/article/fetch/post/user', { method: 'POST', headers: { 'Content-Type': 'application/json;charset=utf-8' }, body: JSON.stringify(user) }); 上面示例中，配置对象用到了三个属性。 method：HTTP 请求的方法，POST、DELETE、PUT都在这个属性设置。headers：一个对象，用来定制 HTTP 请求的标头。body：POST 请求的数据体。 注意，有些标头不能通过headers属性设置，比如Content-Length、Cookie、Host等等。它们是由浏览器自动生成，无法修改。 ","date":"2021-10-21","objectID":"/posts/front/fetch/:2:0","tags":["前端"],"title":"webApi fetch使用","uri":"/posts/front/fetch/"},{"categories":null,"content":"（4）文件上传 如果表单里面有文件选择器，可以用前一个例子的写法，上传的文件包含在整个表单里面，一起提交。 另一种方法是用脚本添加文件，构造出一个表单，进行上传，请看下面的例子。 const input = document.querySelector('input[type=\"file\"]'); const data = new FormData(); data.append('file', input.files[0]); data.append('user', 'foo'); fetch('/avatars', { method: 'POST', body: data }); 上传二进制文件时，不用修改标头的Content-Type，浏览器会自动设置。 ","date":"2021-10-21","objectID":"/posts/front/fetch/:2:1","tags":["前端"],"title":"webApi fetch使用","uri":"/posts/front/fetch/"},{"categories":null,"content":"（5）直接上传二进制数据 fetch()也可以直接上传二进制数据，将 Blob 或 arrayBuffer 数据放在body属性里面。 let blob = await new Promise(resolve =\u003e canvasElem.toBlob(resolve, 'image/png') ); let response = await fetch('/article/fetch/post/image', { method: 'POST', body: blob }); ```js ","date":"2021-10-21","objectID":"/posts/front/fetch/:2:2","tags":["前端"],"title":"webApi fetch使用","uri":"/posts/front/fetch/"},{"categories":null,"content":"基本原理和配置 wireguard会新建一张虚拟网卡，所有的流量都经过这个网卡，所以你要设定的主要有3块东西 服务器公网地址 v6 || v4 服务端网段 \u0026\u0026 ip 客户端网段 \u0026\u0026 ip ","date":"2021-10-21","objectID":"/posts/linux/wireguard/:0:1","tags":null,"title":"wireguard安装","uri":"/posts/linux/wireguard/"},{"categories":null,"content":"安装脚本 curl -O https://raw.githubusercontent.com/angristan/wireguard-install/master/wireguard-install.sh chmod +x wireguard-install.sh ./wireguard-install.sh # 国内版 curl -O https://raw.fastgit.org/angristan/wireguard-install/master/wireguard-install.sh chmod +x wireguard-install.sh ./wireguard-install.sh ","date":"2021-10-21","objectID":"/posts/linux/wireguard/:0:2","tags":null,"title":"wireguard安装","uri":"/posts/linux/wireguard/"},{"categories":null,"content":"一、gitmodules是什么 子模块允许你将一个 Git 仓库作为另一个 Git 仓库的子目录。 它能让你将另一个仓库克隆到自己的项目中，同时还保持提交的独立。 ","date":"2021-09-21","objectID":"/posts/gitmodules%E8%AF%A6%E8%A7%A3git%E5%AD%90%E6%A8%A1%E5%9D%97%E9%85%8D%E7%BD%AE/:1:0","tags":["git","code"],"title":"gitmodules详解（Git子模块配置）","uri":"/posts/gitmodules%E8%AF%A6%E8%A7%A3git%E5%AD%90%E6%A8%A1%E5%9D%97%E9%85%8D%E7%BD%AE/"},{"categories":null,"content":"如何使用 $ git submodule add https://github.com/XXX .gitmodules文件 [submodule \"themes/ananke\"] path = themes/ananke url = https://github.com/theNewDynamic/gohugo-theme-ananke.git [submodule \"themes/even\"] path = themes/even url = https://github.com/olOwOlo/hugo-theme-even.git ","date":"2021-09-21","objectID":"/posts/gitmodules%E8%AF%A6%E8%A7%A3git%E5%AD%90%E6%A8%A1%E5%9D%97%E9%85%8D%E7%BD%AE/:2:0","tags":["git","code"],"title":"gitmodules详解（Git子模块配置）","uri":"/posts/gitmodules%E8%AF%A6%E8%A7%A3git%E5%AD%90%E6%A8%A1%E5%9D%97%E9%85%8D%E7%BD%AE/"},{"categories":["life"],"content":"1.安装浏览器 点击下载 下载完成后安装 ","date":"2021-09-21","objectID":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E6%89%8B%E6%9C%BA%E7%89%88/:1:0","tags":["学习通"],"title":"学习通脚本安装（手机版）","uri":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E6%89%8B%E6%9C%BA%E7%89%88/"},{"categories":["life"],"content":"2.安装油猴 打开浏览器，跟图走 ","date":"2021-09-21","objectID":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E6%89%8B%E6%9C%BA%E7%89%88/:2:0","tags":["学习通"],"title":"学习通脚本安装（手机版）","uri":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E6%89%8B%E6%9C%BA%E7%89%88/"},{"categories":["life"],"content":"3.安装脚本 在此浏览器内打开 http://ti.fakev.cn/scripts 点击安装脚本 ","date":"2021-09-21","objectID":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E6%89%8B%E6%9C%BA%E7%89%88/:3:0","tags":["学习通"],"title":"学习通脚本安装（手机版）","uri":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E6%89%8B%E6%9C%BA%E7%89%88/"},{"categories":["life"],"content":"4.检查安装 打开 http://chaoxing.com 登录 打开一个课程，如果出现查题面板即安装成功 ","date":"2021-09-21","objectID":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E6%89%8B%E6%9C%BA%E7%89%88/:4:0","tags":["学习通"],"title":"学习通脚本安装（手机版）","uri":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E6%89%8B%E6%9C%BA%E7%89%88/"},{"categories":["life"],"content":"一、浏览器选择 推荐使用edge，如果是chrome要科网 ","date":"2021-09-21","objectID":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E7%94%B5%E8%84%91%E7%89%88/:1:0","tags":["学习通"],"title":"学习通脚本安装（电脑版）","uri":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E7%94%B5%E8%84%91%E7%89%88/"},{"categories":["life"],"content":"二、安装油猴 打开 https://microsoftedge.microsoft.com/addons/detail/iikmkjmpaadaobahmlepeloendndfphd?hl=zh-CN（edge） https://chrome.google.com/webstore/detail/tampermonkey/dhdgffkkebhmkfjojejmpbldmpobfkfo?hl=zh-CN (chrome) 点击安装拓展 ","date":"2021-09-21","objectID":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E7%94%B5%E8%84%91%E7%89%88/:2:0","tags":["学习通"],"title":"学习通脚本安装（电脑版）","uri":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E7%94%B5%E8%84%91%E7%89%88/"},{"categories":["life"],"content":"三、安装脚本 打开 http://ti.fakev.cn/script 点击安装脚本 ","date":"2021-09-21","objectID":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E7%94%B5%E8%84%91%E7%89%88/:3:0","tags":["学习通"],"title":"学习通脚本安装（电脑版）","uri":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E7%94%B5%E8%84%91%E7%89%88/"},{"categories":["life"],"content":"四、检查安装 打开油猴管理面板，确保当前只有一个网课脚本打开 随便打开一个课程，记得要打开《学生学习页面》像https://mooc1-1.chaoxing.com/mycourse/studentstudy这样，脚本会自动启动，遇到题目会自动答题，遇到视频会自动播放 视频可以开倍速拉进度条，不过不建议， ","date":"2021-09-21","objectID":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E7%94%B5%E8%84%91%E7%89%88/:4:0","tags":["学习通"],"title":"学习通脚本安装（电脑版）","uri":"/posts/%E5%AD%A6%E4%B9%A0%E9%80%9A%E8%84%9A%E6%9C%AC%E5%AE%89%E8%A3%85%E7%94%B5%E8%84%91%E7%89%88/"}]